{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cifar10_cnn_003.ipynb","provenance":[],"mount_file_id":"14ofPMySne8nOXp-nd7ZBqGpX7TmlqhwM","authorship_tag":"ABX9TyP3GUEK0j/yFYGE6ScwZKq7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"gVcTGSCBtnga","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609686585370,"user_tz":-540,"elapsed":633,"user":{"displayName":"Hioryuki Onishi","photoUrl":"","userId":"17098326242855842755"}},"outputId":"6a48d0ee-53d7-4e60-dc50-dff768797eea"},"source":["%cd /content/drive/MyDrive/Cifar10"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Cifar10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZU4EBmBfhol","outputId":"5b81bccb-9d57-49c5-98eb-9b0afa0027e6"},"source":["import torch\r\n","import torch.nn.functional as f\r\n","from torch.utils.data import DataLoader\r\n","from torchvision import datasets, transforms\r\n","import matplotlib.pyplot as plt\r\n","from tqdm import tqdm\r\n"," \r\n"," \r\n","class MyCNN(torch.nn.Module):\r\n","    def __init__(self):\r\n","        super(MyCNN, self).__init__()\r\n","        self.conv1 = torch.nn.Conv2d(3,  # チャネル入力\r\n","                                     6,  # チャンネル出力\r\n","                                     5,  # カーネルサイズ\r\n","                                     1,  # ストライド (デフォルトは1)\r\n","                                     0,  # パディング (デフォルトは0)\r\n","                                     )\r\n","        self.conv2 = torch.nn.Conv2d(6, 16, 5)\r\n"," \r\n","        self.pool = torch.nn.MaxPool2d(2, 2)  # カーネルサイズ, ストライド\r\n"," \r\n","        self.dropout1 = torch.nn.Dropout2d(p=0.3)  # [new] Dropoutを追加してみる\r\n"," \r\n","        self.fc1 = torch.nn.Linear(16 * 5 * 5, 120)  # 入力サイズ, 出力サイズ\r\n","        self.dropout2 = torch.nn.Dropout(p=0.5)  # [new] Dropoutを追加してみる\r\n","        self.fc2 = torch.nn.Linear(120, 84)\r\n","        self.fc3 = torch.nn.Linear(84, 10)\r\n"," \r\n","    def forward(self, x):\r\n","        x = f.relu(self.conv1(x))\r\n","        x = self.pool(x)\r\n","        x = f.relu(self.conv2(x))\r\n","        x = self.pool(x)\r\n","        x = self.dropout1(x)  # [new] Dropoutを追加\r\n","        x = x.view(-1, 16 * 5 * 5)  # 1次元データに変えて全結合層へ\r\n","        x = f.relu(self.fc1(x))\r\n","        x = self.dropout2(x)   # [new] Dropoutを追加\r\n","        x = f.relu(self.fc2(x))\r\n","        x = self.fc3(x)\r\n"," \r\n","        return x\r\n"," \r\n","    def plot_conv1(self, prefix_num=0):\r\n","        weights1 = self.conv1.weight\r\n","        weights1 = weights1.reshape(3*6, 5, 5)\r\n"," \r\n","        for i, weight in enumerate(weights1):\r\n","            plt.subplot(3, 6, i + 1)\r\n","            plt.imshow(weight.data.to('cpu').numpy(), cmap='winter')\r\n","            plt.tick_params(labelbottom=False,\r\n","                            labelleft=False,\r\n","                            labelright=False,\r\n","                            labeltop=False,\r\n","                            bottom=False,\r\n","                            left=False,\r\n","                            right=False,\r\n","                            top=False)\r\n"," \r\n","        plt.savefig('img/{}_conv1.png'.format(prefix_num))\r\n","        plt.close()\r\n"," \r\n","    def plot_conv2(self, prefix_num=0):\r\n","        weights2 = self.conv2.weight\r\n","        weights2 = weights2.reshape(6*16, 5, 5)\r\n"," \r\n","        for i, weight in enumerate(weights2):\r\n","            plt.subplot(6, 16, i + 1)\r\n","            plt.imshow(weight.data.to('cpu').numpy(), cmap='winter')\r\n","            plt.tick_params(labelbottom=False,\r\n","                            labelleft=False,\r\n","                            labelright=False,\r\n","                            labeltop=False,\r\n","                            bottom=False,\r\n","                            left=False,\r\n","                            right=False,\r\n","                            top=False)\r\n"," \r\n","        plt.savefig('img/{}_conv2.png'.format(prefix_num))\r\n","        plt.close()\r\n"," \r\n"," \r\n","def load_cifar10(batch=128):\r\n","    train_loader = DataLoader(\r\n","        datasets.CIFAR10('./data',\r\n","                         train=True,\r\n","                         download=True,\r\n","                         transform=transforms.Compose([\r\n","                             transforms.ToTensor(),\r\n","                             transforms.Normalize(\r\n","                                [0.5, 0.5, 0.5],  # RGB 平均\r\n","                                [0.5, 0.5, 0.5]   # RGB 標準偏差\r\n","                                )\r\n","                         ])),\r\n","        batch_size=batch,\r\n","        shuffle=True\r\n","    )\r\n"," \r\n","    test_loader = DataLoader(\r\n","        datasets.CIFAR10('./data',\r\n","                         train=False,\r\n","                         download=True,\r\n","                         transform=transforms.Compose([\r\n","                             transforms.ToTensor(),\r\n","                             transforms.Normalize(\r\n","                                 [0.5, 0.5, 0.5],  # RGB 平均\r\n","                                 [0.5, 0.5, 0.5]  # RGB 標準偏差\r\n","                             )\r\n","                         ])),\r\n","        batch_size=batch,\r\n","        shuffle=True\r\n","    )\r\n"," \r\n","    return {'train': train_loader, 'test': test_loader}\r\n"," \r\n"," \r\n","\r\n"," \r\n"," #=====================================================================\r\n","if __name__ == '__main__':\r\n","    epoch = 300\r\n"," \r\n","    loader = load_cifar10()\r\n","    classes = ('plane', 'car', 'bird', 'cat', 'deer',\r\n","               'dog', 'frog', 'horse', 'ship', 'truck')\r\n"," \r\n","    net: MyCNN = MyCNN()\r\n","    criterion = torch.nn.CrossEntropyLoss()  # ロスの計算\r\n","    optimizer = torch.optim.SGD(params=net.parameters(), lr=0.001, momentum=0.9)\r\n"," \r\n","    # もしGPUが使えるなら使う\r\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n","    net.to(device)\r\n","    print(device)\r\n"," \r\n","    # 学習前のフィルタの可視化\r\n","    net.plot_conv1()\r\n","    net.plot_conv2()\r\n"," \r\n","    history = {\r\n","        'train_loss': [],\r\n","        'train_acc': [],\r\n","        'test_acc': []\r\n","    }\r\n"," \r\n","    for e in range(epoch):\r\n","        net.train()\r\n","        loss = None\r\n","        for i, (images, labels) in enumerate(loader['train']):\r\n","            images = images.to(device)  # to GPU?\r\n","            labels = labels.to(device)\r\n"," \r\n","            optimizer.zero_grad()\r\n","            output = net(images)\r\n","            loss = criterion(output, labels)\r\n","            loss.backward()\r\n","            optimizer.step()\r\n"," \r\n","            if i % 10 == 0:\r\n","                print('Training log: {} epoch ({} / 50000 train. data). Loss: {}'.format(e + 1,\r\n","                                                                                         (i + 1) * 128,\r\n","                                                                                         loss.item())\r\n","                      )\r\n"," \r\n","        # 学習過程でのフィルタの可視化\r\n","        # net.plot_conv1(e+1)\r\n","        # net.plot_conv2(e+1)\r\n"," \r\n","        history['train_loss'].append(loss.item())\r\n"," \r\n","        net.eval()\r\n","        correct = 0\r\n","        with torch.no_grad():\r\n","            for i, (images, labels) in enumerate(tqdm(loader['train'])):\r\n","                images = images.to(device)  # to GPU?\r\n","                labels = labels.to(device)\r\n"," \r\n","                outputs = net(images)\r\n","                _, predicted = torch.max(outputs.data, 1)\r\n","                correct += (predicted == labels).sum().item()\r\n"," \r\n","        acc = float(correct / 50000)\r\n","        history['train_acc'].append(acc)\r\n"," \r\n","        correct = 0\r\n","        with torch.no_grad():\r\n","            for i, (images, labels) in enumerate(tqdm(loader['test'])):\r\n","                images = images.to(device)  # to GPU?\r\n","                labels = labels.to(device)\r\n"," \r\n","                outputs = net(images)\r\n","                _, predicted = torch.max(outputs.data, 1)\r\n","                correct += (predicted == labels).sum().item()\r\n"," \r\n","        acc = float(correct / 10000)\r\n","        history['test_acc'].append(acc)\r\n"," \r\n","\r\n","        print(\"Accuracy : %f\" % acc)\r\n","\r\n","    # 学習前のフィルタの可視化\r\n","    net.plot_conv1(300)\r\n","    net.plot_conv2(300)\r\n"," \r\n","    # 結果をプロット\r\n","    plt.plot(range(1, epoch+1), history['train_loss'])\r\n","    plt.title('Training Loss [CIFAR10]')\r\n","    plt.xlabel('epoch')\r\n","    plt.ylabel('loss')\r\n","    plt.savefig('img/cifar10_loss.png')\r\n","    plt.close()\r\n"," \r\n","    plt.plot(range(1, epoch + 1), history['train_acc'], label='train_acc')\r\n","    plt.plot(range(1, epoch + 1), history['test_acc'], label='test_acc')\r\n","    plt.title('Accuracies [CIFAR10]')\r\n","    plt.xlabel('epoch')\r\n","    plt.ylabel('accuracy')\r\n","    plt.legend()\r\n","    plt.savefig('img/cifar10_acc.png')\r\n","    plt.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","cuda:0\n","Training log: 1 epoch (128 / 50000 train. data). Loss: 2.309361696243286\n","Training log: 1 epoch (1408 / 50000 train. data). Loss: 2.2995588779449463\n","Training log: 1 epoch (2688 / 50000 train. data). Loss: 2.3219475746154785\n","Training log: 1 epoch (3968 / 50000 train. data). Loss: 2.297942876815796\n","Training log: 1 epoch (5248 / 50000 train. data). Loss: 2.3103928565979004\n","Training log: 1 epoch (6528 / 50000 train. data). Loss: 2.2943804264068604\n","Training log: 1 epoch (7808 / 50000 train. data). Loss: 2.3099513053894043\n","Training log: 1 epoch (9088 / 50000 train. data). Loss: 2.2991907596588135\n","Training log: 1 epoch (10368 / 50000 train. data). Loss: 2.3085546493530273\n","Training log: 1 epoch (11648 / 50000 train. data). Loss: 2.3028078079223633\n","Training log: 1 epoch (12928 / 50000 train. data). Loss: 2.3089804649353027\n","Training log: 1 epoch (14208 / 50000 train. data). Loss: 2.3017282485961914\n","Training log: 1 epoch (15488 / 50000 train. data). Loss: 2.3017139434814453\n","Training log: 1 epoch (16768 / 50000 train. data). Loss: 2.3158154487609863\n","Training log: 1 epoch (18048 / 50000 train. data). Loss: 2.299405336380005\n","Training log: 1 epoch (19328 / 50000 train. data). Loss: 2.297874689102173\n","Training log: 1 epoch (20608 / 50000 train. data). Loss: 2.304612874984741\n","Training log: 1 epoch (21888 / 50000 train. data). Loss: 2.3101463317871094\n","Training log: 1 epoch (23168 / 50000 train. data). Loss: 2.304821252822876\n","Training log: 1 epoch (24448 / 50000 train. data). Loss: 2.3044893741607666\n","Training log: 1 epoch (25728 / 50000 train. data). Loss: 2.3026955127716064\n","Training log: 1 epoch (27008 / 50000 train. data). Loss: 2.2995293140411377\n","Training log: 1 epoch (28288 / 50000 train. data). Loss: 2.305393934249878\n","Training log: 1 epoch (29568 / 50000 train. data). Loss: 2.3003499507904053\n","Training log: 1 epoch (30848 / 50000 train. data). Loss: 2.3018109798431396\n","Training log: 1 epoch (32128 / 50000 train. data). Loss: 2.3070249557495117\n","Training log: 1 epoch (33408 / 50000 train. data). Loss: 2.3085975646972656\n","Training log: 1 epoch (34688 / 50000 train. data). Loss: 2.303974151611328\n","Training log: 1 epoch (35968 / 50000 train. data). Loss: 2.3054492473602295\n","Training log: 1 epoch (37248 / 50000 train. data). Loss: 2.3082406520843506\n","Training log: 1 epoch (38528 / 50000 train. data). Loss: 2.3000144958496094\n","Training log: 1 epoch (39808 / 50000 train. data). Loss: 2.3011984825134277\n","Training log: 1 epoch (41088 / 50000 train. data). Loss: 2.305518388748169\n","Training log: 1 epoch (42368 / 50000 train. data). Loss: 2.300795078277588\n","Training log: 1 epoch (43648 / 50000 train. data). Loss: 2.303151845932007\n","Training log: 1 epoch (44928 / 50000 train. data). Loss: 2.3027782440185547\n","Training log: 1 epoch (46208 / 50000 train. data). Loss: 2.302978277206421\n","Training log: 1 epoch (47488 / 50000 train. data). Loss: 2.304469347000122\n","Training log: 1 epoch (48768 / 50000 train. data). Loss: 2.297581434249878\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:09, 38.74it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 1 epoch (50048 / 50000 train. data). Loss: 2.2995829582214355\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.72it/s]\n","100%|██████████| 79/79 [00:02<00:00, 38.57it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.096500\n","Training log: 2 epoch (128 / 50000 train. data). Loss: 2.300995349884033\n","Training log: 2 epoch (1408 / 50000 train. data). Loss: 2.300527572631836\n","Training log: 2 epoch (2688 / 50000 train. data). Loss: 2.30379319190979\n","Training log: 2 epoch (3968 / 50000 train. data). Loss: 2.3015196323394775\n","Training log: 2 epoch (5248 / 50000 train. data). Loss: 2.3019869327545166\n","Training log: 2 epoch (6528 / 50000 train. data). Loss: 2.301485776901245\n","Training log: 2 epoch (7808 / 50000 train. data). Loss: 2.2964816093444824\n","Training log: 2 epoch (9088 / 50000 train. data). Loss: 2.299984931945801\n","Training log: 2 epoch (10368 / 50000 train. data). Loss: 2.304205894470215\n","Training log: 2 epoch (11648 / 50000 train. data). Loss: 2.2982451915740967\n","Training log: 2 epoch (12928 / 50000 train. data). Loss: 2.3023524284362793\n","Training log: 2 epoch (14208 / 50000 train. data). Loss: 2.2998299598693848\n","Training log: 2 epoch (15488 / 50000 train. data). Loss: 2.3030591011047363\n","Training log: 2 epoch (16768 / 50000 train. data). Loss: 2.3036766052246094\n","Training log: 2 epoch (18048 / 50000 train. data). Loss: 2.303292989730835\n","Training log: 2 epoch (19328 / 50000 train. data). Loss: 2.302480459213257\n","Training log: 2 epoch (20608 / 50000 train. data). Loss: 2.305781126022339\n","Training log: 2 epoch (21888 / 50000 train. data). Loss: 2.299884557723999\n","Training log: 2 epoch (23168 / 50000 train. data). Loss: 2.294931650161743\n","Training log: 2 epoch (24448 / 50000 train. data). Loss: 2.2939510345458984\n","Training log: 2 epoch (25728 / 50000 train. data). Loss: 2.3000431060791016\n","Training log: 2 epoch (27008 / 50000 train. data). Loss: 2.301694869995117\n","Training log: 2 epoch (28288 / 50000 train. data). Loss: 2.302122116088867\n","Training log: 2 epoch (29568 / 50000 train. data). Loss: 2.3034534454345703\n","Training log: 2 epoch (30848 / 50000 train. data). Loss: 2.306328296661377\n","Training log: 2 epoch (32128 / 50000 train. data). Loss: 2.297689437866211\n","Training log: 2 epoch (33408 / 50000 train. data). Loss: 2.3035783767700195\n","Training log: 2 epoch (34688 / 50000 train. data). Loss: 2.3017873764038086\n","Training log: 2 epoch (35968 / 50000 train. data). Loss: 2.301218271255493\n","Training log: 2 epoch (37248 / 50000 train. data). Loss: 2.295915126800537\n","Training log: 2 epoch (38528 / 50000 train. data). Loss: 2.3001065254211426\n","Training log: 2 epoch (39808 / 50000 train. data). Loss: 2.3017709255218506\n","Training log: 2 epoch (41088 / 50000 train. data). Loss: 2.2999961376190186\n","Training log: 2 epoch (42368 / 50000 train. data). Loss: 2.2962868213653564\n","Training log: 2 epoch (43648 / 50000 train. data). Loss: 2.292877435684204\n","Training log: 2 epoch (44928 / 50000 train. data). Loss: 2.300269365310669\n","Training log: 2 epoch (46208 / 50000 train. data). Loss: 2.300689697265625\n","Training log: 2 epoch (47488 / 50000 train. data). Loss: 2.3042306900024414\n","Training log: 2 epoch (48768 / 50000 train. data). Loss: 2.3038418292999268\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:09, 39.23it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 2 epoch (50048 / 50000 train. data). Loss: 2.296522617340088\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 36.82it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.84it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.132300\n","Training log: 3 epoch (128 / 50000 train. data). Loss: 2.3011255264282227\n","Training log: 3 epoch (1408 / 50000 train. data). Loss: 2.2963063716888428\n","Training log: 3 epoch (2688 / 50000 train. data). Loss: 2.298738479614258\n","Training log: 3 epoch (3968 / 50000 train. data). Loss: 2.295483350753784\n","Training log: 3 epoch (5248 / 50000 train. data). Loss: 2.301095485687256\n","Training log: 3 epoch (6528 / 50000 train. data). Loss: 2.296388626098633\n","Training log: 3 epoch (7808 / 50000 train. data). Loss: 2.2995829582214355\n","Training log: 3 epoch (9088 / 50000 train. data). Loss: 2.2950565814971924\n","Training log: 3 epoch (10368 / 50000 train. data). Loss: 2.2978789806365967\n","Training log: 3 epoch (11648 / 50000 train. data). Loss: 2.29353928565979\n","Training log: 3 epoch (12928 / 50000 train. data). Loss: 2.293642997741699\n","Training log: 3 epoch (14208 / 50000 train. data). Loss: 2.2913732528686523\n","Training log: 3 epoch (15488 / 50000 train. data). Loss: 2.2963545322418213\n","Training log: 3 epoch (16768 / 50000 train. data). Loss: 2.2994134426116943\n","Training log: 3 epoch (18048 / 50000 train. data). Loss: 2.296848773956299\n","Training log: 3 epoch (19328 / 50000 train. data). Loss: 2.292393445968628\n","Training log: 3 epoch (20608 / 50000 train. data). Loss: 2.299226760864258\n","Training log: 3 epoch (21888 / 50000 train. data). Loss: 2.291412115097046\n","Training log: 3 epoch (23168 / 50000 train. data). Loss: 2.289431571960449\n","Training log: 3 epoch (24448 / 50000 train. data). Loss: 2.2900257110595703\n","Training log: 3 epoch (25728 / 50000 train. data). Loss: 2.2843856811523438\n","Training log: 3 epoch (27008 / 50000 train. data). Loss: 2.2968368530273438\n","Training log: 3 epoch (28288 / 50000 train. data). Loss: 2.294332265853882\n","Training log: 3 epoch (29568 / 50000 train. data). Loss: 2.29415225982666\n","Training log: 3 epoch (30848 / 50000 train. data). Loss: 2.282261610031128\n","Training log: 3 epoch (32128 / 50000 train. data). Loss: 2.2989184856414795\n","Training log: 3 epoch (33408 / 50000 train. data). Loss: 2.2981152534484863\n","Training log: 3 epoch (34688 / 50000 train. data). Loss: 2.295304298400879\n","Training log: 3 epoch (35968 / 50000 train. data). Loss: 2.289777994155884\n","Training log: 3 epoch (37248 / 50000 train. data). Loss: 2.2850399017333984\n","Training log: 3 epoch (38528 / 50000 train. data). Loss: 2.2851204872131348\n","Training log: 3 epoch (39808 / 50000 train. data). Loss: 2.295881748199463\n","Training log: 3 epoch (41088 / 50000 train. data). Loss: 2.297382116317749\n","Training log: 3 epoch (42368 / 50000 train. data). Loss: 2.287703037261963\n","Training log: 3 epoch (43648 / 50000 train. data). Loss: 2.280486583709717\n","Training log: 3 epoch (44928 / 50000 train. data). Loss: 2.2902140617370605\n","Training log: 3 epoch (46208 / 50000 train. data). Loss: 2.2881040573120117\n","Training log: 3 epoch (47488 / 50000 train. data). Loss: 2.2941861152648926\n","Training log: 3 epoch (48768 / 50000 train. data). Loss: 2.2847959995269775\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 38.05it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 3 epoch (50048 / 50000 train. data). Loss: 2.289386749267578\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 36.45it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.51it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.188600\n","Training log: 4 epoch (128 / 50000 train. data). Loss: 2.284045457839966\n","Training log: 4 epoch (1408 / 50000 train. data). Loss: 2.2910878658294678\n","Training log: 4 epoch (2688 / 50000 train. data). Loss: 2.2724416255950928\n","Training log: 4 epoch (3968 / 50000 train. data). Loss: 2.273348093032837\n","Training log: 4 epoch (5248 / 50000 train. data). Loss: 2.282320737838745\n","Training log: 4 epoch (6528 / 50000 train. data). Loss: 2.2914748191833496\n","Training log: 4 epoch (7808 / 50000 train. data). Loss: 2.2795019149780273\n","Training log: 4 epoch (9088 / 50000 train. data). Loss: 2.275343179702759\n","Training log: 4 epoch (10368 / 50000 train. data). Loss: 2.274411201477051\n","Training log: 4 epoch (11648 / 50000 train. data). Loss: 2.268627166748047\n","Training log: 4 epoch (12928 / 50000 train. data). Loss: 2.295205593109131\n","Training log: 4 epoch (14208 / 50000 train. data). Loss: 2.2672104835510254\n","Training log: 4 epoch (15488 / 50000 train. data). Loss: 2.280621290206909\n","Training log: 4 epoch (16768 / 50000 train. data). Loss: 2.272696018218994\n","Training log: 4 epoch (18048 / 50000 train. data). Loss: 2.273914337158203\n","Training log: 4 epoch (19328 / 50000 train. data). Loss: 2.2881808280944824\n","Training log: 4 epoch (20608 / 50000 train. data). Loss: 2.267465353012085\n","Training log: 4 epoch (21888 / 50000 train. data). Loss: 2.2859082221984863\n","Training log: 4 epoch (23168 / 50000 train. data). Loss: 2.272270441055298\n","Training log: 4 epoch (24448 / 50000 train. data). Loss: 2.2251386642456055\n","Training log: 4 epoch (25728 / 50000 train. data). Loss: 2.247685194015503\n","Training log: 4 epoch (27008 / 50000 train. data). Loss: 2.253473997116089\n","Training log: 4 epoch (28288 / 50000 train. data). Loss: 2.2488865852355957\n","Training log: 4 epoch (29568 / 50000 train. data). Loss: 2.240128755569458\n","Training log: 4 epoch (30848 / 50000 train. data). Loss: 2.2512214183807373\n","Training log: 4 epoch (32128 / 50000 train. data). Loss: 2.239741086959839\n","Training log: 4 epoch (33408 / 50000 train. data). Loss: 2.218508243560791\n","Training log: 4 epoch (34688 / 50000 train. data). Loss: 2.253068685531616\n","Training log: 4 epoch (35968 / 50000 train. data). Loss: 2.2521955966949463\n","Training log: 4 epoch (37248 / 50000 train. data). Loss: 2.240445852279663\n","Training log: 4 epoch (38528 / 50000 train. data). Loss: 2.2410426139831543\n","Training log: 4 epoch (39808 / 50000 train. data). Loss: 2.2220652103424072\n","Training log: 4 epoch (41088 / 50000 train. data). Loss: 2.247938394546509\n","Training log: 4 epoch (42368 / 50000 train. data). Loss: 2.2259440422058105\n","Training log: 4 epoch (43648 / 50000 train. data). Loss: 2.259705066680908\n","Training log: 4 epoch (44928 / 50000 train. data). Loss: 2.2464547157287598\n","Training log: 4 epoch (46208 / 50000 train. data). Loss: 2.2442188262939453\n","Training log: 4 epoch (47488 / 50000 train. data). Loss: 2.242837905883789\n","Training log: 4 epoch (48768 / 50000 train. data). Loss: 2.2258541584014893\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 38.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 4 epoch (50048 / 50000 train. data). Loss: 2.1997199058532715\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.57it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.47it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.226900\n","Training log: 5 epoch (128 / 50000 train. data). Loss: 2.2160115242004395\n","Training log: 5 epoch (1408 / 50000 train. data). Loss: 2.168379545211792\n","Training log: 5 epoch (2688 / 50000 train. data). Loss: 2.213196277618408\n","Training log: 5 epoch (3968 / 50000 train. data). Loss: 2.2072055339813232\n","Training log: 5 epoch (5248 / 50000 train. data). Loss: 2.2622809410095215\n","Training log: 5 epoch (6528 / 50000 train. data). Loss: 2.1469976902008057\n","Training log: 5 epoch (7808 / 50000 train. data). Loss: 2.2098090648651123\n","Training log: 5 epoch (9088 / 50000 train. data). Loss: 2.125119209289551\n","Training log: 5 epoch (10368 / 50000 train. data). Loss: 2.151864528656006\n","Training log: 5 epoch (11648 / 50000 train. data). Loss: 2.18788480758667\n","Training log: 5 epoch (12928 / 50000 train. data). Loss: 2.1815991401672363\n","Training log: 5 epoch (14208 / 50000 train. data). Loss: 2.1288490295410156\n","Training log: 5 epoch (15488 / 50000 train. data). Loss: 2.1795425415039062\n","Training log: 5 epoch (16768 / 50000 train. data). Loss: 2.200252056121826\n","Training log: 5 epoch (18048 / 50000 train. data). Loss: 2.132657766342163\n","Training log: 5 epoch (19328 / 50000 train. data). Loss: 2.136751174926758\n","Training log: 5 epoch (20608 / 50000 train. data). Loss: 2.1180636882781982\n","Training log: 5 epoch (21888 / 50000 train. data). Loss: 2.1123509407043457\n","Training log: 5 epoch (23168 / 50000 train. data). Loss: 2.1369433403015137\n","Training log: 5 epoch (24448 / 50000 train. data). Loss: 2.0996475219726562\n","Training log: 5 epoch (25728 / 50000 train. data). Loss: 2.204554557800293\n","Training log: 5 epoch (27008 / 50000 train. data). Loss: 2.1473772525787354\n","Training log: 5 epoch (28288 / 50000 train. data). Loss: 2.1084401607513428\n","Training log: 5 epoch (29568 / 50000 train. data). Loss: 2.116135358810425\n","Training log: 5 epoch (30848 / 50000 train. data). Loss: 2.158888816833496\n","Training log: 5 epoch (32128 / 50000 train. data). Loss: 2.154460906982422\n","Training log: 5 epoch (33408 / 50000 train. data). Loss: 2.1339304447174072\n","Training log: 5 epoch (34688 / 50000 train. data). Loss: 2.0518038272857666\n","Training log: 5 epoch (35968 / 50000 train. data). Loss: 2.1793622970581055\n","Training log: 5 epoch (37248 / 50000 train. data). Loss: 2.1540305614471436\n","Training log: 5 epoch (38528 / 50000 train. data). Loss: 2.1892545223236084\n","Training log: 5 epoch (39808 / 50000 train. data). Loss: 2.105527400970459\n","Training log: 5 epoch (41088 / 50000 train. data). Loss: 2.0698671340942383\n","Training log: 5 epoch (42368 / 50000 train. data). Loss: 2.0775818824768066\n","Training log: 5 epoch (43648 / 50000 train. data). Loss: 2.0761516094207764\n","Training log: 5 epoch (44928 / 50000 train. data). Loss: 2.0964248180389404\n","Training log: 5 epoch (46208 / 50000 train. data). Loss: 2.1426122188568115\n","Training log: 5 epoch (47488 / 50000 train. data). Loss: 2.156083583831787\n","Training log: 5 epoch (48768 / 50000 train. data). Loss: 2.1249349117279053\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 5 epoch (50048 / 50000 train. data). Loss: 2.15283203125\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 36.19it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.270600\n","Training log: 6 epoch (128 / 50000 train. data). Loss: 2.1044023036956787\n","Training log: 6 epoch (1408 / 50000 train. data). Loss: 2.09696364402771\n","Training log: 6 epoch (2688 / 50000 train. data). Loss: 2.120560884475708\n","Training log: 6 epoch (3968 / 50000 train. data). Loss: 2.06577205657959\n","Training log: 6 epoch (5248 / 50000 train. data). Loss: 2.1133809089660645\n","Training log: 6 epoch (6528 / 50000 train. data). Loss: 2.0581321716308594\n","Training log: 6 epoch (7808 / 50000 train. data). Loss: 2.132964849472046\n","Training log: 6 epoch (9088 / 50000 train. data). Loss: 2.1961419582366943\n","Training log: 6 epoch (10368 / 50000 train. data). Loss: 2.036128044128418\n","Training log: 6 epoch (11648 / 50000 train. data). Loss: 2.0692341327667236\n","Training log: 6 epoch (12928 / 50000 train. data). Loss: 2.0859124660491943\n","Training log: 6 epoch (14208 / 50000 train. data). Loss: 2.028855323791504\n","Training log: 6 epoch (15488 / 50000 train. data). Loss: 2.067201614379883\n","Training log: 6 epoch (16768 / 50000 train. data). Loss: 2.0839526653289795\n","Training log: 6 epoch (18048 / 50000 train. data). Loss: 1.994316816329956\n","Training log: 6 epoch (19328 / 50000 train. data). Loss: 2.1047134399414062\n","Training log: 6 epoch (20608 / 50000 train. data). Loss: 2.0639901161193848\n","Training log: 6 epoch (21888 / 50000 train. data). Loss: 2.005013942718506\n","Training log: 6 epoch (23168 / 50000 train. data). Loss: 2.0489025115966797\n","Training log: 6 epoch (24448 / 50000 train. data). Loss: 2.130129337310791\n","Training log: 6 epoch (25728 / 50000 train. data). Loss: 2.034109354019165\n","Training log: 6 epoch (27008 / 50000 train. data). Loss: 2.0069615840911865\n","Training log: 6 epoch (28288 / 50000 train. data). Loss: 2.0739314556121826\n","Training log: 6 epoch (29568 / 50000 train. data). Loss: 2.1974217891693115\n","Training log: 6 epoch (30848 / 50000 train. data). Loss: 1.9886921644210815\n","Training log: 6 epoch (32128 / 50000 train. data). Loss: 2.011551856994629\n","Training log: 6 epoch (33408 / 50000 train. data). Loss: 2.061343193054199\n","Training log: 6 epoch (34688 / 50000 train. data). Loss: 2.18892502784729\n","Training log: 6 epoch (35968 / 50000 train. data). Loss: 1.9255683422088623\n","Training log: 6 epoch (37248 / 50000 train. data). Loss: 2.062326669692993\n","Training log: 6 epoch (38528 / 50000 train. data). Loss: 1.982750415802002\n","Training log: 6 epoch (39808 / 50000 train. data). Loss: 2.0072669982910156\n","Training log: 6 epoch (41088 / 50000 train. data). Loss: 2.0909221172332764\n","Training log: 6 epoch (42368 / 50000 train. data). Loss: 2.103044271469116\n","Training log: 6 epoch (43648 / 50000 train. data). Loss: 1.95669686794281\n","Training log: 6 epoch (44928 / 50000 train. data). Loss: 2.163227081298828\n","Training log: 6 epoch (46208 / 50000 train. data). Loss: 2.0994672775268555\n","Training log: 6 epoch (47488 / 50000 train. data). Loss: 2.149355411529541\n","Training log: 6 epoch (48768 / 50000 train. data). Loss: 2.0591492652893066\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 6 epoch (50048 / 50000 train. data). Loss: 2.003934383392334\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 36.50it/s]\n","100%|██████████| 79/79 [00:02<00:00, 37.35it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.294400\n","Training log: 7 epoch (128 / 50000 train. data). Loss: 2.1333651542663574\n","Training log: 7 epoch (1408 / 50000 train. data). Loss: 2.007742166519165\n","Training log: 7 epoch (2688 / 50000 train. data). Loss: 1.998680830001831\n","Training log: 7 epoch (3968 / 50000 train. data). Loss: 2.0269877910614014\n","Training log: 7 epoch (5248 / 50000 train. data). Loss: 2.0448262691497803\n","Training log: 7 epoch (6528 / 50000 train. data). Loss: 2.121002674102783\n","Training log: 7 epoch (7808 / 50000 train. data). Loss: 1.9864704608917236\n","Training log: 7 epoch (9088 / 50000 train. data). Loss: 2.074253559112549\n","Training log: 7 epoch (10368 / 50000 train. data). Loss: 2.0823612213134766\n","Training log: 7 epoch (11648 / 50000 train. data). Loss: 2.091435194015503\n","Training log: 7 epoch (12928 / 50000 train. data). Loss: 2.07502818107605\n","Training log: 7 epoch (14208 / 50000 train. data). Loss: 1.9830621480941772\n","Training log: 7 epoch (15488 / 50000 train. data). Loss: 1.9540486335754395\n","Training log: 7 epoch (16768 / 50000 train. data). Loss: 2.07041335105896\n","Training log: 7 epoch (18048 / 50000 train. data). Loss: 2.059312582015991\n","Training log: 7 epoch (19328 / 50000 train. data). Loss: 2.016505479812622\n","Training log: 7 epoch (20608 / 50000 train. data). Loss: 2.123119831085205\n","Training log: 7 epoch (21888 / 50000 train. data). Loss: 2.0721840858459473\n","Training log: 7 epoch (23168 / 50000 train. data). Loss: 2.049868583679199\n","Training log: 7 epoch (24448 / 50000 train. data). Loss: 2.0191218852996826\n","Training log: 7 epoch (25728 / 50000 train. data). Loss: 2.0667572021484375\n","Training log: 7 epoch (27008 / 50000 train. data). Loss: 2.0280001163482666\n","Training log: 7 epoch (28288 / 50000 train. data). Loss: 2.010826826095581\n","Training log: 7 epoch (29568 / 50000 train. data). Loss: 2.0079805850982666\n","Training log: 7 epoch (30848 / 50000 train. data). Loss: 2.0419609546661377\n","Training log: 7 epoch (32128 / 50000 train. data). Loss: 1.9855877161026\n","Training log: 7 epoch (33408 / 50000 train. data). Loss: 1.9883805513381958\n","Training log: 7 epoch (34688 / 50000 train. data). Loss: 2.1437571048736572\n","Training log: 7 epoch (35968 / 50000 train. data). Loss: 1.9897178411483765\n","Training log: 7 epoch (37248 / 50000 train. data). Loss: 2.002753496170044\n","Training log: 7 epoch (38528 / 50000 train. data). Loss: 1.973356008529663\n","Training log: 7 epoch (39808 / 50000 train. data). Loss: 1.9600989818572998\n","Training log: 7 epoch (41088 / 50000 train. data). Loss: 1.9789470434188843\n","Training log: 7 epoch (42368 / 50000 train. data). Loss: 2.0250391960144043\n","Training log: 7 epoch (43648 / 50000 train. data). Loss: 2.0084760189056396\n","Training log: 7 epoch (44928 / 50000 train. data). Loss: 1.965887427330017\n","Training log: 7 epoch (46208 / 50000 train. data). Loss: 1.9831666946411133\n","Training log: 7 epoch (47488 / 50000 train. data). Loss: 2.0329947471618652\n","Training log: 7 epoch (48768 / 50000 train. data). Loss: 1.9490559101104736\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 38.17it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 7 epoch (50048 / 50000 train. data). Loss: 1.8706114292144775\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.81it/s]\n","100%|██████████| 79/79 [00:02<00:00, 32.47it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.301300\n","Training log: 8 epoch (128 / 50000 train. data). Loss: 1.9546020030975342\n","Training log: 8 epoch (1408 / 50000 train. data). Loss: 1.9429786205291748\n","Training log: 8 epoch (2688 / 50000 train. data). Loss: 1.8667336702346802\n","Training log: 8 epoch (3968 / 50000 train. data). Loss: 2.007753372192383\n","Training log: 8 epoch (5248 / 50000 train. data). Loss: 1.9024578332901\n","Training log: 8 epoch (6528 / 50000 train. data). Loss: 2.0485024452209473\n","Training log: 8 epoch (7808 / 50000 train. data). Loss: 2.0231986045837402\n","Training log: 8 epoch (9088 / 50000 train. data). Loss: 1.9280145168304443\n","Training log: 8 epoch (10368 / 50000 train. data). Loss: 2.0091817378997803\n","Training log: 8 epoch (11648 / 50000 train. data). Loss: 2.0721006393432617\n","Training log: 8 epoch (12928 / 50000 train. data). Loss: 1.9715276956558228\n","Training log: 8 epoch (14208 / 50000 train. data). Loss: 2.0130109786987305\n","Training log: 8 epoch (15488 / 50000 train. data). Loss: 1.9281164407730103\n","Training log: 8 epoch (16768 / 50000 train. data). Loss: 1.881600260734558\n","Training log: 8 epoch (18048 / 50000 train. data). Loss: 2.033386468887329\n","Training log: 8 epoch (19328 / 50000 train. data). Loss: 2.0750458240509033\n","Training log: 8 epoch (20608 / 50000 train. data). Loss: 2.0076992511749268\n","Training log: 8 epoch (21888 / 50000 train. data). Loss: 1.8807032108306885\n","Training log: 8 epoch (23168 / 50000 train. data). Loss: 2.05336594581604\n","Training log: 8 epoch (24448 / 50000 train. data). Loss: 1.9543790817260742\n","Training log: 8 epoch (25728 / 50000 train. data). Loss: 2.011770009994507\n","Training log: 8 epoch (27008 / 50000 train. data). Loss: 2.0476527214050293\n","Training log: 8 epoch (28288 / 50000 train. data). Loss: 1.9677790403366089\n","Training log: 8 epoch (29568 / 50000 train. data). Loss: 2.000385046005249\n","Training log: 8 epoch (30848 / 50000 train. data). Loss: 1.956015944480896\n","Training log: 8 epoch (32128 / 50000 train. data). Loss: 1.9521350860595703\n","Training log: 8 epoch (33408 / 50000 train. data). Loss: 1.9466886520385742\n","Training log: 8 epoch (34688 / 50000 train. data). Loss: 1.9113688468933105\n","Training log: 8 epoch (35968 / 50000 train. data). Loss: 1.9315080642700195\n","Training log: 8 epoch (37248 / 50000 train. data). Loss: 1.9504295587539673\n","Training log: 8 epoch (38528 / 50000 train. data). Loss: 1.937226414680481\n","Training log: 8 epoch (39808 / 50000 train. data). Loss: 1.9143309593200684\n","Training log: 8 epoch (41088 / 50000 train. data). Loss: 2.020866632461548\n","Training log: 8 epoch (42368 / 50000 train. data). Loss: 1.8962810039520264\n","Training log: 8 epoch (43648 / 50000 train. data). Loss: 2.0840253829956055\n","Training log: 8 epoch (44928 / 50000 train. data). Loss: 1.9459844827651978\n","Training log: 8 epoch (46208 / 50000 train. data). Loss: 1.9169058799743652\n","Training log: 8 epoch (47488 / 50000 train. data). Loss: 1.871885895729065\n","Training log: 8 epoch (48768 / 50000 train. data). Loss: 1.9885085821151733\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 38.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 8 epoch (50048 / 50000 train. data). Loss: 2.010633945465088\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 37.50it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.39it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.325000\n","Training log: 9 epoch (128 / 50000 train. data). Loss: 1.9787248373031616\n","Training log: 9 epoch (1408 / 50000 train. data). Loss: 2.007955551147461\n","Training log: 9 epoch (2688 / 50000 train. data). Loss: 1.9576629400253296\n","Training log: 9 epoch (3968 / 50000 train. data). Loss: 1.9298754930496216\n","Training log: 9 epoch (5248 / 50000 train. data). Loss: 1.9636318683624268\n","Training log: 9 epoch (6528 / 50000 train. data). Loss: 1.9177234172821045\n","Training log: 9 epoch (7808 / 50000 train. data). Loss: 1.9261430501937866\n","Training log: 9 epoch (9088 / 50000 train. data). Loss: 1.8888850212097168\n","Training log: 9 epoch (10368 / 50000 train. data). Loss: 1.9427523612976074\n","Training log: 9 epoch (11648 / 50000 train. data). Loss: 2.005580186843872\n","Training log: 9 epoch (12928 / 50000 train. data). Loss: 1.9186880588531494\n","Training log: 9 epoch (14208 / 50000 train. data). Loss: 2.048548460006714\n","Training log: 9 epoch (15488 / 50000 train. data). Loss: 1.9487375020980835\n","Training log: 9 epoch (16768 / 50000 train. data). Loss: 1.8956718444824219\n","Training log: 9 epoch (18048 / 50000 train. data). Loss: 1.9296627044677734\n","Training log: 9 epoch (19328 / 50000 train. data). Loss: 1.8836795091629028\n","Training log: 9 epoch (20608 / 50000 train. data). Loss: 1.8776785135269165\n","Training log: 9 epoch (21888 / 50000 train. data). Loss: 1.8781335353851318\n","Training log: 9 epoch (23168 / 50000 train. data). Loss: 1.9389936923980713\n","Training log: 9 epoch (24448 / 50000 train. data). Loss: 1.9195290803909302\n","Training log: 9 epoch (25728 / 50000 train. data). Loss: 2.014183759689331\n","Training log: 9 epoch (27008 / 50000 train. data). Loss: 1.9727041721343994\n","Training log: 9 epoch (28288 / 50000 train. data). Loss: 1.8385423421859741\n","Training log: 9 epoch (29568 / 50000 train. data). Loss: 1.9280844926834106\n","Training log: 9 epoch (30848 / 50000 train. data). Loss: 1.8878535032272339\n","Training log: 9 epoch (32128 / 50000 train. data). Loss: 1.9461112022399902\n","Training log: 9 epoch (33408 / 50000 train. data). Loss: 2.0626440048217773\n","Training log: 9 epoch (34688 / 50000 train. data). Loss: 1.8703395128250122\n","Training log: 9 epoch (35968 / 50000 train. data). Loss: 1.9600718021392822\n","Training log: 9 epoch (37248 / 50000 train. data). Loss: 1.8903617858886719\n","Training log: 9 epoch (38528 / 50000 train. data). Loss: 1.9668993949890137\n","Training log: 9 epoch (39808 / 50000 train. data). Loss: 1.8137621879577637\n","Training log: 9 epoch (41088 / 50000 train. data). Loss: 1.8950155973434448\n","Training log: 9 epoch (42368 / 50000 train. data). Loss: 1.9325637817382812\n","Training log: 9 epoch (43648 / 50000 train. data). Loss: 1.9450511932373047\n","Training log: 9 epoch (44928 / 50000 train. data). Loss: 1.8687742948532104\n","Training log: 9 epoch (46208 / 50000 train. data). Loss: 1.8466795682907104\n","Training log: 9 epoch (47488 / 50000 train. data). Loss: 1.9111825227737427\n","Training log: 9 epoch (48768 / 50000 train. data). Loss: 2.0402143001556396\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 9 epoch (50048 / 50000 train. data). Loss: 1.9158647060394287\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 36.18it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.82it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.345100\n","Training log: 10 epoch (128 / 50000 train. data). Loss: 1.9100027084350586\n","Training log: 10 epoch (1408 / 50000 train. data). Loss: 1.9488887786865234\n","Training log: 10 epoch (2688 / 50000 train. data). Loss: 1.835825800895691\n","Training log: 10 epoch (3968 / 50000 train. data). Loss: 1.884598970413208\n","Training log: 10 epoch (5248 / 50000 train. data). Loss: 1.8928929567337036\n","Training log: 10 epoch (6528 / 50000 train. data). Loss: 2.000192880630493\n","Training log: 10 epoch (7808 / 50000 train. data). Loss: 1.8492166996002197\n","Training log: 10 epoch (9088 / 50000 train. data). Loss: 1.9396240711212158\n","Training log: 10 epoch (10368 / 50000 train. data). Loss: 2.0972819328308105\n","Training log: 10 epoch (11648 / 50000 train. data). Loss: 1.9456982612609863\n","Training log: 10 epoch (12928 / 50000 train. data). Loss: 1.9279125928878784\n","Training log: 10 epoch (14208 / 50000 train. data). Loss: 1.827165126800537\n","Training log: 10 epoch (15488 / 50000 train. data). Loss: 1.9614222049713135\n","Training log: 10 epoch (16768 / 50000 train. data). Loss: 1.9097079038619995\n","Training log: 10 epoch (18048 / 50000 train. data). Loss: 1.9164568185806274\n","Training log: 10 epoch (19328 / 50000 train. data). Loss: 1.9386683702468872\n","Training log: 10 epoch (20608 / 50000 train. data). Loss: 1.8883010149002075\n","Training log: 10 epoch (21888 / 50000 train. data). Loss: 1.9691694974899292\n","Training log: 10 epoch (23168 / 50000 train. data). Loss: 1.8205530643463135\n","Training log: 10 epoch (24448 / 50000 train. data). Loss: 1.9577860832214355\n","Training log: 10 epoch (25728 / 50000 train. data). Loss: 1.9602842330932617\n","Training log: 10 epoch (27008 / 50000 train. data). Loss: 1.8747949600219727\n","Training log: 10 epoch (28288 / 50000 train. data). Loss: 1.7894580364227295\n","Training log: 10 epoch (29568 / 50000 train. data). Loss: 1.8317497968673706\n","Training log: 10 epoch (30848 / 50000 train. data). Loss: 1.9095730781555176\n","Training log: 10 epoch (32128 / 50000 train. data). Loss: 1.8756500482559204\n","Training log: 10 epoch (33408 / 50000 train. data). Loss: 1.8200191259384155\n","Training log: 10 epoch (34688 / 50000 train. data). Loss: 1.8794208765029907\n","Training log: 10 epoch (35968 / 50000 train. data). Loss: 1.8778163194656372\n","Training log: 10 epoch (37248 / 50000 train. data). Loss: 1.8210804462432861\n","Training log: 10 epoch (38528 / 50000 train. data). Loss: 1.9006271362304688\n","Training log: 10 epoch (39808 / 50000 train. data). Loss: 1.7636882066726685\n","Training log: 10 epoch (41088 / 50000 train. data). Loss: 1.8786486387252808\n","Training log: 10 epoch (42368 / 50000 train. data). Loss: 1.8166606426239014\n","Training log: 10 epoch (43648 / 50000 train. data). Loss: 1.8704798221588135\n","Training log: 10 epoch (44928 / 50000 train. data). Loss: 1.9384689331054688\n","Training log: 10 epoch (46208 / 50000 train. data). Loss: 1.8926130533218384\n","Training log: 10 epoch (47488 / 50000 train. data). Loss: 1.833184838294983\n","Training log: 10 epoch (48768 / 50000 train. data). Loss: 1.8309886455535889\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 10 epoch (50048 / 50000 train. data). Loss: 1.8157333135604858\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.60it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.00it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.359100\n","Training log: 11 epoch (128 / 50000 train. data). Loss: 1.8461103439331055\n","Training log: 11 epoch (1408 / 50000 train. data). Loss: 1.9624760150909424\n","Training log: 11 epoch (2688 / 50000 train. data). Loss: 1.9032809734344482\n","Training log: 11 epoch (3968 / 50000 train. data). Loss: 1.8207414150238037\n","Training log: 11 epoch (5248 / 50000 train. data). Loss: 1.9521764516830444\n","Training log: 11 epoch (6528 / 50000 train. data). Loss: 1.916583776473999\n","Training log: 11 epoch (7808 / 50000 train. data). Loss: 1.6827356815338135\n","Training log: 11 epoch (9088 / 50000 train. data). Loss: 1.8151195049285889\n","Training log: 11 epoch (10368 / 50000 train. data). Loss: 1.8770161867141724\n","Training log: 11 epoch (11648 / 50000 train. data). Loss: 1.9641196727752686\n","Training log: 11 epoch (12928 / 50000 train. data). Loss: 1.936109185218811\n","Training log: 11 epoch (14208 / 50000 train. data). Loss: 1.8073348999023438\n","Training log: 11 epoch (15488 / 50000 train. data). Loss: 1.9560532569885254\n","Training log: 11 epoch (16768 / 50000 train. data). Loss: 1.8452696800231934\n","Training log: 11 epoch (18048 / 50000 train. data). Loss: 1.7764060497283936\n","Training log: 11 epoch (19328 / 50000 train. data). Loss: 1.7947150468826294\n","Training log: 11 epoch (20608 / 50000 train. data). Loss: 1.8733501434326172\n","Training log: 11 epoch (21888 / 50000 train. data). Loss: 1.7911953926086426\n","Training log: 11 epoch (23168 / 50000 train. data). Loss: 1.9105592966079712\n","Training log: 11 epoch (24448 / 50000 train. data). Loss: 1.8586727380752563\n","Training log: 11 epoch (25728 / 50000 train. data). Loss: 1.7501567602157593\n","Training log: 11 epoch (27008 / 50000 train. data). Loss: 1.8138340711593628\n","Training log: 11 epoch (28288 / 50000 train. data). Loss: 1.841894268989563\n","Training log: 11 epoch (29568 / 50000 train. data). Loss: 1.9053153991699219\n","Training log: 11 epoch (30848 / 50000 train. data). Loss: 1.7900210618972778\n","Training log: 11 epoch (32128 / 50000 train. data). Loss: 1.8079280853271484\n","Training log: 11 epoch (33408 / 50000 train. data). Loss: 1.904157280921936\n","Training log: 11 epoch (34688 / 50000 train. data). Loss: 1.8272349834442139\n","Training log: 11 epoch (35968 / 50000 train. data). Loss: 1.9636253118515015\n","Training log: 11 epoch (37248 / 50000 train. data). Loss: 1.8405513763427734\n","Training log: 11 epoch (38528 / 50000 train. data). Loss: 1.9220824241638184\n","Training log: 11 epoch (39808 / 50000 train. data). Loss: 1.8894588947296143\n","Training log: 11 epoch (41088 / 50000 train. data). Loss: 1.8054448366165161\n","Training log: 11 epoch (42368 / 50000 train. data). Loss: 1.871328592300415\n","Training log: 11 epoch (43648 / 50000 train. data). Loss: 1.9895154237747192\n","Training log: 11 epoch (44928 / 50000 train. data). Loss: 1.7772270441055298\n","Training log: 11 epoch (46208 / 50000 train. data). Loss: 1.8382689952850342\n","Training log: 11 epoch (47488 / 50000 train. data). Loss: 1.9489943981170654\n","Training log: 11 epoch (48768 / 50000 train. data). Loss: 1.8595917224884033\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 11 epoch (50048 / 50000 train. data). Loss: 1.6861763000488281\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.22it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.43it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.369700\n","Training log: 12 epoch (128 / 50000 train. data). Loss: 1.901210069656372\n","Training log: 12 epoch (1408 / 50000 train. data). Loss: 1.77530038356781\n","Training log: 12 epoch (2688 / 50000 train. data). Loss: 1.8941020965576172\n","Training log: 12 epoch (3968 / 50000 train. data). Loss: 1.8510311841964722\n","Training log: 12 epoch (5248 / 50000 train. data). Loss: 1.77662992477417\n","Training log: 12 epoch (6528 / 50000 train. data). Loss: 1.8509193658828735\n","Training log: 12 epoch (7808 / 50000 train. data). Loss: 1.959550380706787\n","Training log: 12 epoch (9088 / 50000 train. data). Loss: 1.910622477531433\n","Training log: 12 epoch (10368 / 50000 train. data). Loss: 1.8656589984893799\n","Training log: 12 epoch (11648 / 50000 train. data). Loss: 1.8513288497924805\n","Training log: 12 epoch (12928 / 50000 train. data). Loss: 1.791528344154358\n","Training log: 12 epoch (14208 / 50000 train. data). Loss: 1.8272384405136108\n","Training log: 12 epoch (15488 / 50000 train. data). Loss: 1.7518717050552368\n","Training log: 12 epoch (16768 / 50000 train. data). Loss: 1.8505326509475708\n","Training log: 12 epoch (18048 / 50000 train. data). Loss: 1.744139552116394\n","Training log: 12 epoch (19328 / 50000 train. data). Loss: 1.978965401649475\n","Training log: 12 epoch (20608 / 50000 train. data). Loss: 1.8584460020065308\n","Training log: 12 epoch (21888 / 50000 train. data). Loss: 1.8632572889328003\n","Training log: 12 epoch (23168 / 50000 train. data). Loss: 1.8940657377243042\n","Training log: 12 epoch (24448 / 50000 train. data). Loss: 1.7666072845458984\n","Training log: 12 epoch (25728 / 50000 train. data). Loss: 1.8016031980514526\n","Training log: 12 epoch (27008 / 50000 train. data). Loss: 1.782566785812378\n","Training log: 12 epoch (28288 / 50000 train. data). Loss: 1.836007833480835\n","Training log: 12 epoch (29568 / 50000 train. data). Loss: 1.8086161613464355\n","Training log: 12 epoch (30848 / 50000 train. data). Loss: 1.816876769065857\n","Training log: 12 epoch (32128 / 50000 train. data). Loss: 1.8345614671707153\n","Training log: 12 epoch (33408 / 50000 train. data). Loss: 1.9479418992996216\n","Training log: 12 epoch (34688 / 50000 train. data). Loss: 1.8340773582458496\n","Training log: 12 epoch (35968 / 50000 train. data). Loss: 1.7558510303497314\n","Training log: 12 epoch (37248 / 50000 train. data). Loss: 1.885785460472107\n","Training log: 12 epoch (38528 / 50000 train. data). Loss: 1.8224118947982788\n","Training log: 12 epoch (39808 / 50000 train. data). Loss: 1.8728258609771729\n","Training log: 12 epoch (41088 / 50000 train. data). Loss: 1.7927638292312622\n","Training log: 12 epoch (42368 / 50000 train. data). Loss: 1.6860400438308716\n","Training log: 12 epoch (43648 / 50000 train. data). Loss: 1.9261269569396973\n","Training log: 12 epoch (44928 / 50000 train. data). Loss: 1.7541168928146362\n","Training log: 12 epoch (46208 / 50000 train. data). Loss: 1.7701095342636108\n","Training log: 12 epoch (47488 / 50000 train. data). Loss: 1.8104928731918335\n","Training log: 12 epoch (48768 / 50000 train. data). Loss: 1.8065816164016724\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.42it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 12 epoch (50048 / 50000 train. data). Loss: 1.663012146949768\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.76it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.63it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.378900\n","Training log: 13 epoch (128 / 50000 train. data). Loss: 1.9333521127700806\n","Training log: 13 epoch (1408 / 50000 train. data). Loss: 1.7229788303375244\n","Training log: 13 epoch (2688 / 50000 train. data). Loss: 1.8084429502487183\n","Training log: 13 epoch (3968 / 50000 train. data). Loss: 1.88572359085083\n","Training log: 13 epoch (5248 / 50000 train. data). Loss: 1.748749852180481\n","Training log: 13 epoch (6528 / 50000 train. data). Loss: 1.7865595817565918\n","Training log: 13 epoch (7808 / 50000 train. data). Loss: 1.952532410621643\n","Training log: 13 epoch (9088 / 50000 train. data). Loss: 1.84647536277771\n","Training log: 13 epoch (10368 / 50000 train. data). Loss: 1.8316450119018555\n","Training log: 13 epoch (11648 / 50000 train. data). Loss: 1.8778679370880127\n","Training log: 13 epoch (12928 / 50000 train. data). Loss: 1.8116897344589233\n","Training log: 13 epoch (14208 / 50000 train. data). Loss: 1.7509098052978516\n","Training log: 13 epoch (15488 / 50000 train. data). Loss: 1.8594354391098022\n","Training log: 13 epoch (16768 / 50000 train. data). Loss: 1.5722404718399048\n","Training log: 13 epoch (18048 / 50000 train. data). Loss: 1.78401517868042\n","Training log: 13 epoch (19328 / 50000 train. data). Loss: 1.7528141736984253\n","Training log: 13 epoch (20608 / 50000 train. data). Loss: 1.8779280185699463\n","Training log: 13 epoch (21888 / 50000 train. data). Loss: 1.6820509433746338\n","Training log: 13 epoch (23168 / 50000 train. data). Loss: 1.6444891691207886\n","Training log: 13 epoch (24448 / 50000 train. data). Loss: 1.80251944065094\n","Training log: 13 epoch (25728 / 50000 train. data). Loss: 1.770986795425415\n","Training log: 13 epoch (27008 / 50000 train. data). Loss: 1.7754464149475098\n","Training log: 13 epoch (28288 / 50000 train. data). Loss: 1.753633737564087\n","Training log: 13 epoch (29568 / 50000 train. data). Loss: 1.8798226118087769\n","Training log: 13 epoch (30848 / 50000 train. data). Loss: 1.6900568008422852\n","Training log: 13 epoch (32128 / 50000 train. data). Loss: 1.7658648490905762\n","Training log: 13 epoch (33408 / 50000 train. data). Loss: 1.815437912940979\n","Training log: 13 epoch (34688 / 50000 train. data). Loss: 1.6205722093582153\n","Training log: 13 epoch (35968 / 50000 train. data). Loss: 1.7202045917510986\n","Training log: 13 epoch (37248 / 50000 train. data). Loss: 1.7620580196380615\n","Training log: 13 epoch (38528 / 50000 train. data). Loss: 1.8476225137710571\n","Training log: 13 epoch (39808 / 50000 train. data). Loss: 1.6876572370529175\n","Training log: 13 epoch (41088 / 50000 train. data). Loss: 1.911926031112671\n","Training log: 13 epoch (42368 / 50000 train. data). Loss: 1.718810796737671\n","Training log: 13 epoch (43648 / 50000 train. data). Loss: 1.5930744409561157\n","Training log: 13 epoch (44928 / 50000 train. data). Loss: 1.813358187675476\n","Training log: 13 epoch (46208 / 50000 train. data). Loss: 1.7885980606079102\n","Training log: 13 epoch (47488 / 50000 train. data). Loss: 1.852925419807434\n","Training log: 13 epoch (48768 / 50000 train. data). Loss: 1.6575719118118286\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 13 epoch (50048 / 50000 train. data). Loss: 1.704064965248108\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.73it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.97it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.388400\n","Training log: 14 epoch (128 / 50000 train. data). Loss: 1.7311595678329468\n","Training log: 14 epoch (1408 / 50000 train. data). Loss: 1.9389142990112305\n","Training log: 14 epoch (2688 / 50000 train. data). Loss: 1.7779061794281006\n","Training log: 14 epoch (3968 / 50000 train. data). Loss: 1.8237656354904175\n","Training log: 14 epoch (5248 / 50000 train. data). Loss: 1.749976634979248\n","Training log: 14 epoch (6528 / 50000 train. data). Loss: 1.6808018684387207\n","Training log: 14 epoch (7808 / 50000 train. data). Loss: 1.7305022478103638\n","Training log: 14 epoch (9088 / 50000 train. data). Loss: 1.871867060661316\n","Training log: 14 epoch (10368 / 50000 train. data). Loss: 1.724829912185669\n","Training log: 14 epoch (11648 / 50000 train. data). Loss: 1.6082942485809326\n","Training log: 14 epoch (12928 / 50000 train. data). Loss: 1.8776607513427734\n","Training log: 14 epoch (14208 / 50000 train. data). Loss: 1.672609567642212\n","Training log: 14 epoch (15488 / 50000 train. data). Loss: 1.841248869895935\n","Training log: 14 epoch (16768 / 50000 train. data). Loss: 1.8515172004699707\n","Training log: 14 epoch (18048 / 50000 train. data). Loss: 1.7475526332855225\n","Training log: 14 epoch (19328 / 50000 train. data). Loss: 1.729138970375061\n","Training log: 14 epoch (20608 / 50000 train. data). Loss: 1.666882872581482\n","Training log: 14 epoch (21888 / 50000 train. data). Loss: 1.774402141571045\n","Training log: 14 epoch (23168 / 50000 train. data). Loss: 1.8347482681274414\n","Training log: 14 epoch (24448 / 50000 train. data). Loss: 1.73239004611969\n","Training log: 14 epoch (25728 / 50000 train. data). Loss: 1.6645478010177612\n","Training log: 14 epoch (27008 / 50000 train. data). Loss: 1.7842378616333008\n","Training log: 14 epoch (28288 / 50000 train. data). Loss: 1.641737937927246\n","Training log: 14 epoch (29568 / 50000 train. data). Loss: 1.787448525428772\n","Training log: 14 epoch (30848 / 50000 train. data). Loss: 1.7598154544830322\n","Training log: 14 epoch (32128 / 50000 train. data). Loss: 1.8114598989486694\n","Training log: 14 epoch (33408 / 50000 train. data). Loss: 1.7647130489349365\n","Training log: 14 epoch (34688 / 50000 train. data). Loss: 1.735806941986084\n","Training log: 14 epoch (35968 / 50000 train. data). Loss: 1.8223153352737427\n","Training log: 14 epoch (37248 / 50000 train. data). Loss: 1.8216108083724976\n","Training log: 14 epoch (38528 / 50000 train. data). Loss: 1.8907196521759033\n","Training log: 14 epoch (39808 / 50000 train. data). Loss: 1.6828054189682007\n","Training log: 14 epoch (41088 / 50000 train. data). Loss: 1.7805660963058472\n","Training log: 14 epoch (42368 / 50000 train. data). Loss: 1.6662802696228027\n","Training log: 14 epoch (43648 / 50000 train. data). Loss: 1.857940435409546\n","Training log: 14 epoch (44928 / 50000 train. data). Loss: 1.7932264804840088\n","Training log: 14 epoch (46208 / 50000 train. data). Loss: 1.793021321296692\n","Training log: 14 epoch (47488 / 50000 train. data). Loss: 1.7564537525177002\n","Training log: 14 epoch (48768 / 50000 train. data). Loss: 1.702425479888916\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 14 epoch (50048 / 50000 train. data). Loss: 1.9780514240264893\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.61it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.02it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.401400\n","Training log: 15 epoch (128 / 50000 train. data). Loss: 1.884389042854309\n","Training log: 15 epoch (1408 / 50000 train. data). Loss: 1.6183702945709229\n","Training log: 15 epoch (2688 / 50000 train. data). Loss: 1.756645679473877\n","Training log: 15 epoch (3968 / 50000 train. data). Loss: 1.7120710611343384\n","Training log: 15 epoch (5248 / 50000 train. data). Loss: 1.707561731338501\n","Training log: 15 epoch (6528 / 50000 train. data). Loss: 1.605128526687622\n","Training log: 15 epoch (7808 / 50000 train. data). Loss: 1.9254192113876343\n","Training log: 15 epoch (9088 / 50000 train. data). Loss: 1.8715291023254395\n","Training log: 15 epoch (10368 / 50000 train. data). Loss: 1.619257926940918\n","Training log: 15 epoch (11648 / 50000 train. data). Loss: 1.8118505477905273\n","Training log: 15 epoch (12928 / 50000 train. data). Loss: 1.6510899066925049\n","Training log: 15 epoch (14208 / 50000 train. data). Loss: 1.750502347946167\n","Training log: 15 epoch (15488 / 50000 train. data). Loss: 1.830543875694275\n","Training log: 15 epoch (16768 / 50000 train. data). Loss: 1.7204867601394653\n","Training log: 15 epoch (18048 / 50000 train. data). Loss: 1.6635946035385132\n","Training log: 15 epoch (19328 / 50000 train. data). Loss: 1.7195945978164673\n","Training log: 15 epoch (20608 / 50000 train. data). Loss: 1.8136614561080933\n","Training log: 15 epoch (21888 / 50000 train. data). Loss: 1.8240225315093994\n","Training log: 15 epoch (23168 / 50000 train. data). Loss: 1.547831416130066\n","Training log: 15 epoch (24448 / 50000 train. data). Loss: 1.7229878902435303\n","Training log: 15 epoch (25728 / 50000 train. data). Loss: 1.6308631896972656\n","Training log: 15 epoch (27008 / 50000 train. data). Loss: 1.7262336015701294\n","Training log: 15 epoch (28288 / 50000 train. data). Loss: 1.8174724578857422\n","Training log: 15 epoch (29568 / 50000 train. data). Loss: 1.5705746412277222\n","Training log: 15 epoch (30848 / 50000 train. data). Loss: 1.9334665536880493\n","Training log: 15 epoch (32128 / 50000 train. data). Loss: 1.739366054534912\n","Training log: 15 epoch (33408 / 50000 train. data). Loss: 1.8091111183166504\n","Training log: 15 epoch (34688 / 50000 train. data). Loss: 1.6950087547302246\n","Training log: 15 epoch (35968 / 50000 train. data). Loss: 1.7457616329193115\n","Training log: 15 epoch (37248 / 50000 train. data). Loss: 1.699752926826477\n","Training log: 15 epoch (38528 / 50000 train. data). Loss: 1.7407547235488892\n","Training log: 15 epoch (39808 / 50000 train. data). Loss: 1.6237943172454834\n","Training log: 15 epoch (41088 / 50000 train. data). Loss: 1.7370390892028809\n","Training log: 15 epoch (42368 / 50000 train. data). Loss: 1.5808089971542358\n","Training log: 15 epoch (43648 / 50000 train. data). Loss: 1.6996817588806152\n","Training log: 15 epoch (44928 / 50000 train. data). Loss: 1.7010931968688965\n","Training log: 15 epoch (46208 / 50000 train. data). Loss: 1.6970279216766357\n","Training log: 15 epoch (47488 / 50000 train. data). Loss: 1.7212347984313965\n","Training log: 15 epoch (48768 / 50000 train. data). Loss: 1.6773160696029663\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 15 epoch (50048 / 50000 train. data). Loss: 1.7272427082061768\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.55it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.405800\n","Training log: 16 epoch (128 / 50000 train. data). Loss: 1.8711737394332886\n","Training log: 16 epoch (1408 / 50000 train. data). Loss: 1.847736120223999\n","Training log: 16 epoch (2688 / 50000 train. data). Loss: 1.8122129440307617\n","Training log: 16 epoch (3968 / 50000 train. data). Loss: 1.7531230449676514\n","Training log: 16 epoch (5248 / 50000 train. data). Loss: 1.7123674154281616\n","Training log: 16 epoch (6528 / 50000 train. data). Loss: 1.7022600173950195\n","Training log: 16 epoch (7808 / 50000 train. data). Loss: 1.7708622217178345\n","Training log: 16 epoch (9088 / 50000 train. data). Loss: 1.8022692203521729\n","Training log: 16 epoch (10368 / 50000 train. data). Loss: 1.7586537599563599\n","Training log: 16 epoch (11648 / 50000 train. data). Loss: 1.9312775135040283\n","Training log: 16 epoch (12928 / 50000 train. data). Loss: 1.8436164855957031\n","Training log: 16 epoch (14208 / 50000 train. data). Loss: 1.6651661396026611\n","Training log: 16 epoch (15488 / 50000 train. data). Loss: 1.717498540878296\n","Training log: 16 epoch (16768 / 50000 train. data). Loss: 1.5465158224105835\n","Training log: 16 epoch (18048 / 50000 train. data). Loss: 1.8265142440795898\n","Training log: 16 epoch (19328 / 50000 train. data). Loss: 1.7925779819488525\n","Training log: 16 epoch (20608 / 50000 train. data). Loss: 1.7240040302276611\n","Training log: 16 epoch (21888 / 50000 train. data). Loss: 1.7372987270355225\n","Training log: 16 epoch (23168 / 50000 train. data). Loss: 1.6521776914596558\n","Training log: 16 epoch (24448 / 50000 train. data). Loss: 1.6983661651611328\n","Training log: 16 epoch (25728 / 50000 train. data). Loss: 1.6750671863555908\n","Training log: 16 epoch (27008 / 50000 train. data). Loss: 1.7126904726028442\n","Training log: 16 epoch (28288 / 50000 train. data). Loss: 1.647395133972168\n","Training log: 16 epoch (29568 / 50000 train. data). Loss: 1.8690037727355957\n","Training log: 16 epoch (30848 / 50000 train. data). Loss: 1.7377095222473145\n","Training log: 16 epoch (32128 / 50000 train. data). Loss: 1.6786350011825562\n","Training log: 16 epoch (33408 / 50000 train. data). Loss: 1.757961392402649\n","Training log: 16 epoch (34688 / 50000 train. data). Loss: 1.6695133447647095\n","Training log: 16 epoch (35968 / 50000 train. data). Loss: 1.7595758438110352\n","Training log: 16 epoch (37248 / 50000 train. data). Loss: 1.7832740545272827\n","Training log: 16 epoch (38528 / 50000 train. data). Loss: 1.759792685508728\n","Training log: 16 epoch (39808 / 50000 train. data). Loss: 1.7170331478118896\n","Training log: 16 epoch (41088 / 50000 train. data). Loss: 1.6246615648269653\n","Training log: 16 epoch (42368 / 50000 train. data). Loss: 1.5339338779449463\n","Training log: 16 epoch (43648 / 50000 train. data). Loss: 1.6632201671600342\n","Training log: 16 epoch (44928 / 50000 train. data). Loss: 1.7097914218902588\n","Training log: 16 epoch (46208 / 50000 train. data). Loss: 1.6505177021026611\n","Training log: 16 epoch (47488 / 50000 train. data). Loss: 1.61388099193573\n","Training log: 16 epoch (48768 / 50000 train. data). Loss: 1.679025650024414\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:09, 38.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 16 epoch (50048 / 50000 train. data). Loss: 1.7548907995224\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.60it/s]\n","100%|██████████| 79/79 [00:02<00:00, 37.17it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.411700\n","Training log: 17 epoch (128 / 50000 train. data). Loss: 1.683200716972351\n","Training log: 17 epoch (1408 / 50000 train. data). Loss: 1.7131253480911255\n","Training log: 17 epoch (2688 / 50000 train. data). Loss: 1.6836988925933838\n","Training log: 17 epoch (3968 / 50000 train. data). Loss: 1.6596931219100952\n","Training log: 17 epoch (5248 / 50000 train. data). Loss: 1.7435908317565918\n","Training log: 17 epoch (6528 / 50000 train. data). Loss: 1.7052257061004639\n","Training log: 17 epoch (7808 / 50000 train. data). Loss: 1.5799392461776733\n","Training log: 17 epoch (9088 / 50000 train. data). Loss: 1.6752114295959473\n","Training log: 17 epoch (10368 / 50000 train. data). Loss: 1.5294629335403442\n","Training log: 17 epoch (11648 / 50000 train. data). Loss: 1.6954758167266846\n","Training log: 17 epoch (12928 / 50000 train. data). Loss: 1.7145564556121826\n","Training log: 17 epoch (14208 / 50000 train. data). Loss: 1.6948734521865845\n","Training log: 17 epoch (15488 / 50000 train. data). Loss: 1.6022272109985352\n","Training log: 17 epoch (16768 / 50000 train. data). Loss: 1.9609054327011108\n","Training log: 17 epoch (18048 / 50000 train. data). Loss: 1.7714285850524902\n","Training log: 17 epoch (19328 / 50000 train. data). Loss: 1.6656652688980103\n","Training log: 17 epoch (20608 / 50000 train. data). Loss: 1.66488516330719\n","Training log: 17 epoch (21888 / 50000 train. data). Loss: 1.5791040658950806\n","Training log: 17 epoch (23168 / 50000 train. data). Loss: 1.7443389892578125\n","Training log: 17 epoch (24448 / 50000 train. data). Loss: 1.7710891962051392\n","Training log: 17 epoch (25728 / 50000 train. data). Loss: 1.7607930898666382\n","Training log: 17 epoch (27008 / 50000 train. data). Loss: 1.690966010093689\n","Training log: 17 epoch (28288 / 50000 train. data). Loss: 1.7635095119476318\n","Training log: 17 epoch (29568 / 50000 train. data). Loss: 1.6263964176177979\n","Training log: 17 epoch (30848 / 50000 train. data). Loss: 1.7446829080581665\n","Training log: 17 epoch (32128 / 50000 train. data). Loss: 1.5585688352584839\n","Training log: 17 epoch (33408 / 50000 train. data). Loss: 1.7139232158660889\n","Training log: 17 epoch (34688 / 50000 train. data). Loss: 1.6588327884674072\n","Training log: 17 epoch (35968 / 50000 train. data). Loss: 1.6611676216125488\n","Training log: 17 epoch (37248 / 50000 train. data). Loss: 1.5655337572097778\n","Training log: 17 epoch (38528 / 50000 train. data). Loss: 1.7259995937347412\n","Training log: 17 epoch (39808 / 50000 train. data). Loss: 1.747968077659607\n","Training log: 17 epoch (41088 / 50000 train. data). Loss: 1.7246754169464111\n","Training log: 17 epoch (42368 / 50000 train. data). Loss: 1.642773985862732\n","Training log: 17 epoch (43648 / 50000 train. data). Loss: 1.6855740547180176\n","Training log: 17 epoch (44928 / 50000 train. data). Loss: 1.6882436275482178\n","Training log: 17 epoch (46208 / 50000 train. data). Loss: 1.6659399271011353\n","Training log: 17 epoch (47488 / 50000 train. data). Loss: 1.5355799198150635\n","Training log: 17 epoch (48768 / 50000 train. data). Loss: 1.660741925239563\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 38.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 17 epoch (50048 / 50000 train. data). Loss: 1.7061131000518799\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.57it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.72it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.421800\n","Training log: 18 epoch (128 / 50000 train. data). Loss: 1.8396098613739014\n","Training log: 18 epoch (1408 / 50000 train. data). Loss: 1.5749634504318237\n","Training log: 18 epoch (2688 / 50000 train. data). Loss: 1.7287827730178833\n","Training log: 18 epoch (3968 / 50000 train. data). Loss: 1.6954772472381592\n","Training log: 18 epoch (5248 / 50000 train. data). Loss: 1.5764120817184448\n","Training log: 18 epoch (6528 / 50000 train. data). Loss: 1.6311876773834229\n","Training log: 18 epoch (7808 / 50000 train. data). Loss: 1.8004558086395264\n","Training log: 18 epoch (9088 / 50000 train. data). Loss: 1.6017141342163086\n","Training log: 18 epoch (10368 / 50000 train. data). Loss: 1.610917329788208\n","Training log: 18 epoch (11648 / 50000 train. data). Loss: 1.6187715530395508\n","Training log: 18 epoch (12928 / 50000 train. data). Loss: 1.6445847749710083\n","Training log: 18 epoch (14208 / 50000 train. data). Loss: 1.736505389213562\n","Training log: 18 epoch (15488 / 50000 train. data). Loss: 1.6820706129074097\n","Training log: 18 epoch (16768 / 50000 train. data). Loss: 1.8345794677734375\n","Training log: 18 epoch (18048 / 50000 train. data). Loss: 1.7253386974334717\n","Training log: 18 epoch (19328 / 50000 train. data). Loss: 1.7837538719177246\n","Training log: 18 epoch (20608 / 50000 train. data). Loss: 1.5905275344848633\n","Training log: 18 epoch (21888 / 50000 train. data). Loss: 1.6506510972976685\n","Training log: 18 epoch (23168 / 50000 train. data). Loss: 1.696426510810852\n","Training log: 18 epoch (24448 / 50000 train. data). Loss: 1.699118733406067\n","Training log: 18 epoch (25728 / 50000 train. data). Loss: 1.7231236696243286\n","Training log: 18 epoch (27008 / 50000 train. data). Loss: 1.622990369796753\n","Training log: 18 epoch (28288 / 50000 train. data). Loss: 1.687886118888855\n","Training log: 18 epoch (29568 / 50000 train. data). Loss: 1.6804251670837402\n","Training log: 18 epoch (30848 / 50000 train. data). Loss: 1.7495757341384888\n","Training log: 18 epoch (32128 / 50000 train. data). Loss: 1.681567668914795\n","Training log: 18 epoch (33408 / 50000 train. data). Loss: 1.617866039276123\n","Training log: 18 epoch (34688 / 50000 train. data). Loss: 1.7125780582427979\n","Training log: 18 epoch (35968 / 50000 train. data). Loss: 1.7979422807693481\n","Training log: 18 epoch (37248 / 50000 train. data). Loss: 1.6625514030456543\n","Training log: 18 epoch (38528 / 50000 train. data). Loss: 1.6713941097259521\n","Training log: 18 epoch (39808 / 50000 train. data). Loss: 1.793414831161499\n","Training log: 18 epoch (41088 / 50000 train. data). Loss: 1.6200225353240967\n","Training log: 18 epoch (42368 / 50000 train. data). Loss: 1.6083481311798096\n","Training log: 18 epoch (43648 / 50000 train. data). Loss: 1.7181278467178345\n","Training log: 18 epoch (44928 / 50000 train. data). Loss: 1.6353102922439575\n","Training log: 18 epoch (46208 / 50000 train. data). Loss: 1.7051713466644287\n","Training log: 18 epoch (47488 / 50000 train. data). Loss: 1.7681156396865845\n","Training log: 18 epoch (48768 / 50000 train. data). Loss: 1.7661648988723755\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 38.13it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 18 epoch (50048 / 50000 train. data). Loss: 1.5496633052825928\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.31it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.80it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.427500\n","Training log: 19 epoch (128 / 50000 train. data). Loss: 1.628454566001892\n","Training log: 19 epoch (1408 / 50000 train. data). Loss: 1.6917533874511719\n","Training log: 19 epoch (2688 / 50000 train. data). Loss: 1.6552971601486206\n","Training log: 19 epoch (3968 / 50000 train. data). Loss: 1.5498239994049072\n","Training log: 19 epoch (5248 / 50000 train. data). Loss: 1.762183427810669\n","Training log: 19 epoch (6528 / 50000 train. data). Loss: 1.6288352012634277\n","Training log: 19 epoch (7808 / 50000 train. data). Loss: 1.7144814729690552\n","Training log: 19 epoch (9088 / 50000 train. data). Loss: 1.524212121963501\n","Training log: 19 epoch (10368 / 50000 train. data). Loss: 1.5559312105178833\n","Training log: 19 epoch (11648 / 50000 train. data). Loss: 1.5703039169311523\n","Training log: 19 epoch (12928 / 50000 train. data). Loss: 1.5287383794784546\n","Training log: 19 epoch (14208 / 50000 train. data). Loss: 1.7304890155792236\n","Training log: 19 epoch (15488 / 50000 train. data). Loss: 1.5620543956756592\n","Training log: 19 epoch (16768 / 50000 train. data). Loss: 1.7629026174545288\n","Training log: 19 epoch (18048 / 50000 train. data). Loss: 1.5625636577606201\n","Training log: 19 epoch (19328 / 50000 train. data). Loss: 1.6536537408828735\n","Training log: 19 epoch (20608 / 50000 train. data). Loss: 1.6949894428253174\n","Training log: 19 epoch (21888 / 50000 train. data). Loss: 1.6036090850830078\n","Training log: 19 epoch (23168 / 50000 train. data). Loss: 1.6133474111557007\n","Training log: 19 epoch (24448 / 50000 train. data). Loss: 1.6780017614364624\n","Training log: 19 epoch (25728 / 50000 train. data). Loss: 1.5871164798736572\n","Training log: 19 epoch (27008 / 50000 train. data). Loss: 1.6509109735488892\n","Training log: 19 epoch (28288 / 50000 train. data). Loss: 1.7253392934799194\n","Training log: 19 epoch (29568 / 50000 train. data). Loss: 1.7298786640167236\n","Training log: 19 epoch (30848 / 50000 train. data). Loss: 1.6722712516784668\n","Training log: 19 epoch (32128 / 50000 train. data). Loss: 1.649471640586853\n","Training log: 19 epoch (33408 / 50000 train. data). Loss: 1.7125517129898071\n","Training log: 19 epoch (34688 / 50000 train. data). Loss: 1.7021796703338623\n","Training log: 19 epoch (35968 / 50000 train. data). Loss: 1.6981295347213745\n","Training log: 19 epoch (37248 / 50000 train. data). Loss: 1.6363821029663086\n","Training log: 19 epoch (38528 / 50000 train. data). Loss: 1.575258731842041\n","Training log: 19 epoch (39808 / 50000 train. data). Loss: 1.7249609231948853\n","Training log: 19 epoch (41088 / 50000 train. data). Loss: 1.6402251720428467\n","Training log: 19 epoch (42368 / 50000 train. data). Loss: 1.7254111766815186\n","Training log: 19 epoch (43648 / 50000 train. data). Loss: 1.6333180665969849\n","Training log: 19 epoch (44928 / 50000 train. data). Loss: 1.6312042474746704\n","Training log: 19 epoch (46208 / 50000 train. data). Loss: 1.7016656398773193\n","Training log: 19 epoch (47488 / 50000 train. data). Loss: 1.6726776361465454\n","Training log: 19 epoch (48768 / 50000 train. data). Loss: 1.5130360126495361\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 33.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 19 epoch (50048 / 50000 train. data). Loss: 1.8043352365493774\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.89it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.20it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.434400\n","Training log: 20 epoch (128 / 50000 train. data). Loss: 1.6351282596588135\n","Training log: 20 epoch (1408 / 50000 train. data). Loss: 1.6387889385223389\n","Training log: 20 epoch (2688 / 50000 train. data). Loss: 1.6583292484283447\n","Training log: 20 epoch (3968 / 50000 train. data). Loss: 1.6188040971755981\n","Training log: 20 epoch (5248 / 50000 train. data). Loss: 1.4817535877227783\n","Training log: 20 epoch (6528 / 50000 train. data). Loss: 1.8601006269454956\n","Training log: 20 epoch (7808 / 50000 train. data). Loss: 1.635941743850708\n","Training log: 20 epoch (9088 / 50000 train. data). Loss: 1.7348535060882568\n","Training log: 20 epoch (10368 / 50000 train. data). Loss: 1.6172116994857788\n","Training log: 20 epoch (11648 / 50000 train. data). Loss: 1.5947684049606323\n","Training log: 20 epoch (12928 / 50000 train. data). Loss: 1.5944410562515259\n","Training log: 20 epoch (14208 / 50000 train. data). Loss: 1.6919962167739868\n","Training log: 20 epoch (15488 / 50000 train. data). Loss: 1.5955016613006592\n","Training log: 20 epoch (16768 / 50000 train. data). Loss: 1.7105175256729126\n","Training log: 20 epoch (18048 / 50000 train. data). Loss: 1.7141722440719604\n","Training log: 20 epoch (19328 / 50000 train. data). Loss: 1.6732381582260132\n","Training log: 20 epoch (20608 / 50000 train. data). Loss: 1.8098695278167725\n","Training log: 20 epoch (21888 / 50000 train. data). Loss: 1.6102720499038696\n","Training log: 20 epoch (23168 / 50000 train. data). Loss: 1.6830421686172485\n","Training log: 20 epoch (24448 / 50000 train. data). Loss: 1.6318966150283813\n","Training log: 20 epoch (25728 / 50000 train. data). Loss: 1.6519687175750732\n","Training log: 20 epoch (27008 / 50000 train. data). Loss: 1.6766393184661865\n","Training log: 20 epoch (28288 / 50000 train. data). Loss: 1.704249620437622\n","Training log: 20 epoch (29568 / 50000 train. data). Loss: 1.7399470806121826\n","Training log: 20 epoch (30848 / 50000 train. data). Loss: 1.5648471117019653\n","Training log: 20 epoch (32128 / 50000 train. data). Loss: 1.8554811477661133\n","Training log: 20 epoch (33408 / 50000 train. data). Loss: 1.5583103895187378\n","Training log: 20 epoch (34688 / 50000 train. data). Loss: 1.7264881134033203\n","Training log: 20 epoch (35968 / 50000 train. data). Loss: 1.5391024351119995\n","Training log: 20 epoch (37248 / 50000 train. data). Loss: 1.5218538045883179\n","Training log: 20 epoch (38528 / 50000 train. data). Loss: 1.6567203998565674\n","Training log: 20 epoch (39808 / 50000 train. data). Loss: 1.5084224939346313\n","Training log: 20 epoch (41088 / 50000 train. data). Loss: 1.7607173919677734\n","Training log: 20 epoch (42368 / 50000 train. data). Loss: 1.703810691833496\n","Training log: 20 epoch (43648 / 50000 train. data). Loss: 1.708983063697815\n","Training log: 20 epoch (44928 / 50000 train. data). Loss: 1.6592439413070679\n","Training log: 20 epoch (46208 / 50000 train. data). Loss: 1.621422529220581\n","Training log: 20 epoch (47488 / 50000 train. data). Loss: 1.7681148052215576\n","Training log: 20 epoch (48768 / 50000 train. data). Loss: 1.5825066566467285\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 34.20it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 20 epoch (50048 / 50000 train. data). Loss: 1.7361911535263062\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.17it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.14it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.444600\n","Training log: 21 epoch (128 / 50000 train. data). Loss: 1.5788618326187134\n","Training log: 21 epoch (1408 / 50000 train. data). Loss: 1.6239246129989624\n","Training log: 21 epoch (2688 / 50000 train. data). Loss: 1.560601830482483\n","Training log: 21 epoch (3968 / 50000 train. data). Loss: 1.6439591646194458\n","Training log: 21 epoch (5248 / 50000 train. data). Loss: 1.5997756719589233\n","Training log: 21 epoch (6528 / 50000 train. data). Loss: 1.5886677503585815\n","Training log: 21 epoch (7808 / 50000 train. data). Loss: 1.766325831413269\n","Training log: 21 epoch (9088 / 50000 train. data). Loss: 1.7167601585388184\n","Training log: 21 epoch (10368 / 50000 train. data). Loss: 1.5216412544250488\n","Training log: 21 epoch (11648 / 50000 train. data). Loss: 1.798631191253662\n","Training log: 21 epoch (12928 / 50000 train. data). Loss: 1.5327874422073364\n","Training log: 21 epoch (14208 / 50000 train. data). Loss: 1.6505303382873535\n","Training log: 21 epoch (15488 / 50000 train. data). Loss: 1.702744483947754\n","Training log: 21 epoch (16768 / 50000 train. data). Loss: 1.7173583507537842\n","Training log: 21 epoch (18048 / 50000 train. data). Loss: 1.557201862335205\n","Training log: 21 epoch (19328 / 50000 train. data). Loss: 1.6795538663864136\n","Training log: 21 epoch (20608 / 50000 train. data). Loss: 1.5269441604614258\n","Training log: 21 epoch (21888 / 50000 train. data). Loss: 1.7061057090759277\n","Training log: 21 epoch (23168 / 50000 train. data). Loss: 1.6834367513656616\n","Training log: 21 epoch (24448 / 50000 train. data). Loss: 1.6797592639923096\n","Training log: 21 epoch (25728 / 50000 train. data). Loss: 1.755460500717163\n","Training log: 21 epoch (27008 / 50000 train. data). Loss: 1.7458531856536865\n","Training log: 21 epoch (28288 / 50000 train. data). Loss: 1.587007761001587\n","Training log: 21 epoch (29568 / 50000 train. data). Loss: 1.790075659751892\n","Training log: 21 epoch (30848 / 50000 train. data). Loss: 1.537020206451416\n","Training log: 21 epoch (32128 / 50000 train. data). Loss: 1.7013508081436157\n","Training log: 21 epoch (33408 / 50000 train. data). Loss: 1.6093854904174805\n","Training log: 21 epoch (34688 / 50000 train. data). Loss: 1.6302423477172852\n","Training log: 21 epoch (35968 / 50000 train. data). Loss: 1.644582748413086\n","Training log: 21 epoch (37248 / 50000 train. data). Loss: 1.6519150733947754\n","Training log: 21 epoch (38528 / 50000 train. data). Loss: 1.610533595085144\n","Training log: 21 epoch (39808 / 50000 train. data). Loss: 1.7244633436203003\n","Training log: 21 epoch (41088 / 50000 train. data). Loss: 1.5139676332473755\n","Training log: 21 epoch (42368 / 50000 train. data). Loss: 1.71883225440979\n","Training log: 21 epoch (43648 / 50000 train. data). Loss: 1.6062815189361572\n","Training log: 21 epoch (44928 / 50000 train. data). Loss: 1.4372966289520264\n","Training log: 21 epoch (46208 / 50000 train. data). Loss: 1.7142518758773804\n","Training log: 21 epoch (47488 / 50000 train. data). Loss: 1.6304186582565308\n","Training log: 21 epoch (48768 / 50000 train. data). Loss: 1.68131422996521\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 34.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 21 epoch (50048 / 50000 train. data). Loss: 1.6036567687988281\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.87it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.23it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.449100\n","Training log: 22 epoch (128 / 50000 train. data). Loss: 1.6094326972961426\n","Training log: 22 epoch (1408 / 50000 train. data). Loss: 1.5221004486083984\n","Training log: 22 epoch (2688 / 50000 train. data). Loss: 1.781780481338501\n","Training log: 22 epoch (3968 / 50000 train. data). Loss: 1.5521873235702515\n","Training log: 22 epoch (5248 / 50000 train. data). Loss: 1.6108812093734741\n","Training log: 22 epoch (6528 / 50000 train. data). Loss: 1.8083000183105469\n","Training log: 22 epoch (7808 / 50000 train. data). Loss: 1.5517622232437134\n","Training log: 22 epoch (9088 / 50000 train. data). Loss: 1.5913938283920288\n","Training log: 22 epoch (10368 / 50000 train. data). Loss: 1.60472571849823\n","Training log: 22 epoch (11648 / 50000 train. data). Loss: 1.6171908378601074\n","Training log: 22 epoch (12928 / 50000 train. data). Loss: 1.6070196628570557\n","Training log: 22 epoch (14208 / 50000 train. data). Loss: 1.4727582931518555\n","Training log: 22 epoch (15488 / 50000 train. data). Loss: 1.4985623359680176\n","Training log: 22 epoch (16768 / 50000 train. data). Loss: 1.6716790199279785\n","Training log: 22 epoch (18048 / 50000 train. data). Loss: 1.6854876279830933\n","Training log: 22 epoch (19328 / 50000 train. data). Loss: 1.8012845516204834\n","Training log: 22 epoch (20608 / 50000 train. data). Loss: 1.6768944263458252\n","Training log: 22 epoch (21888 / 50000 train. data). Loss: 1.6039668321609497\n","Training log: 22 epoch (23168 / 50000 train. data). Loss: 1.6522883176803589\n","Training log: 22 epoch (24448 / 50000 train. data). Loss: 1.6665458679199219\n","Training log: 22 epoch (25728 / 50000 train. data). Loss: 1.741268277168274\n","Training log: 22 epoch (27008 / 50000 train. data). Loss: 1.5370266437530518\n","Training log: 22 epoch (28288 / 50000 train. data). Loss: 1.7252117395401\n","Training log: 22 epoch (29568 / 50000 train. data). Loss: 1.6577647924423218\n","Training log: 22 epoch (30848 / 50000 train. data). Loss: 1.5117318630218506\n","Training log: 22 epoch (32128 / 50000 train. data). Loss: 1.5259555578231812\n","Training log: 22 epoch (33408 / 50000 train. data). Loss: 1.4709129333496094\n","Training log: 22 epoch (34688 / 50000 train. data). Loss: 1.7200546264648438\n","Training log: 22 epoch (35968 / 50000 train. data). Loss: 1.6671541929244995\n","Training log: 22 epoch (37248 / 50000 train. data). Loss: 1.6749908924102783\n","Training log: 22 epoch (38528 / 50000 train. data). Loss: 1.605029821395874\n","Training log: 22 epoch (39808 / 50000 train. data). Loss: 1.656595230102539\n","Training log: 22 epoch (41088 / 50000 train. data). Loss: 1.669447660446167\n","Training log: 22 epoch (42368 / 50000 train. data). Loss: 1.6540144681930542\n","Training log: 22 epoch (43648 / 50000 train. data). Loss: 1.6337746381759644\n","Training log: 22 epoch (44928 / 50000 train. data). Loss: 1.5554804801940918\n","Training log: 22 epoch (46208 / 50000 train. data). Loss: 1.7034680843353271\n","Training log: 22 epoch (47488 / 50000 train. data). Loss: 1.5850770473480225\n","Training log: 22 epoch (48768 / 50000 train. data). Loss: 1.5690573453903198\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 35.13it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 22 epoch (50048 / 50000 train. data). Loss: 1.5710560083389282\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.12it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.94it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.457200\n","Training log: 23 epoch (128 / 50000 train. data). Loss: 1.590543508529663\n","Training log: 23 epoch (1408 / 50000 train. data). Loss: 1.6120911836624146\n","Training log: 23 epoch (2688 / 50000 train. data). Loss: 1.6319875717163086\n","Training log: 23 epoch (3968 / 50000 train. data). Loss: 1.5959073305130005\n","Training log: 23 epoch (5248 / 50000 train. data). Loss: 1.5939851999282837\n","Training log: 23 epoch (6528 / 50000 train. data). Loss: 1.727201223373413\n","Training log: 23 epoch (7808 / 50000 train. data). Loss: 1.6494152545928955\n","Training log: 23 epoch (9088 / 50000 train. data). Loss: 1.5601943731307983\n","Training log: 23 epoch (10368 / 50000 train. data). Loss: 1.6206940412521362\n","Training log: 23 epoch (11648 / 50000 train. data). Loss: 1.4738917350769043\n","Training log: 23 epoch (12928 / 50000 train. data). Loss: 1.681591272354126\n","Training log: 23 epoch (14208 / 50000 train. data). Loss: 1.6741636991500854\n","Training log: 23 epoch (15488 / 50000 train. data). Loss: 1.6042112112045288\n","Training log: 23 epoch (16768 / 50000 train. data). Loss: 1.596553087234497\n","Training log: 23 epoch (18048 / 50000 train. data). Loss: 1.7012653350830078\n","Training log: 23 epoch (19328 / 50000 train. data). Loss: 1.5817067623138428\n","Training log: 23 epoch (20608 / 50000 train. data). Loss: 1.659464716911316\n","Training log: 23 epoch (21888 / 50000 train. data). Loss: 1.7243762016296387\n","Training log: 23 epoch (23168 / 50000 train. data). Loss: 1.5781080722808838\n","Training log: 23 epoch (24448 / 50000 train. data). Loss: 1.6433672904968262\n","Training log: 23 epoch (25728 / 50000 train. data). Loss: 1.5849183797836304\n","Training log: 23 epoch (27008 / 50000 train. data). Loss: 1.6854665279388428\n","Training log: 23 epoch (28288 / 50000 train. data). Loss: 1.6348215341567993\n","Training log: 23 epoch (29568 / 50000 train. data). Loss: 1.7824605703353882\n","Training log: 23 epoch (30848 / 50000 train. data). Loss: 1.6192282438278198\n","Training log: 23 epoch (32128 / 50000 train. data). Loss: 1.5357683897018433\n","Training log: 23 epoch (33408 / 50000 train. data). Loss: 1.6034350395202637\n","Training log: 23 epoch (34688 / 50000 train. data). Loss: 1.4808045625686646\n","Training log: 23 epoch (35968 / 50000 train. data). Loss: 1.5834012031555176\n","Training log: 23 epoch (37248 / 50000 train. data). Loss: 1.603635549545288\n","Training log: 23 epoch (38528 / 50000 train. data). Loss: 1.648101568222046\n","Training log: 23 epoch (39808 / 50000 train. data). Loss: 1.7364184856414795\n","Training log: 23 epoch (41088 / 50000 train. data). Loss: 1.7205724716186523\n","Training log: 23 epoch (42368 / 50000 train. data). Loss: 1.5227049589157104\n","Training log: 23 epoch (43648 / 50000 train. data). Loss: 1.6541084051132202\n","Training log: 23 epoch (44928 / 50000 train. data). Loss: 1.6807163953781128\n","Training log: 23 epoch (46208 / 50000 train. data). Loss: 1.6021350622177124\n","Training log: 23 epoch (47488 / 50000 train. data). Loss: 1.716639757156372\n","Training log: 23 epoch (48768 / 50000 train. data). Loss: 1.6918281316757202\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 23 epoch (50048 / 50000 train. data). Loss: 1.5708024501800537\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.06it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.01it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.458400\n","Training log: 24 epoch (128 / 50000 train. data). Loss: 1.5779237747192383\n","Training log: 24 epoch (1408 / 50000 train. data). Loss: 1.5958173274993896\n","Training log: 24 epoch (2688 / 50000 train. data). Loss: 1.6082361936569214\n","Training log: 24 epoch (3968 / 50000 train. data). Loss: 1.6330429315567017\n","Training log: 24 epoch (5248 / 50000 train. data). Loss: 1.658158779144287\n","Training log: 24 epoch (6528 / 50000 train. data). Loss: 1.5607281923294067\n","Training log: 24 epoch (7808 / 50000 train. data). Loss: 1.6240488290786743\n","Training log: 24 epoch (9088 / 50000 train. data). Loss: 1.5162416696548462\n","Training log: 24 epoch (10368 / 50000 train. data). Loss: 1.5371675491333008\n","Training log: 24 epoch (11648 / 50000 train. data). Loss: 1.7162542343139648\n","Training log: 24 epoch (12928 / 50000 train. data). Loss: 1.6463866233825684\n","Training log: 24 epoch (14208 / 50000 train. data). Loss: 1.523659586906433\n","Training log: 24 epoch (15488 / 50000 train. data). Loss: 1.7123119831085205\n","Training log: 24 epoch (16768 / 50000 train. data). Loss: 1.671500563621521\n","Training log: 24 epoch (18048 / 50000 train. data). Loss: 1.5254982709884644\n","Training log: 24 epoch (19328 / 50000 train. data). Loss: 1.5876109600067139\n","Training log: 24 epoch (20608 / 50000 train. data). Loss: 1.528005838394165\n","Training log: 24 epoch (21888 / 50000 train. data). Loss: 1.5921547412872314\n","Training log: 24 epoch (23168 / 50000 train. data). Loss: 1.559503436088562\n","Training log: 24 epoch (24448 / 50000 train. data). Loss: 1.6248457431793213\n","Training log: 24 epoch (25728 / 50000 train. data). Loss: 1.7258455753326416\n","Training log: 24 epoch (27008 / 50000 train. data). Loss: 1.6890966892242432\n","Training log: 24 epoch (28288 / 50000 train. data). Loss: 1.772129774093628\n","Training log: 24 epoch (29568 / 50000 train. data). Loss: 1.6716184616088867\n","Training log: 24 epoch (30848 / 50000 train. data). Loss: 1.478502631187439\n","Training log: 24 epoch (32128 / 50000 train. data). Loss: 1.4893794059753418\n","Training log: 24 epoch (33408 / 50000 train. data). Loss: 1.6366857290267944\n","Training log: 24 epoch (34688 / 50000 train. data). Loss: 1.619001865386963\n","Training log: 24 epoch (35968 / 50000 train. data). Loss: 1.6330090761184692\n","Training log: 24 epoch (37248 / 50000 train. data). Loss: 1.439855933189392\n","Training log: 24 epoch (38528 / 50000 train. data). Loss: 1.7409865856170654\n","Training log: 24 epoch (39808 / 50000 train. data). Loss: 1.6013904809951782\n","Training log: 24 epoch (41088 / 50000 train. data). Loss: 1.4465389251708984\n","Training log: 24 epoch (42368 / 50000 train. data). Loss: 1.5299071073532104\n","Training log: 24 epoch (43648 / 50000 train. data). Loss: 1.6568232774734497\n","Training log: 24 epoch (44928 / 50000 train. data). Loss: 1.7267953157424927\n","Training log: 24 epoch (46208 / 50000 train. data). Loss: 1.4570293426513672\n","Training log: 24 epoch (47488 / 50000 train. data). Loss: 1.4512040615081787\n","Training log: 24 epoch (48768 / 50000 train. data). Loss: 1.5678763389587402\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.23it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 24 epoch (50048 / 50000 train. data). Loss: 1.7112993001937866\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.23it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.64it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.460800\n","Training log: 25 epoch (128 / 50000 train. data). Loss: 1.501845359802246\n","Training log: 25 epoch (1408 / 50000 train. data). Loss: 1.5038658380508423\n","Training log: 25 epoch (2688 / 50000 train. data). Loss: 1.7690811157226562\n","Training log: 25 epoch (3968 / 50000 train. data). Loss: 1.521921992301941\n","Training log: 25 epoch (5248 / 50000 train. data). Loss: 1.6471905708312988\n","Training log: 25 epoch (6528 / 50000 train. data). Loss: 1.607579231262207\n","Training log: 25 epoch (7808 / 50000 train. data). Loss: 1.5678530931472778\n","Training log: 25 epoch (9088 / 50000 train. data). Loss: 1.6461371183395386\n","Training log: 25 epoch (10368 / 50000 train. data). Loss: 1.6247564554214478\n","Training log: 25 epoch (11648 / 50000 train. data). Loss: 1.652562141418457\n","Training log: 25 epoch (12928 / 50000 train. data). Loss: 1.612884283065796\n","Training log: 25 epoch (14208 / 50000 train. data). Loss: 1.6962398290634155\n","Training log: 25 epoch (15488 / 50000 train. data). Loss: 1.5781713724136353\n","Training log: 25 epoch (16768 / 50000 train. data). Loss: 1.6151126623153687\n","Training log: 25 epoch (18048 / 50000 train. data). Loss: 1.6466683149337769\n","Training log: 25 epoch (19328 / 50000 train. data). Loss: 1.4110215902328491\n","Training log: 25 epoch (20608 / 50000 train. data). Loss: 1.5505281686782837\n","Training log: 25 epoch (21888 / 50000 train. data). Loss: 1.4918196201324463\n","Training log: 25 epoch (23168 / 50000 train. data). Loss: 1.552404522895813\n","Training log: 25 epoch (24448 / 50000 train. data). Loss: 1.695115089416504\n","Training log: 25 epoch (25728 / 50000 train. data). Loss: 1.5059078931808472\n","Training log: 25 epoch (27008 / 50000 train. data). Loss: 1.7377582788467407\n","Training log: 25 epoch (28288 / 50000 train. data). Loss: 1.574114441871643\n","Training log: 25 epoch (29568 / 50000 train. data). Loss: 1.5667859315872192\n","Training log: 25 epoch (30848 / 50000 train. data). Loss: 1.631129264831543\n","Training log: 25 epoch (32128 / 50000 train. data). Loss: 1.599256992340088\n","Training log: 25 epoch (33408 / 50000 train. data). Loss: 1.6312499046325684\n","Training log: 25 epoch (34688 / 50000 train. data). Loss: 1.6161431074142456\n","Training log: 25 epoch (35968 / 50000 train. data). Loss: 1.5880365371704102\n","Training log: 25 epoch (37248 / 50000 train. data). Loss: 1.5673913955688477\n","Training log: 25 epoch (38528 / 50000 train. data). Loss: 1.6235787868499756\n","Training log: 25 epoch (39808 / 50000 train. data). Loss: 1.5032631158828735\n","Training log: 25 epoch (41088 / 50000 train. data). Loss: 1.5061845779418945\n","Training log: 25 epoch (42368 / 50000 train. data). Loss: 1.5963748693466187\n","Training log: 25 epoch (43648 / 50000 train. data). Loss: 1.5710229873657227\n","Training log: 25 epoch (44928 / 50000 train. data). Loss: 1.5522311925888062\n","Training log: 25 epoch (46208 / 50000 train. data). Loss: 1.5245825052261353\n","Training log: 25 epoch (47488 / 50000 train. data). Loss: 1.6690573692321777\n","Training log: 25 epoch (48768 / 50000 train. data). Loss: 1.5793778896331787\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 35.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 25 epoch (50048 / 50000 train. data). Loss: 1.6483581066131592\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.68it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.49it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.467800\n","Training log: 26 epoch (128 / 50000 train. data). Loss: 1.71952223777771\n","Training log: 26 epoch (1408 / 50000 train. data). Loss: 1.7455030679702759\n","Training log: 26 epoch (2688 / 50000 train. data). Loss: 1.6839263439178467\n","Training log: 26 epoch (3968 / 50000 train. data). Loss: 1.6448099613189697\n","Training log: 26 epoch (5248 / 50000 train. data). Loss: 1.6148771047592163\n","Training log: 26 epoch (6528 / 50000 train. data). Loss: 1.7211813926696777\n","Training log: 26 epoch (7808 / 50000 train. data). Loss: 1.5694527626037598\n","Training log: 26 epoch (9088 / 50000 train. data). Loss: 1.5313222408294678\n","Training log: 26 epoch (10368 / 50000 train. data). Loss: 1.5515633821487427\n","Training log: 26 epoch (11648 / 50000 train. data). Loss: 1.7442890405654907\n","Training log: 26 epoch (12928 / 50000 train. data). Loss: 1.5116257667541504\n","Training log: 26 epoch (14208 / 50000 train. data). Loss: 1.6172599792480469\n","Training log: 26 epoch (15488 / 50000 train. data). Loss: 1.5222715139389038\n","Training log: 26 epoch (16768 / 50000 train. data). Loss: 1.587249994277954\n","Training log: 26 epoch (18048 / 50000 train. data). Loss: 1.507122278213501\n","Training log: 26 epoch (19328 / 50000 train. data). Loss: 1.5973650217056274\n","Training log: 26 epoch (20608 / 50000 train. data). Loss: 1.507523536682129\n","Training log: 26 epoch (21888 / 50000 train. data). Loss: 1.589763879776001\n","Training log: 26 epoch (23168 / 50000 train. data). Loss: 1.6532065868377686\n","Training log: 26 epoch (24448 / 50000 train. data). Loss: 1.699828028678894\n","Training log: 26 epoch (25728 / 50000 train. data). Loss: 1.7548224925994873\n","Training log: 26 epoch (27008 / 50000 train. data). Loss: 1.5539191961288452\n","Training log: 26 epoch (28288 / 50000 train. data). Loss: 1.5489004850387573\n","Training log: 26 epoch (29568 / 50000 train. data). Loss: 1.4603930711746216\n","Training log: 26 epoch (30848 / 50000 train. data). Loss: 1.4681767225265503\n","Training log: 26 epoch (32128 / 50000 train. data). Loss: 1.4501062631607056\n","Training log: 26 epoch (33408 / 50000 train. data). Loss: 1.437941074371338\n","Training log: 26 epoch (34688 / 50000 train. data). Loss: 1.634260654449463\n","Training log: 26 epoch (35968 / 50000 train. data). Loss: 1.5178815126419067\n","Training log: 26 epoch (37248 / 50000 train. data). Loss: 1.5423942804336548\n","Training log: 26 epoch (38528 / 50000 train. data). Loss: 1.4888564348220825\n","Training log: 26 epoch (39808 / 50000 train. data). Loss: 1.375486969947815\n","Training log: 26 epoch (41088 / 50000 train. data). Loss: 1.5589089393615723\n","Training log: 26 epoch (42368 / 50000 train. data). Loss: 1.5461176633834839\n","Training log: 26 epoch (43648 / 50000 train. data). Loss: 1.477330207824707\n","Training log: 26 epoch (44928 / 50000 train. data). Loss: 1.5005710124969482\n","Training log: 26 epoch (46208 / 50000 train. data). Loss: 1.6460074186325073\n","Training log: 26 epoch (47488 / 50000 train. data). Loss: 1.5648987293243408\n","Training log: 26 epoch (48768 / 50000 train. data). Loss: 1.5586187839508057\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 26 epoch (50048 / 50000 train. data). Loss: 1.418761968612671\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.69it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.91it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.472800\n","Training log: 27 epoch (128 / 50000 train. data). Loss: 1.3154367208480835\n","Training log: 27 epoch (1408 / 50000 train. data). Loss: 1.6920771598815918\n","Training log: 27 epoch (2688 / 50000 train. data). Loss: 1.6478662490844727\n","Training log: 27 epoch (3968 / 50000 train. data). Loss: 1.5763728618621826\n","Training log: 27 epoch (5248 / 50000 train. data). Loss: 1.5896587371826172\n","Training log: 27 epoch (6528 / 50000 train. data). Loss: 1.639824628829956\n","Training log: 27 epoch (7808 / 50000 train. data). Loss: 1.5919196605682373\n","Training log: 27 epoch (9088 / 50000 train. data). Loss: 1.5018991231918335\n","Training log: 27 epoch (10368 / 50000 train. data). Loss: 1.5528042316436768\n","Training log: 27 epoch (11648 / 50000 train. data). Loss: 1.5579314231872559\n","Training log: 27 epoch (12928 / 50000 train. data). Loss: 1.6015487909317017\n","Training log: 27 epoch (14208 / 50000 train. data). Loss: 1.6145159006118774\n","Training log: 27 epoch (15488 / 50000 train. data). Loss: 1.5753995180130005\n","Training log: 27 epoch (16768 / 50000 train. data). Loss: 1.478140950202942\n","Training log: 27 epoch (18048 / 50000 train. data). Loss: 1.66488516330719\n","Training log: 27 epoch (19328 / 50000 train. data). Loss: 1.696866750717163\n","Training log: 27 epoch (20608 / 50000 train. data). Loss: 1.7554606199264526\n","Training log: 27 epoch (21888 / 50000 train. data). Loss: 1.4498544931411743\n","Training log: 27 epoch (23168 / 50000 train. data). Loss: 1.5870064496994019\n","Training log: 27 epoch (24448 / 50000 train. data). Loss: 1.6067447662353516\n","Training log: 27 epoch (25728 / 50000 train. data). Loss: 1.5648338794708252\n","Training log: 27 epoch (27008 / 50000 train. data). Loss: 1.6305979490280151\n","Training log: 27 epoch (28288 / 50000 train. data). Loss: 1.6865826845169067\n","Training log: 27 epoch (29568 / 50000 train. data). Loss: 1.5326229333877563\n","Training log: 27 epoch (30848 / 50000 train. data). Loss: 1.5118341445922852\n","Training log: 27 epoch (32128 / 50000 train. data). Loss: 1.5474250316619873\n","Training log: 27 epoch (33408 / 50000 train. data). Loss: 1.5852348804473877\n","Training log: 27 epoch (34688 / 50000 train. data). Loss: 1.4712023735046387\n","Training log: 27 epoch (35968 / 50000 train. data). Loss: 1.5624347925186157\n","Training log: 27 epoch (37248 / 50000 train. data). Loss: 1.5493171215057373\n","Training log: 27 epoch (38528 / 50000 train. data). Loss: 1.4525599479675293\n","Training log: 27 epoch (39808 / 50000 train. data). Loss: 1.4156438112258911\n","Training log: 27 epoch (41088 / 50000 train. data). Loss: 1.576712965965271\n","Training log: 27 epoch (42368 / 50000 train. data). Loss: 1.600675106048584\n","Training log: 27 epoch (43648 / 50000 train. data). Loss: 1.5802770853042603\n","Training log: 27 epoch (44928 / 50000 train. data). Loss: 1.764168381690979\n","Training log: 27 epoch (46208 / 50000 train. data). Loss: 1.5922141075134277\n","Training log: 27 epoch (47488 / 50000 train. data). Loss: 1.559261679649353\n","Training log: 27 epoch (48768 / 50000 train. data). Loss: 1.6643760204315186\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 27 epoch (50048 / 50000 train. data). Loss: 1.6863845586776733\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.91it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.21it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.479900\n","Training log: 28 epoch (128 / 50000 train. data). Loss: 1.710431456565857\n","Training log: 28 epoch (1408 / 50000 train. data). Loss: 1.6684447526931763\n","Training log: 28 epoch (2688 / 50000 train. data). Loss: 1.411847710609436\n","Training log: 28 epoch (3968 / 50000 train. data). Loss: 1.5369980335235596\n","Training log: 28 epoch (5248 / 50000 train. data). Loss: 1.610252022743225\n","Training log: 28 epoch (6528 / 50000 train. data). Loss: 1.5611473321914673\n","Training log: 28 epoch (7808 / 50000 train. data). Loss: 1.4996012449264526\n","Training log: 28 epoch (9088 / 50000 train. data). Loss: 1.514952540397644\n","Training log: 28 epoch (10368 / 50000 train. data). Loss: 1.5016294717788696\n","Training log: 28 epoch (11648 / 50000 train. data). Loss: 1.6285051107406616\n","Training log: 28 epoch (12928 / 50000 train. data). Loss: 1.5683608055114746\n","Training log: 28 epoch (14208 / 50000 train. data). Loss: 1.603294014930725\n","Training log: 28 epoch (15488 / 50000 train. data). Loss: 1.4782977104187012\n","Training log: 28 epoch (16768 / 50000 train. data). Loss: 1.5469896793365479\n","Training log: 28 epoch (18048 / 50000 train. data). Loss: 1.5486236810684204\n","Training log: 28 epoch (19328 / 50000 train. data). Loss: 1.5590966939926147\n","Training log: 28 epoch (20608 / 50000 train. data). Loss: 1.3809971809387207\n","Training log: 28 epoch (21888 / 50000 train. data). Loss: 1.5357908010482788\n","Training log: 28 epoch (23168 / 50000 train. data). Loss: 1.6381844282150269\n","Training log: 28 epoch (24448 / 50000 train. data). Loss: 1.5049962997436523\n","Training log: 28 epoch (25728 / 50000 train. data). Loss: 1.6149072647094727\n","Training log: 28 epoch (27008 / 50000 train. data). Loss: 1.5191575288772583\n","Training log: 28 epoch (28288 / 50000 train. data). Loss: 1.65031099319458\n","Training log: 28 epoch (29568 / 50000 train. data). Loss: 1.6183720827102661\n","Training log: 28 epoch (30848 / 50000 train. data). Loss: 1.5233362913131714\n","Training log: 28 epoch (32128 / 50000 train. data). Loss: 1.6193207502365112\n","Training log: 28 epoch (33408 / 50000 train. data). Loss: 1.4810303449630737\n","Training log: 28 epoch (34688 / 50000 train. data). Loss: 1.6545629501342773\n","Training log: 28 epoch (35968 / 50000 train. data). Loss: 1.608197808265686\n","Training log: 28 epoch (37248 / 50000 train. data). Loss: 1.6324623823165894\n","Training log: 28 epoch (38528 / 50000 train. data). Loss: 1.78986656665802\n","Training log: 28 epoch (39808 / 50000 train. data). Loss: 1.5540395975112915\n","Training log: 28 epoch (41088 / 50000 train. data). Loss: 1.5550570487976074\n","Training log: 28 epoch (42368 / 50000 train. data). Loss: 1.5242894887924194\n","Training log: 28 epoch (43648 / 50000 train. data). Loss: 1.6326870918273926\n","Training log: 28 epoch (44928 / 50000 train. data). Loss: 1.464172124862671\n","Training log: 28 epoch (46208 / 50000 train. data). Loss: 1.4753084182739258\n","Training log: 28 epoch (47488 / 50000 train. data). Loss: 1.5298075675964355\n","Training log: 28 epoch (48768 / 50000 train. data). Loss: 1.5216935873031616\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 28 epoch (50048 / 50000 train. data). Loss: 1.3862160444259644\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.49it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.66it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.482900\n","Training log: 29 epoch (128 / 50000 train. data). Loss: 1.5881431102752686\n","Training log: 29 epoch (1408 / 50000 train. data). Loss: 1.565949559211731\n","Training log: 29 epoch (2688 / 50000 train. data). Loss: 1.6060547828674316\n","Training log: 29 epoch (3968 / 50000 train. data). Loss: 1.3251737356185913\n","Training log: 29 epoch (5248 / 50000 train. data). Loss: 1.5331672430038452\n","Training log: 29 epoch (6528 / 50000 train. data). Loss: 1.5913622379302979\n","Training log: 29 epoch (7808 / 50000 train. data). Loss: 1.6626538038253784\n","Training log: 29 epoch (9088 / 50000 train. data). Loss: 1.4966460466384888\n","Training log: 29 epoch (10368 / 50000 train. data). Loss: 1.759508490562439\n","Training log: 29 epoch (11648 / 50000 train. data). Loss: 1.6640619039535522\n","Training log: 29 epoch (12928 / 50000 train. data). Loss: 1.59944486618042\n","Training log: 29 epoch (14208 / 50000 train. data). Loss: 1.6653584241867065\n","Training log: 29 epoch (15488 / 50000 train. data). Loss: 1.5591773986816406\n","Training log: 29 epoch (16768 / 50000 train. data). Loss: 1.7451720237731934\n","Training log: 29 epoch (18048 / 50000 train. data). Loss: 1.4353152513504028\n","Training log: 29 epoch (19328 / 50000 train. data). Loss: 1.6187576055526733\n","Training log: 29 epoch (20608 / 50000 train. data). Loss: 1.6175957918167114\n","Training log: 29 epoch (21888 / 50000 train. data). Loss: 1.613329529762268\n","Training log: 29 epoch (23168 / 50000 train. data). Loss: 1.5477981567382812\n","Training log: 29 epoch (24448 / 50000 train. data). Loss: 1.6889009475708008\n","Training log: 29 epoch (25728 / 50000 train. data). Loss: 1.6026138067245483\n","Training log: 29 epoch (27008 / 50000 train. data). Loss: 1.4414504766464233\n","Training log: 29 epoch (28288 / 50000 train. data). Loss: 1.524117350578308\n","Training log: 29 epoch (29568 / 50000 train. data). Loss: 1.532519817352295\n","Training log: 29 epoch (30848 / 50000 train. data). Loss: 1.564571738243103\n","Training log: 29 epoch (32128 / 50000 train. data). Loss: 1.3730825185775757\n","Training log: 29 epoch (33408 / 50000 train. data). Loss: 1.5622293949127197\n","Training log: 29 epoch (34688 / 50000 train. data). Loss: 1.5300426483154297\n","Training log: 29 epoch (35968 / 50000 train. data). Loss: 1.6430460214614868\n","Training log: 29 epoch (37248 / 50000 train. data). Loss: 1.6278104782104492\n","Training log: 29 epoch (38528 / 50000 train. data). Loss: 1.6559834480285645\n","Training log: 29 epoch (39808 / 50000 train. data). Loss: 1.5360372066497803\n","Training log: 29 epoch (41088 / 50000 train. data). Loss: 1.52638840675354\n","Training log: 29 epoch (42368 / 50000 train. data). Loss: 1.5072903633117676\n","Training log: 29 epoch (43648 / 50000 train. data). Loss: 1.5484058856964111\n","Training log: 29 epoch (44928 / 50000 train. data). Loss: 1.4025335311889648\n","Training log: 29 epoch (46208 / 50000 train. data). Loss: 1.5545834302902222\n","Training log: 29 epoch (47488 / 50000 train. data). Loss: 1.5043888092041016\n","Training log: 29 epoch (48768 / 50000 train. data). Loss: 1.520232081413269\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 33.18it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 29 epoch (50048 / 50000 train. data). Loss: 1.5723562240600586\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.65it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.482100\n","Training log: 30 epoch (128 / 50000 train. data). Loss: 1.3457800149917603\n","Training log: 30 epoch (1408 / 50000 train. data). Loss: 1.4810857772827148\n","Training log: 30 epoch (2688 / 50000 train. data). Loss: 1.560081958770752\n","Training log: 30 epoch (3968 / 50000 train. data). Loss: 1.5947908163070679\n","Training log: 30 epoch (5248 / 50000 train. data). Loss: 1.4612854719161987\n","Training log: 30 epoch (6528 / 50000 train. data). Loss: 1.5954898595809937\n","Training log: 30 epoch (7808 / 50000 train. data). Loss: 1.4177179336547852\n","Training log: 30 epoch (9088 / 50000 train. data). Loss: 1.6489442586898804\n","Training log: 30 epoch (10368 / 50000 train. data). Loss: 1.5585970878601074\n","Training log: 30 epoch (11648 / 50000 train. data). Loss: 1.4583072662353516\n","Training log: 30 epoch (12928 / 50000 train. data). Loss: 1.4417927265167236\n","Training log: 30 epoch (14208 / 50000 train. data). Loss: 1.6008543968200684\n","Training log: 30 epoch (15488 / 50000 train. data). Loss: 1.4690204858779907\n","Training log: 30 epoch (16768 / 50000 train. data). Loss: 1.5240265130996704\n","Training log: 30 epoch (18048 / 50000 train. data). Loss: 1.4852336645126343\n","Training log: 30 epoch (19328 / 50000 train. data). Loss: 1.713053822517395\n","Training log: 30 epoch (20608 / 50000 train. data). Loss: 1.525460124015808\n","Training log: 30 epoch (21888 / 50000 train. data). Loss: 1.576738953590393\n","Training log: 30 epoch (23168 / 50000 train. data). Loss: 1.6155627965927124\n","Training log: 30 epoch (24448 / 50000 train. data). Loss: 1.5436999797821045\n","Training log: 30 epoch (25728 / 50000 train. data). Loss: 1.5630944967269897\n","Training log: 30 epoch (27008 / 50000 train. data). Loss: 1.5457751750946045\n","Training log: 30 epoch (28288 / 50000 train. data). Loss: 1.4712966680526733\n","Training log: 30 epoch (29568 / 50000 train. data). Loss: 1.562811017036438\n","Training log: 30 epoch (30848 / 50000 train. data). Loss: 1.3976166248321533\n","Training log: 30 epoch (32128 / 50000 train. data). Loss: 1.312025547027588\n","Training log: 30 epoch (33408 / 50000 train. data). Loss: 1.742226004600525\n","Training log: 30 epoch (34688 / 50000 train. data). Loss: 1.4805989265441895\n","Training log: 30 epoch (35968 / 50000 train. data). Loss: 1.4763121604919434\n","Training log: 30 epoch (37248 / 50000 train. data). Loss: 1.5916447639465332\n","Training log: 30 epoch (38528 / 50000 train. data). Loss: 1.6325340270996094\n","Training log: 30 epoch (39808 / 50000 train. data). Loss: 1.6027843952178955\n","Training log: 30 epoch (41088 / 50000 train. data). Loss: 1.553366780281067\n","Training log: 30 epoch (42368 / 50000 train. data). Loss: 1.5997581481933594\n","Training log: 30 epoch (43648 / 50000 train. data). Loss: 1.548691749572754\n","Training log: 30 epoch (44928 / 50000 train. data). Loss: 1.6506160497665405\n","Training log: 30 epoch (46208 / 50000 train. data). Loss: 1.6312018632888794\n","Training log: 30 epoch (47488 / 50000 train. data). Loss: 1.4810667037963867\n","Training log: 30 epoch (48768 / 50000 train. data). Loss: 1.4280471801757812\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 30 epoch (50048 / 50000 train. data). Loss: 1.4565027952194214\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.46it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.81it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.489700\n","Training log: 31 epoch (128 / 50000 train. data). Loss: 1.5335360765457153\n","Training log: 31 epoch (1408 / 50000 train. data). Loss: 1.552530288696289\n","Training log: 31 epoch (2688 / 50000 train. data). Loss: 1.4597338438034058\n","Training log: 31 epoch (3968 / 50000 train. data). Loss: 1.4617726802825928\n","Training log: 31 epoch (5248 / 50000 train. data). Loss: 1.5363383293151855\n","Training log: 31 epoch (6528 / 50000 train. data). Loss: 1.6499212980270386\n","Training log: 31 epoch (7808 / 50000 train. data). Loss: 1.6129813194274902\n","Training log: 31 epoch (9088 / 50000 train. data). Loss: 1.5110750198364258\n","Training log: 31 epoch (10368 / 50000 train. data). Loss: 1.5124051570892334\n","Training log: 31 epoch (11648 / 50000 train. data). Loss: 1.4381383657455444\n","Training log: 31 epoch (12928 / 50000 train. data). Loss: 1.4912865161895752\n","Training log: 31 epoch (14208 / 50000 train. data). Loss: 1.5896698236465454\n","Training log: 31 epoch (15488 / 50000 train. data). Loss: 1.6717737913131714\n","Training log: 31 epoch (16768 / 50000 train. data). Loss: 1.5093903541564941\n","Training log: 31 epoch (18048 / 50000 train. data). Loss: 1.4943761825561523\n","Training log: 31 epoch (19328 / 50000 train. data). Loss: 1.6627933979034424\n","Training log: 31 epoch (20608 / 50000 train. data). Loss: 1.3936896324157715\n","Training log: 31 epoch (21888 / 50000 train. data). Loss: 1.4242662191390991\n","Training log: 31 epoch (23168 / 50000 train. data). Loss: 1.7049994468688965\n","Training log: 31 epoch (24448 / 50000 train. data). Loss: 1.530202031135559\n","Training log: 31 epoch (25728 / 50000 train. data). Loss: 1.5157842636108398\n","Training log: 31 epoch (27008 / 50000 train. data). Loss: 1.4996461868286133\n","Training log: 31 epoch (28288 / 50000 train. data). Loss: 1.4469244480133057\n","Training log: 31 epoch (29568 / 50000 train. data). Loss: 1.6507285833358765\n","Training log: 31 epoch (30848 / 50000 train. data). Loss: 1.5905479192733765\n","Training log: 31 epoch (32128 / 50000 train. data). Loss: 1.585856318473816\n","Training log: 31 epoch (33408 / 50000 train. data). Loss: 1.5444903373718262\n","Training log: 31 epoch (34688 / 50000 train. data). Loss: 1.5490858554840088\n","Training log: 31 epoch (35968 / 50000 train. data). Loss: 1.474360466003418\n","Training log: 31 epoch (37248 / 50000 train. data). Loss: 1.5644611120224\n","Training log: 31 epoch (38528 / 50000 train. data). Loss: 1.4892287254333496\n","Training log: 31 epoch (39808 / 50000 train. data). Loss: 1.5162220001220703\n","Training log: 31 epoch (41088 / 50000 train. data). Loss: 1.633602261543274\n","Training log: 31 epoch (42368 / 50000 train. data). Loss: 1.5317081212997437\n","Training log: 31 epoch (43648 / 50000 train. data). Loss: 1.567482352256775\n","Training log: 31 epoch (44928 / 50000 train. data). Loss: 1.4567620754241943\n","Training log: 31 epoch (46208 / 50000 train. data). Loss: 1.476762294769287\n","Training log: 31 epoch (47488 / 50000 train. data). Loss: 1.4164003133773804\n","Training log: 31 epoch (48768 / 50000 train. data). Loss: 1.6561869382858276\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 31 epoch (50048 / 50000 train. data). Loss: 1.5496015548706055\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.21it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.51it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.488200\n","Training log: 32 epoch (128 / 50000 train. data). Loss: 1.4202648401260376\n","Training log: 32 epoch (1408 / 50000 train. data). Loss: 1.3908751010894775\n","Training log: 32 epoch (2688 / 50000 train. data). Loss: 1.4260687828063965\n","Training log: 32 epoch (3968 / 50000 train. data). Loss: 1.5323359966278076\n","Training log: 32 epoch (5248 / 50000 train. data). Loss: 1.498988151550293\n","Training log: 32 epoch (6528 / 50000 train. data). Loss: 1.4057509899139404\n","Training log: 32 epoch (7808 / 50000 train. data). Loss: 1.5913289785385132\n","Training log: 32 epoch (9088 / 50000 train. data). Loss: 1.695833444595337\n","Training log: 32 epoch (10368 / 50000 train. data). Loss: 1.5475581884384155\n","Training log: 32 epoch (11648 / 50000 train. data). Loss: 1.638350486755371\n","Training log: 32 epoch (12928 / 50000 train. data). Loss: 1.6469297409057617\n","Training log: 32 epoch (14208 / 50000 train. data). Loss: 1.4328773021697998\n","Training log: 32 epoch (15488 / 50000 train. data). Loss: 1.5961270332336426\n","Training log: 32 epoch (16768 / 50000 train. data). Loss: 1.4451266527175903\n","Training log: 32 epoch (18048 / 50000 train. data). Loss: 1.5654702186584473\n","Training log: 32 epoch (19328 / 50000 train. data). Loss: 1.5724600553512573\n","Training log: 32 epoch (20608 / 50000 train. data). Loss: 1.492171287536621\n","Training log: 32 epoch (21888 / 50000 train. data). Loss: 1.5809530019760132\n","Training log: 32 epoch (23168 / 50000 train. data). Loss: 1.5413854122161865\n","Training log: 32 epoch (24448 / 50000 train. data). Loss: 1.5388633012771606\n","Training log: 32 epoch (25728 / 50000 train. data). Loss: 1.4835487604141235\n","Training log: 32 epoch (27008 / 50000 train. data). Loss: 1.6446131467819214\n","Training log: 32 epoch (28288 / 50000 train. data). Loss: 1.5127190351486206\n","Training log: 32 epoch (29568 / 50000 train. data). Loss: 1.4422754049301147\n","Training log: 32 epoch (30848 / 50000 train. data). Loss: 1.522904396057129\n","Training log: 32 epoch (32128 / 50000 train. data). Loss: 1.668668270111084\n","Training log: 32 epoch (33408 / 50000 train. data). Loss: 1.6205240488052368\n","Training log: 32 epoch (34688 / 50000 train. data). Loss: 1.5002950429916382\n","Training log: 32 epoch (35968 / 50000 train. data). Loss: 1.4333391189575195\n","Training log: 32 epoch (37248 / 50000 train. data). Loss: 1.501610517501831\n","Training log: 32 epoch (38528 / 50000 train. data). Loss: 1.583700180053711\n","Training log: 32 epoch (39808 / 50000 train. data). Loss: 1.4308935403823853\n","Training log: 32 epoch (41088 / 50000 train. data). Loss: 1.6139518022537231\n","Training log: 32 epoch (42368 / 50000 train. data). Loss: 1.4878734350204468\n","Training log: 32 epoch (43648 / 50000 train. data). Loss: 1.4638429880142212\n","Training log: 32 epoch (44928 / 50000 train. data). Loss: 1.4407027959823608\n","Training log: 32 epoch (46208 / 50000 train. data). Loss: 1.5363750457763672\n","Training log: 32 epoch (47488 / 50000 train. data). Loss: 1.4726941585540771\n","Training log: 32 epoch (48768 / 50000 train. data). Loss: 1.5504568815231323\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 32 epoch (50048 / 50000 train. data). Loss: 1.6377185583114624\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.37it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.15it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.492200\n","Training log: 33 epoch (128 / 50000 train. data). Loss: 1.531400442123413\n","Training log: 33 epoch (1408 / 50000 train. data). Loss: 1.4848181009292603\n","Training log: 33 epoch (2688 / 50000 train. data). Loss: 1.4739525318145752\n","Training log: 33 epoch (3968 / 50000 train. data). Loss: 1.5740288496017456\n","Training log: 33 epoch (5248 / 50000 train. data). Loss: 1.4522660970687866\n","Training log: 33 epoch (6528 / 50000 train. data). Loss: 1.4332644939422607\n","Training log: 33 epoch (7808 / 50000 train. data). Loss: 1.5560829639434814\n","Training log: 33 epoch (9088 / 50000 train. data). Loss: 1.5718870162963867\n","Training log: 33 epoch (10368 / 50000 train. data). Loss: 1.4046785831451416\n","Training log: 33 epoch (11648 / 50000 train. data). Loss: 1.4021090269088745\n","Training log: 33 epoch (12928 / 50000 train. data). Loss: 1.394376277923584\n","Training log: 33 epoch (14208 / 50000 train. data). Loss: 1.5048140287399292\n","Training log: 33 epoch (15488 / 50000 train. data). Loss: 1.6220123767852783\n","Training log: 33 epoch (16768 / 50000 train. data). Loss: 1.503814697265625\n","Training log: 33 epoch (18048 / 50000 train. data). Loss: 1.550905704498291\n","Training log: 33 epoch (19328 / 50000 train. data). Loss: 1.5454260110855103\n","Training log: 33 epoch (20608 / 50000 train. data). Loss: 1.4989598989486694\n","Training log: 33 epoch (21888 / 50000 train. data). Loss: 1.4491424560546875\n","Training log: 33 epoch (23168 / 50000 train. data). Loss: 1.3988122940063477\n","Training log: 33 epoch (24448 / 50000 train. data). Loss: 1.4556770324707031\n","Training log: 33 epoch (25728 / 50000 train. data). Loss: 1.509345293045044\n","Training log: 33 epoch (27008 / 50000 train. data). Loss: 1.4918572902679443\n","Training log: 33 epoch (28288 / 50000 train. data). Loss: 1.5375285148620605\n","Training log: 33 epoch (29568 / 50000 train. data). Loss: 1.4370040893554688\n","Training log: 33 epoch (30848 / 50000 train. data). Loss: 1.4778857231140137\n","Training log: 33 epoch (32128 / 50000 train. data). Loss: 1.4160629510879517\n","Training log: 33 epoch (33408 / 50000 train. data). Loss: 1.5531368255615234\n","Training log: 33 epoch (34688 / 50000 train. data). Loss: 1.6572343111038208\n","Training log: 33 epoch (35968 / 50000 train. data). Loss: 1.4341671466827393\n","Training log: 33 epoch (37248 / 50000 train. data). Loss: 1.5790313482284546\n","Training log: 33 epoch (38528 / 50000 train. data). Loss: 1.4289096593856812\n","Training log: 33 epoch (39808 / 50000 train. data). Loss: 1.6662185192108154\n","Training log: 33 epoch (41088 / 50000 train. data). Loss: 1.6136236190795898\n","Training log: 33 epoch (42368 / 50000 train. data). Loss: 1.513772964477539\n","Training log: 33 epoch (43648 / 50000 train. data). Loss: 1.4444897174835205\n","Training log: 33 epoch (44928 / 50000 train. data). Loss: 1.5648857355117798\n","Training log: 33 epoch (46208 / 50000 train. data). Loss: 1.6138100624084473\n","Training log: 33 epoch (47488 / 50000 train. data). Loss: 1.4663764238357544\n","Training log: 33 epoch (48768 / 50000 train. data). Loss: 1.425613522529602\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 33 epoch (50048 / 50000 train. data). Loss: 1.6652072668075562\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.81it/s]\n","100%|██████████| 79/79 [00:02<00:00, 32.26it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.493900\n","Training log: 34 epoch (128 / 50000 train. data). Loss: 1.656224012374878\n","Training log: 34 epoch (1408 / 50000 train. data). Loss: 1.584256887435913\n","Training log: 34 epoch (2688 / 50000 train. data). Loss: 1.5448150634765625\n","Training log: 34 epoch (3968 / 50000 train. data). Loss: 1.4731929302215576\n","Training log: 34 epoch (5248 / 50000 train. data). Loss: 1.7226603031158447\n","Training log: 34 epoch (6528 / 50000 train. data). Loss: 1.6719591617584229\n","Training log: 34 epoch (7808 / 50000 train. data). Loss: 1.4168612957000732\n","Training log: 34 epoch (9088 / 50000 train. data). Loss: 1.556614875793457\n","Training log: 34 epoch (10368 / 50000 train. data). Loss: 1.6577657461166382\n","Training log: 34 epoch (11648 / 50000 train. data). Loss: 1.5133442878723145\n","Training log: 34 epoch (12928 / 50000 train. data). Loss: 1.4334018230438232\n","Training log: 34 epoch (14208 / 50000 train. data). Loss: 1.7565510272979736\n","Training log: 34 epoch (15488 / 50000 train. data). Loss: 1.6355677843093872\n","Training log: 34 epoch (16768 / 50000 train. data). Loss: 1.5654715299606323\n","Training log: 34 epoch (18048 / 50000 train. data). Loss: 1.4291925430297852\n","Training log: 34 epoch (19328 / 50000 train. data). Loss: 1.4130996465682983\n","Training log: 34 epoch (20608 / 50000 train. data). Loss: 1.5374993085861206\n","Training log: 34 epoch (21888 / 50000 train. data). Loss: 1.5683116912841797\n","Training log: 34 epoch (23168 / 50000 train. data). Loss: 1.5448386669158936\n","Training log: 34 epoch (24448 / 50000 train. data). Loss: 1.6774556636810303\n","Training log: 34 epoch (25728 / 50000 train. data). Loss: 1.6062990427017212\n","Training log: 34 epoch (27008 / 50000 train. data). Loss: 1.3563679456710815\n","Training log: 34 epoch (28288 / 50000 train. data). Loss: 1.5707073211669922\n","Training log: 34 epoch (29568 / 50000 train. data). Loss: 1.5243775844573975\n","Training log: 34 epoch (30848 / 50000 train. data). Loss: 1.3623552322387695\n","Training log: 34 epoch (32128 / 50000 train. data). Loss: 1.5431703329086304\n","Training log: 34 epoch (33408 / 50000 train. data). Loss: 1.5386595726013184\n","Training log: 34 epoch (34688 / 50000 train. data). Loss: 1.5340490341186523\n","Training log: 34 epoch (35968 / 50000 train. data). Loss: 1.4973078966140747\n","Training log: 34 epoch (37248 / 50000 train. data). Loss: 1.4329041242599487\n","Training log: 34 epoch (38528 / 50000 train. data). Loss: 1.5215567350387573\n","Training log: 34 epoch (39808 / 50000 train. data). Loss: 1.5198054313659668\n","Training log: 34 epoch (41088 / 50000 train. data). Loss: 1.4940063953399658\n","Training log: 34 epoch (42368 / 50000 train. data). Loss: 1.480486512184143\n","Training log: 34 epoch (43648 / 50000 train. data). Loss: 1.5670331716537476\n","Training log: 34 epoch (44928 / 50000 train. data). Loss: 1.3745442628860474\n","Training log: 34 epoch (46208 / 50000 train. data). Loss: 1.5628217458724976\n","Training log: 34 epoch (47488 / 50000 train. data). Loss: 1.5239534378051758\n","Training log: 34 epoch (48768 / 50000 train. data). Loss: 1.5823389291763306\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.45it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 34 epoch (50048 / 50000 train. data). Loss: 1.444128155708313\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.31it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.70it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.499700\n","Training log: 35 epoch (128 / 50000 train. data). Loss: 1.5602796077728271\n","Training log: 35 epoch (1408 / 50000 train. data). Loss: 1.4782624244689941\n","Training log: 35 epoch (2688 / 50000 train. data). Loss: 1.402587890625\n","Training log: 35 epoch (3968 / 50000 train. data). Loss: 1.5045939683914185\n","Training log: 35 epoch (5248 / 50000 train. data). Loss: 1.6218502521514893\n","Training log: 35 epoch (6528 / 50000 train. data). Loss: 1.4196165800094604\n","Training log: 35 epoch (7808 / 50000 train. data). Loss: 1.4198123216629028\n","Training log: 35 epoch (9088 / 50000 train. data). Loss: 1.5289459228515625\n","Training log: 35 epoch (10368 / 50000 train. data). Loss: 1.439871907234192\n","Training log: 35 epoch (11648 / 50000 train. data). Loss: 1.640337586402893\n","Training log: 35 epoch (12928 / 50000 train. data). Loss: 1.4598913192749023\n","Training log: 35 epoch (14208 / 50000 train. data). Loss: 1.3904751539230347\n","Training log: 35 epoch (15488 / 50000 train. data). Loss: 1.3822370767593384\n","Training log: 35 epoch (16768 / 50000 train. data). Loss: 1.5727628469467163\n","Training log: 35 epoch (18048 / 50000 train. data). Loss: 1.452148199081421\n","Training log: 35 epoch (19328 / 50000 train. data). Loss: 1.6285734176635742\n","Training log: 35 epoch (20608 / 50000 train. data). Loss: 1.517096996307373\n","Training log: 35 epoch (21888 / 50000 train. data). Loss: 1.5041232109069824\n","Training log: 35 epoch (23168 / 50000 train. data). Loss: 1.5848513841629028\n","Training log: 35 epoch (24448 / 50000 train. data). Loss: 1.4413024187088013\n","Training log: 35 epoch (25728 / 50000 train. data). Loss: 1.4752413034439087\n","Training log: 35 epoch (27008 / 50000 train. data). Loss: 1.5809011459350586\n","Training log: 35 epoch (28288 / 50000 train. data). Loss: 1.562950849533081\n","Training log: 35 epoch (29568 / 50000 train. data). Loss: 1.43846595287323\n","Training log: 35 epoch (30848 / 50000 train. data). Loss: 1.4772017002105713\n","Training log: 35 epoch (32128 / 50000 train. data). Loss: 1.4913671016693115\n","Training log: 35 epoch (33408 / 50000 train. data). Loss: 1.510611891746521\n","Training log: 35 epoch (34688 / 50000 train. data). Loss: 1.5257457494735718\n","Training log: 35 epoch (35968 / 50000 train. data). Loss: 1.5965341329574585\n","Training log: 35 epoch (37248 / 50000 train. data). Loss: 1.5055813789367676\n","Training log: 35 epoch (38528 / 50000 train. data). Loss: 1.5921355485916138\n","Training log: 35 epoch (39808 / 50000 train. data). Loss: 1.5918006896972656\n","Training log: 35 epoch (41088 / 50000 train. data). Loss: 1.4889204502105713\n","Training log: 35 epoch (42368 / 50000 train. data). Loss: 1.4302303791046143\n","Training log: 35 epoch (43648 / 50000 train. data). Loss: 1.5693880319595337\n","Training log: 35 epoch (44928 / 50000 train. data). Loss: 1.4587323665618896\n","Training log: 35 epoch (46208 / 50000 train. data). Loss: 1.4864803552627563\n","Training log: 35 epoch (47488 / 50000 train. data). Loss: 1.5064226388931274\n","Training log: 35 epoch (48768 / 50000 train. data). Loss: 1.403642177581787\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:12, 30.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 35 epoch (50048 / 50000 train. data). Loss: 1.3359172344207764\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.79it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.30it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.500200\n","Training log: 36 epoch (128 / 50000 train. data). Loss: 1.7606867551803589\n","Training log: 36 epoch (1408 / 50000 train. data). Loss: 1.3104056119918823\n","Training log: 36 epoch (2688 / 50000 train. data). Loss: 1.4800242185592651\n","Training log: 36 epoch (3968 / 50000 train. data). Loss: 1.5606664419174194\n","Training log: 36 epoch (5248 / 50000 train. data). Loss: 1.4974820613861084\n","Training log: 36 epoch (6528 / 50000 train. data). Loss: 1.6988850831985474\n","Training log: 36 epoch (7808 / 50000 train. data). Loss: 1.337783932685852\n","Training log: 36 epoch (9088 / 50000 train. data). Loss: 1.496949315071106\n","Training log: 36 epoch (10368 / 50000 train. data). Loss: 1.4518917798995972\n","Training log: 36 epoch (11648 / 50000 train. data). Loss: 1.39667809009552\n","Training log: 36 epoch (12928 / 50000 train. data). Loss: 1.598608374595642\n","Training log: 36 epoch (14208 / 50000 train. data). Loss: 1.628145456314087\n","Training log: 36 epoch (15488 / 50000 train. data). Loss: 1.468846082687378\n","Training log: 36 epoch (16768 / 50000 train. data). Loss: 1.441209316253662\n","Training log: 36 epoch (18048 / 50000 train. data). Loss: 1.527854561805725\n","Training log: 36 epoch (19328 / 50000 train. data). Loss: 1.3117077350616455\n","Training log: 36 epoch (20608 / 50000 train. data). Loss: 1.3305349349975586\n","Training log: 36 epoch (21888 / 50000 train. data). Loss: 1.4922699928283691\n","Training log: 36 epoch (23168 / 50000 train. data). Loss: 1.5034949779510498\n","Training log: 36 epoch (24448 / 50000 train. data). Loss: 1.5360488891601562\n","Training log: 36 epoch (25728 / 50000 train. data). Loss: 1.3579987287521362\n","Training log: 36 epoch (27008 / 50000 train. data). Loss: 1.4049874544143677\n","Training log: 36 epoch (28288 / 50000 train. data). Loss: 1.4972628355026245\n","Training log: 36 epoch (29568 / 50000 train. data). Loss: 1.487499475479126\n","Training log: 36 epoch (30848 / 50000 train. data). Loss: 1.54934561252594\n","Training log: 36 epoch (32128 / 50000 train. data). Loss: 1.3846570253372192\n","Training log: 36 epoch (33408 / 50000 train. data). Loss: 1.6026723384857178\n","Training log: 36 epoch (34688 / 50000 train. data). Loss: 1.5955872535705566\n","Training log: 36 epoch (35968 / 50000 train. data). Loss: 1.5626015663146973\n","Training log: 36 epoch (37248 / 50000 train. data). Loss: 1.5449680089950562\n","Training log: 36 epoch (38528 / 50000 train. data). Loss: 1.550264596939087\n","Training log: 36 epoch (39808 / 50000 train. data). Loss: 1.477986454963684\n","Training log: 36 epoch (41088 / 50000 train. data). Loss: 1.531638264656067\n","Training log: 36 epoch (42368 / 50000 train. data). Loss: 1.3619976043701172\n","Training log: 36 epoch (43648 / 50000 train. data). Loss: 1.483580470085144\n","Training log: 36 epoch (44928 / 50000 train. data). Loss: 1.5007944107055664\n","Training log: 36 epoch (46208 / 50000 train. data). Loss: 1.4213347434997559\n","Training log: 36 epoch (47488 / 50000 train. data). Loss: 1.5207500457763672\n","Training log: 36 epoch (48768 / 50000 train. data). Loss: 1.5059574842453003\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.59it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 36 epoch (50048 / 50000 train. data). Loss: 1.6090490818023682\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.18it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.23it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.507100\n","Training log: 37 epoch (128 / 50000 train. data). Loss: 1.5261890888214111\n","Training log: 37 epoch (1408 / 50000 train. data). Loss: 1.3995686769485474\n","Training log: 37 epoch (2688 / 50000 train. data). Loss: 1.421755313873291\n","Training log: 37 epoch (3968 / 50000 train. data). Loss: 1.5585086345672607\n","Training log: 37 epoch (5248 / 50000 train. data). Loss: 1.563583254814148\n","Training log: 37 epoch (6528 / 50000 train. data). Loss: 1.3874297142028809\n","Training log: 37 epoch (7808 / 50000 train. data). Loss: 1.3768140077590942\n","Training log: 37 epoch (9088 / 50000 train. data). Loss: 1.665827751159668\n","Training log: 37 epoch (10368 / 50000 train. data). Loss: 1.3956395387649536\n","Training log: 37 epoch (11648 / 50000 train. data). Loss: 1.5647600889205933\n","Training log: 37 epoch (12928 / 50000 train. data). Loss: 1.565384030342102\n","Training log: 37 epoch (14208 / 50000 train. data). Loss: 1.4664632081985474\n","Training log: 37 epoch (15488 / 50000 train. data). Loss: 1.5522066354751587\n","Training log: 37 epoch (16768 / 50000 train. data). Loss: 1.5109028816223145\n","Training log: 37 epoch (18048 / 50000 train. data). Loss: 1.6790205240249634\n","Training log: 37 epoch (19328 / 50000 train. data). Loss: 1.5948073863983154\n","Training log: 37 epoch (20608 / 50000 train. data). Loss: 1.5125787258148193\n","Training log: 37 epoch (21888 / 50000 train. data). Loss: 1.3726564645767212\n","Training log: 37 epoch (23168 / 50000 train. data). Loss: 1.6272625923156738\n","Training log: 37 epoch (24448 / 50000 train. data). Loss: 1.4976376295089722\n","Training log: 37 epoch (25728 / 50000 train. data). Loss: 1.5322821140289307\n","Training log: 37 epoch (27008 / 50000 train. data). Loss: 1.5027580261230469\n","Training log: 37 epoch (28288 / 50000 train. data). Loss: 1.4570178985595703\n","Training log: 37 epoch (29568 / 50000 train. data). Loss: 1.4544535875320435\n","Training log: 37 epoch (30848 / 50000 train. data). Loss: 1.445556640625\n","Training log: 37 epoch (32128 / 50000 train. data). Loss: 1.4342782497406006\n","Training log: 37 epoch (33408 / 50000 train. data). Loss: 1.556270718574524\n","Training log: 37 epoch (34688 / 50000 train. data). Loss: 1.6105984449386597\n","Training log: 37 epoch (35968 / 50000 train. data). Loss: 1.451304316520691\n","Training log: 37 epoch (37248 / 50000 train. data). Loss: 1.4711631536483765\n","Training log: 37 epoch (38528 / 50000 train. data). Loss: 1.592922329902649\n","Training log: 37 epoch (39808 / 50000 train. data). Loss: 1.5255666971206665\n","Training log: 37 epoch (41088 / 50000 train. data). Loss: 1.4398058652877808\n","Training log: 37 epoch (42368 / 50000 train. data). Loss: 1.5410267114639282\n","Training log: 37 epoch (43648 / 50000 train. data). Loss: 1.555208444595337\n","Training log: 37 epoch (44928 / 50000 train. data). Loss: 1.4905366897583008\n","Training log: 37 epoch (46208 / 50000 train. data). Loss: 1.4228781461715698\n","Training log: 37 epoch (47488 / 50000 train. data). Loss: 1.530282735824585\n","Training log: 37 epoch (48768 / 50000 train. data). Loss: 1.4645719528198242\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 37 epoch (50048 / 50000 train. data). Loss: 1.520878791809082\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.20it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.31it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.506000\n","Training log: 38 epoch (128 / 50000 train. data). Loss: 1.3826242685317993\n","Training log: 38 epoch (1408 / 50000 train. data). Loss: 1.295277714729309\n","Training log: 38 epoch (2688 / 50000 train. data). Loss: 1.500937819480896\n","Training log: 38 epoch (3968 / 50000 train. data). Loss: 1.4281752109527588\n","Training log: 38 epoch (5248 / 50000 train. data). Loss: 1.430342674255371\n","Training log: 38 epoch (6528 / 50000 train. data). Loss: 1.389694333076477\n","Training log: 38 epoch (7808 / 50000 train. data). Loss: 1.4873316287994385\n","Training log: 38 epoch (9088 / 50000 train. data). Loss: 1.354062557220459\n","Training log: 38 epoch (10368 / 50000 train. data). Loss: 1.7658289670944214\n","Training log: 38 epoch (11648 / 50000 train. data). Loss: 1.4947383403778076\n","Training log: 38 epoch (12928 / 50000 train. data). Loss: 1.4436273574829102\n","Training log: 38 epoch (14208 / 50000 train. data). Loss: 1.5579501390457153\n","Training log: 38 epoch (15488 / 50000 train. data). Loss: 1.601217269897461\n","Training log: 38 epoch (16768 / 50000 train. data). Loss: 1.3918474912643433\n","Training log: 38 epoch (18048 / 50000 train. data). Loss: 1.4784584045410156\n","Training log: 38 epoch (19328 / 50000 train. data). Loss: 1.3630669116973877\n","Training log: 38 epoch (20608 / 50000 train. data). Loss: 1.441011667251587\n","Training log: 38 epoch (21888 / 50000 train. data). Loss: 1.4706124067306519\n","Training log: 38 epoch (23168 / 50000 train. data). Loss: 1.4541881084442139\n","Training log: 38 epoch (24448 / 50000 train. data). Loss: 1.6117125749588013\n","Training log: 38 epoch (25728 / 50000 train. data). Loss: 1.5258616209030151\n","Training log: 38 epoch (27008 / 50000 train. data). Loss: 1.4329560995101929\n","Training log: 38 epoch (28288 / 50000 train. data). Loss: 1.5633022785186768\n","Training log: 38 epoch (29568 / 50000 train. data). Loss: 1.4661592245101929\n","Training log: 38 epoch (30848 / 50000 train. data). Loss: 1.5827937126159668\n","Training log: 38 epoch (32128 / 50000 train. data). Loss: 1.3658628463745117\n","Training log: 38 epoch (33408 / 50000 train. data). Loss: 1.4321702718734741\n","Training log: 38 epoch (34688 / 50000 train. data). Loss: 1.306239128112793\n","Training log: 38 epoch (35968 / 50000 train. data). Loss: 1.4787392616271973\n","Training log: 38 epoch (37248 / 50000 train. data). Loss: 1.4049097299575806\n","Training log: 38 epoch (38528 / 50000 train. data). Loss: 1.329260230064392\n","Training log: 38 epoch (39808 / 50000 train. data). Loss: 1.4675601720809937\n","Training log: 38 epoch (41088 / 50000 train. data). Loss: 1.459146499633789\n","Training log: 38 epoch (42368 / 50000 train. data). Loss: 1.4914199113845825\n","Training log: 38 epoch (43648 / 50000 train. data). Loss: 1.573958158493042\n","Training log: 38 epoch (44928 / 50000 train. data). Loss: 1.4664379358291626\n","Training log: 38 epoch (46208 / 50000 train. data). Loss: 1.4218499660491943\n","Training log: 38 epoch (47488 / 50000 train. data). Loss: 1.5021158456802368\n","Training log: 38 epoch (48768 / 50000 train. data). Loss: 1.572460412979126\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 33.42it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 38 epoch (50048 / 50000 train. data). Loss: 1.3170205354690552\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 32.79it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.19it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.506900\n","Training log: 39 epoch (128 / 50000 train. data). Loss: 1.5577555894851685\n","Training log: 39 epoch (1408 / 50000 train. data). Loss: 1.4717979431152344\n","Training log: 39 epoch (2688 / 50000 train. data). Loss: 1.2947627305984497\n","Training log: 39 epoch (3968 / 50000 train. data). Loss: 1.3615599870681763\n","Training log: 39 epoch (5248 / 50000 train. data). Loss: 1.5526238679885864\n","Training log: 39 epoch (6528 / 50000 train. data). Loss: 1.4976739883422852\n","Training log: 39 epoch (7808 / 50000 train. data). Loss: 1.4959574937820435\n","Training log: 39 epoch (9088 / 50000 train. data). Loss: 1.3803082704544067\n","Training log: 39 epoch (10368 / 50000 train. data). Loss: 1.3382072448730469\n","Training log: 39 epoch (11648 / 50000 train. data). Loss: 1.4895577430725098\n","Training log: 39 epoch (12928 / 50000 train. data). Loss: 1.444911003112793\n","Training log: 39 epoch (14208 / 50000 train. data). Loss: 1.3948367834091187\n","Training log: 39 epoch (15488 / 50000 train. data). Loss: 1.5398238897323608\n","Training log: 39 epoch (16768 / 50000 train. data). Loss: 1.4458675384521484\n","Training log: 39 epoch (18048 / 50000 train. data). Loss: 1.402147650718689\n","Training log: 39 epoch (19328 / 50000 train. data). Loss: 1.4930311441421509\n","Training log: 39 epoch (20608 / 50000 train. data). Loss: 1.50132155418396\n","Training log: 39 epoch (21888 / 50000 train. data). Loss: 1.428848385810852\n","Training log: 39 epoch (23168 / 50000 train. data). Loss: 1.5791107416152954\n","Training log: 39 epoch (24448 / 50000 train. data). Loss: 1.5904072523117065\n","Training log: 39 epoch (25728 / 50000 train. data). Loss: 1.5386347770690918\n","Training log: 39 epoch (27008 / 50000 train. data). Loss: 1.4643677473068237\n","Training log: 39 epoch (28288 / 50000 train. data). Loss: 1.3008092641830444\n","Training log: 39 epoch (29568 / 50000 train. data). Loss: 1.7001770734786987\n","Training log: 39 epoch (30848 / 50000 train. data). Loss: 1.5014785528182983\n","Training log: 39 epoch (32128 / 50000 train. data). Loss: 1.5032432079315186\n","Training log: 39 epoch (33408 / 50000 train. data). Loss: 1.5401018857955933\n","Training log: 39 epoch (34688 / 50000 train. data). Loss: 1.5084418058395386\n","Training log: 39 epoch (35968 / 50000 train. data). Loss: 1.5989744663238525\n","Training log: 39 epoch (37248 / 50000 train. data). Loss: 1.3403401374816895\n","Training log: 39 epoch (38528 / 50000 train. data). Loss: 1.5226465463638306\n","Training log: 39 epoch (39808 / 50000 train. data). Loss: 1.6257455348968506\n","Training log: 39 epoch (41088 / 50000 train. data). Loss: 1.3600549697875977\n","Training log: 39 epoch (42368 / 50000 train. data). Loss: 1.5817179679870605\n","Training log: 39 epoch (43648 / 50000 train. data). Loss: 1.4959192276000977\n","Training log: 39 epoch (44928 / 50000 train. data). Loss: 1.3869858980178833\n","Training log: 39 epoch (46208 / 50000 train. data). Loss: 1.5553290843963623\n","Training log: 39 epoch (47488 / 50000 train. data). Loss: 1.6386075019836426\n","Training log: 39 epoch (48768 / 50000 train. data). Loss: 1.3685842752456665\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 39 epoch (50048 / 50000 train. data). Loss: 1.5941567420959473\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.18it/s]\n","100%|██████████| 79/79 [00:02<00:00, 32.91it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.513100\n","Training log: 40 epoch (128 / 50000 train. data). Loss: 1.4386779069900513\n","Training log: 40 epoch (1408 / 50000 train. data). Loss: 1.448973298072815\n","Training log: 40 epoch (2688 / 50000 train. data). Loss: 1.4447213411331177\n","Training log: 40 epoch (3968 / 50000 train. data). Loss: 1.3948866128921509\n","Training log: 40 epoch (5248 / 50000 train. data). Loss: 1.544663667678833\n","Training log: 40 epoch (6528 / 50000 train. data). Loss: 1.4150595664978027\n","Training log: 40 epoch (7808 / 50000 train. data). Loss: 1.3604416847229004\n","Training log: 40 epoch (9088 / 50000 train. data). Loss: 1.6193459033966064\n","Training log: 40 epoch (10368 / 50000 train. data). Loss: 1.2821264266967773\n","Training log: 40 epoch (11648 / 50000 train. data). Loss: 1.5399107933044434\n","Training log: 40 epoch (12928 / 50000 train. data). Loss: 1.607654333114624\n","Training log: 40 epoch (14208 / 50000 train. data). Loss: 1.489072561264038\n","Training log: 40 epoch (15488 / 50000 train. data). Loss: 1.493571400642395\n","Training log: 40 epoch (16768 / 50000 train. data). Loss: 1.533922553062439\n","Training log: 40 epoch (18048 / 50000 train. data). Loss: 1.515089988708496\n","Training log: 40 epoch (19328 / 50000 train. data). Loss: 1.6279010772705078\n","Training log: 40 epoch (20608 / 50000 train. data). Loss: 1.454700231552124\n","Training log: 40 epoch (21888 / 50000 train. data). Loss: 1.4304784536361694\n","Training log: 40 epoch (23168 / 50000 train. data). Loss: 1.5968482494354248\n","Training log: 40 epoch (24448 / 50000 train. data). Loss: 1.4198954105377197\n","Training log: 40 epoch (25728 / 50000 train. data). Loss: 1.460898756980896\n","Training log: 40 epoch (27008 / 50000 train. data). Loss: 1.3829432725906372\n","Training log: 40 epoch (28288 / 50000 train. data). Loss: 1.7223491668701172\n","Training log: 40 epoch (29568 / 50000 train. data). Loss: 1.4110525846481323\n","Training log: 40 epoch (30848 / 50000 train. data). Loss: 1.507596492767334\n","Training log: 40 epoch (32128 / 50000 train. data). Loss: 1.581210970878601\n","Training log: 40 epoch (33408 / 50000 train. data). Loss: 1.5619558095932007\n","Training log: 40 epoch (34688 / 50000 train. data). Loss: 1.5144144296646118\n","Training log: 40 epoch (35968 / 50000 train. data). Loss: 1.5918567180633545\n","Training log: 40 epoch (37248 / 50000 train. data). Loss: 1.5616480112075806\n","Training log: 40 epoch (38528 / 50000 train. data). Loss: 1.4147992134094238\n","Training log: 40 epoch (39808 / 50000 train. data). Loss: 1.473089575767517\n","Training log: 40 epoch (41088 / 50000 train. data). Loss: 1.6017333269119263\n","Training log: 40 epoch (42368 / 50000 train. data). Loss: 1.7633020877838135\n","Training log: 40 epoch (43648 / 50000 train. data). Loss: 1.454329490661621\n","Training log: 40 epoch (44928 / 50000 train. data). Loss: 1.4036030769348145\n","Training log: 40 epoch (46208 / 50000 train. data). Loss: 1.5073275566101074\n","Training log: 40 epoch (47488 / 50000 train. data). Loss: 1.5284994840621948\n","Training log: 40 epoch (48768 / 50000 train. data). Loss: 1.7115789651870728\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 40 epoch (50048 / 50000 train. data). Loss: 1.3342853784561157\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.98it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.63it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.516400\n","Training log: 41 epoch (128 / 50000 train. data). Loss: 1.6344666481018066\n","Training log: 41 epoch (1408 / 50000 train. data). Loss: 1.4906165599822998\n","Training log: 41 epoch (2688 / 50000 train. data). Loss: 1.3234045505523682\n","Training log: 41 epoch (3968 / 50000 train. data). Loss: 1.6868828535079956\n","Training log: 41 epoch (5248 / 50000 train. data). Loss: 1.3262698650360107\n","Training log: 41 epoch (6528 / 50000 train. data). Loss: 1.5365993976593018\n","Training log: 41 epoch (7808 / 50000 train. data). Loss: 1.4292972087860107\n","Training log: 41 epoch (9088 / 50000 train. data). Loss: 1.4786789417266846\n","Training log: 41 epoch (10368 / 50000 train. data). Loss: 1.4541770219802856\n","Training log: 41 epoch (11648 / 50000 train. data). Loss: 1.5502053499221802\n","Training log: 41 epoch (12928 / 50000 train. data). Loss: 1.5284298658370972\n","Training log: 41 epoch (14208 / 50000 train. data). Loss: 1.4957165718078613\n","Training log: 41 epoch (15488 / 50000 train. data). Loss: 1.3628202676773071\n","Training log: 41 epoch (16768 / 50000 train. data). Loss: 1.5529996156692505\n","Training log: 41 epoch (18048 / 50000 train. data). Loss: 1.5592005252838135\n","Training log: 41 epoch (19328 / 50000 train. data). Loss: 1.4545356035232544\n","Training log: 41 epoch (20608 / 50000 train. data). Loss: 1.4875128269195557\n","Training log: 41 epoch (21888 / 50000 train. data). Loss: 1.7351335287094116\n","Training log: 41 epoch (23168 / 50000 train. data). Loss: 1.4441156387329102\n","Training log: 41 epoch (24448 / 50000 train. data). Loss: 1.3793246746063232\n","Training log: 41 epoch (25728 / 50000 train. data). Loss: 1.4319953918457031\n","Training log: 41 epoch (27008 / 50000 train. data). Loss: 1.450297474861145\n","Training log: 41 epoch (28288 / 50000 train. data). Loss: 1.4105067253112793\n","Training log: 41 epoch (29568 / 50000 train. data). Loss: 1.5135605335235596\n","Training log: 41 epoch (30848 / 50000 train. data). Loss: 1.3845512866973877\n","Training log: 41 epoch (32128 / 50000 train. data). Loss: 1.4590922594070435\n","Training log: 41 epoch (33408 / 50000 train. data). Loss: 1.5319572687149048\n","Training log: 41 epoch (34688 / 50000 train. data). Loss: 1.4195294380187988\n","Training log: 41 epoch (35968 / 50000 train. data). Loss: 1.5315909385681152\n","Training log: 41 epoch (37248 / 50000 train. data). Loss: 1.4411448240280151\n","Training log: 41 epoch (38528 / 50000 train. data). Loss: 1.4450767040252686\n","Training log: 41 epoch (39808 / 50000 train. data). Loss: 1.486521601676941\n","Training log: 41 epoch (41088 / 50000 train. data). Loss: 1.5880104303359985\n","Training log: 41 epoch (42368 / 50000 train. data). Loss: 1.3550881147384644\n","Training log: 41 epoch (43648 / 50000 train. data). Loss: 1.6061254739761353\n","Training log: 41 epoch (44928 / 50000 train. data). Loss: 1.5650534629821777\n","Training log: 41 epoch (46208 / 50000 train. data). Loss: 1.3404487371444702\n","Training log: 41 epoch (47488 / 50000 train. data). Loss: 1.416287899017334\n","Training log: 41 epoch (48768 / 50000 train. data). Loss: 1.630124807357788\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 41 epoch (50048 / 50000 train. data). Loss: 1.5765396356582642\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.17it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.91it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.516200\n","Training log: 42 epoch (128 / 50000 train. data). Loss: 1.482106328010559\n","Training log: 42 epoch (1408 / 50000 train. data). Loss: 1.6933543682098389\n","Training log: 42 epoch (2688 / 50000 train. data). Loss: 1.4810113906860352\n","Training log: 42 epoch (3968 / 50000 train. data). Loss: 1.4354203939437866\n","Training log: 42 epoch (5248 / 50000 train. data). Loss: 1.3217264413833618\n","Training log: 42 epoch (6528 / 50000 train. data). Loss: 1.3137528896331787\n","Training log: 42 epoch (7808 / 50000 train. data). Loss: 1.5657262802124023\n","Training log: 42 epoch (9088 / 50000 train. data). Loss: 1.5494508743286133\n","Training log: 42 epoch (10368 / 50000 train. data). Loss: 1.3784704208374023\n","Training log: 42 epoch (11648 / 50000 train. data). Loss: 1.5467661619186401\n","Training log: 42 epoch (12928 / 50000 train. data). Loss: 1.60512375831604\n","Training log: 42 epoch (14208 / 50000 train. data). Loss: 1.5994319915771484\n","Training log: 42 epoch (15488 / 50000 train. data). Loss: 1.5512921810150146\n","Training log: 42 epoch (16768 / 50000 train. data). Loss: 1.42330002784729\n","Training log: 42 epoch (18048 / 50000 train. data). Loss: 1.3764076232910156\n","Training log: 42 epoch (19328 / 50000 train. data). Loss: 1.2868763208389282\n","Training log: 42 epoch (20608 / 50000 train. data). Loss: 1.416696548461914\n","Training log: 42 epoch (21888 / 50000 train. data). Loss: 1.5479518175125122\n","Training log: 42 epoch (23168 / 50000 train. data). Loss: 1.4235200881958008\n","Training log: 42 epoch (24448 / 50000 train. data). Loss: 1.2498751878738403\n","Training log: 42 epoch (25728 / 50000 train. data). Loss: 1.5139570236206055\n","Training log: 42 epoch (27008 / 50000 train. data). Loss: 1.4235320091247559\n","Training log: 42 epoch (28288 / 50000 train. data). Loss: 1.3382865190505981\n","Training log: 42 epoch (29568 / 50000 train. data). Loss: 1.46885085105896\n","Training log: 42 epoch (30848 / 50000 train. data). Loss: 1.3496124744415283\n","Training log: 42 epoch (32128 / 50000 train. data). Loss: 1.432051420211792\n","Training log: 42 epoch (33408 / 50000 train. data). Loss: 1.5257034301757812\n","Training log: 42 epoch (34688 / 50000 train. data). Loss: 1.4324698448181152\n","Training log: 42 epoch (35968 / 50000 train. data). Loss: 1.3543145656585693\n","Training log: 42 epoch (37248 / 50000 train. data). Loss: 1.5048754215240479\n","Training log: 42 epoch (38528 / 50000 train. data). Loss: 1.2953577041625977\n","Training log: 42 epoch (39808 / 50000 train. data). Loss: 1.4065452814102173\n","Training log: 42 epoch (41088 / 50000 train. data). Loss: 1.5569106340408325\n","Training log: 42 epoch (42368 / 50000 train. data). Loss: 1.439775824546814\n","Training log: 42 epoch (43648 / 50000 train. data). Loss: 1.4560686349868774\n","Training log: 42 epoch (44928 / 50000 train. data). Loss: 1.5476512908935547\n","Training log: 42 epoch (46208 / 50000 train. data). Loss: 1.574887990951538\n","Training log: 42 epoch (47488 / 50000 train. data). Loss: 1.5108994245529175\n","Training log: 42 epoch (48768 / 50000 train. data). Loss: 1.5266860723495483\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 42 epoch (50048 / 50000 train. data). Loss: 1.4412291049957275\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.80it/s]\n","100%|██████████| 79/79 [00:02<00:00, 32.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.517100\n","Training log: 43 epoch (128 / 50000 train. data). Loss: 1.4574506282806396\n","Training log: 43 epoch (1408 / 50000 train. data). Loss: 1.478499174118042\n","Training log: 43 epoch (2688 / 50000 train. data). Loss: 1.5125483274459839\n","Training log: 43 epoch (3968 / 50000 train. data). Loss: 1.4489219188690186\n","Training log: 43 epoch (5248 / 50000 train. data). Loss: 1.4686498641967773\n","Training log: 43 epoch (6528 / 50000 train. data). Loss: 1.4709311723709106\n","Training log: 43 epoch (7808 / 50000 train. data). Loss: 1.5098241567611694\n","Training log: 43 epoch (9088 / 50000 train. data). Loss: 1.465224266052246\n","Training log: 43 epoch (10368 / 50000 train. data). Loss: 1.5207972526550293\n","Training log: 43 epoch (11648 / 50000 train. data). Loss: 1.6292787790298462\n","Training log: 43 epoch (12928 / 50000 train. data). Loss: 1.5245647430419922\n","Training log: 43 epoch (14208 / 50000 train. data). Loss: 1.3183470964431763\n","Training log: 43 epoch (15488 / 50000 train. data). Loss: 1.3791773319244385\n","Training log: 43 epoch (16768 / 50000 train. data). Loss: 1.6141629219055176\n","Training log: 43 epoch (18048 / 50000 train. data). Loss: 1.4002530574798584\n","Training log: 43 epoch (19328 / 50000 train. data). Loss: 1.401073694229126\n","Training log: 43 epoch (20608 / 50000 train. data). Loss: 1.2834237813949585\n","Training log: 43 epoch (21888 / 50000 train. data). Loss: 1.417228102684021\n","Training log: 43 epoch (23168 / 50000 train. data). Loss: 1.237025499343872\n","Training log: 43 epoch (24448 / 50000 train. data). Loss: 1.5181125402450562\n","Training log: 43 epoch (25728 / 50000 train. data). Loss: 1.4698679447174072\n","Training log: 43 epoch (27008 / 50000 train. data). Loss: 1.495288372039795\n","Training log: 43 epoch (28288 / 50000 train. data). Loss: 1.4359408617019653\n","Training log: 43 epoch (29568 / 50000 train. data). Loss: 1.3664050102233887\n","Training log: 43 epoch (30848 / 50000 train. data). Loss: 1.4868733882904053\n","Training log: 43 epoch (32128 / 50000 train. data). Loss: 1.3165067434310913\n","Training log: 43 epoch (33408 / 50000 train. data). Loss: 1.5098965167999268\n","Training log: 43 epoch (34688 / 50000 train. data). Loss: 1.39947509765625\n","Training log: 43 epoch (35968 / 50000 train. data). Loss: 1.3850898742675781\n","Training log: 43 epoch (37248 / 50000 train. data). Loss: 1.414156436920166\n","Training log: 43 epoch (38528 / 50000 train. data). Loss: 1.4593926668167114\n","Training log: 43 epoch (39808 / 50000 train. data). Loss: 1.438511848449707\n","Training log: 43 epoch (41088 / 50000 train. data). Loss: 1.3627232313156128\n","Training log: 43 epoch (42368 / 50000 train. data). Loss: 1.3515112400054932\n","Training log: 43 epoch (43648 / 50000 train. data). Loss: 1.516880989074707\n","Training log: 43 epoch (44928 / 50000 train. data). Loss: 1.4431860446929932\n","Training log: 43 epoch (46208 / 50000 train. data). Loss: 1.5311434268951416\n","Training log: 43 epoch (47488 / 50000 train. data). Loss: 1.5343986749649048\n","Training log: 43 epoch (48768 / 50000 train. data). Loss: 1.3723900318145752\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 34.37it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 43 epoch (50048 / 50000 train. data). Loss: 1.502638578414917\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.79it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.98it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.521100\n","Training log: 44 epoch (128 / 50000 train. data). Loss: 1.4528506994247437\n","Training log: 44 epoch (1408 / 50000 train. data). Loss: 1.659493088722229\n","Training log: 44 epoch (2688 / 50000 train. data). Loss: 1.510166883468628\n","Training log: 44 epoch (3968 / 50000 train. data). Loss: 1.4485054016113281\n","Training log: 44 epoch (5248 / 50000 train. data). Loss: 1.3679662942886353\n","Training log: 44 epoch (6528 / 50000 train. data). Loss: 1.3374873399734497\n","Training log: 44 epoch (7808 / 50000 train. data). Loss: 1.5159589052200317\n","Training log: 44 epoch (9088 / 50000 train. data). Loss: 1.4827264547348022\n","Training log: 44 epoch (10368 / 50000 train. data). Loss: 1.5344516038894653\n","Training log: 44 epoch (11648 / 50000 train. data). Loss: 1.4080407619476318\n","Training log: 44 epoch (12928 / 50000 train. data). Loss: 1.476530909538269\n","Training log: 44 epoch (14208 / 50000 train. data). Loss: 1.4660005569458008\n","Training log: 44 epoch (15488 / 50000 train. data). Loss: 1.4998607635498047\n","Training log: 44 epoch (16768 / 50000 train. data). Loss: 1.5943840742111206\n","Training log: 44 epoch (18048 / 50000 train. data). Loss: 1.3848532438278198\n","Training log: 44 epoch (19328 / 50000 train. data). Loss: 1.4810583591461182\n","Training log: 44 epoch (20608 / 50000 train. data). Loss: 1.2874127626419067\n","Training log: 44 epoch (21888 / 50000 train. data). Loss: 1.2301273345947266\n","Training log: 44 epoch (23168 / 50000 train. data). Loss: 1.5248074531555176\n","Training log: 44 epoch (24448 / 50000 train. data). Loss: 1.5025492906570435\n","Training log: 44 epoch (25728 / 50000 train. data). Loss: 1.482253074645996\n","Training log: 44 epoch (27008 / 50000 train. data). Loss: 1.3947193622589111\n","Training log: 44 epoch (28288 / 50000 train. data). Loss: 1.352036714553833\n","Training log: 44 epoch (29568 / 50000 train. data). Loss: 1.305119276046753\n","Training log: 44 epoch (30848 / 50000 train. data). Loss: 1.4879512786865234\n","Training log: 44 epoch (32128 / 50000 train. data). Loss: 1.5647114515304565\n","Training log: 44 epoch (33408 / 50000 train. data). Loss: 1.403602123260498\n","Training log: 44 epoch (34688 / 50000 train. data). Loss: 1.34320068359375\n","Training log: 44 epoch (35968 / 50000 train. data). Loss: 1.3674530982971191\n","Training log: 44 epoch (37248 / 50000 train. data). Loss: 1.5777380466461182\n","Training log: 44 epoch (38528 / 50000 train. data). Loss: 1.4184101819992065\n","Training log: 44 epoch (39808 / 50000 train. data). Loss: 1.628737211227417\n","Training log: 44 epoch (41088 / 50000 train. data). Loss: 1.412453293800354\n","Training log: 44 epoch (42368 / 50000 train. data). Loss: 1.4321318864822388\n","Training log: 44 epoch (43648 / 50000 train. data). Loss: 1.5816442966461182\n","Training log: 44 epoch (44928 / 50000 train. data). Loss: 1.3687653541564941\n","Training log: 44 epoch (46208 / 50000 train. data). Loss: 1.3618322610855103\n","Training log: 44 epoch (47488 / 50000 train. data). Loss: 1.3779352903366089\n","Training log: 44 epoch (48768 / 50000 train. data). Loss: 1.4729948043823242\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 44 epoch (50048 / 50000 train. data). Loss: 1.410532832145691\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.64it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.16it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.524100\n","Training log: 45 epoch (128 / 50000 train. data). Loss: 1.415771245956421\n","Training log: 45 epoch (1408 / 50000 train. data). Loss: 1.3476147651672363\n","Training log: 45 epoch (2688 / 50000 train. data). Loss: 1.4725978374481201\n","Training log: 45 epoch (3968 / 50000 train. data). Loss: 1.337058186531067\n","Training log: 45 epoch (5248 / 50000 train. data). Loss: 1.6536794900894165\n","Training log: 45 epoch (6528 / 50000 train. data). Loss: 1.4675945043563843\n","Training log: 45 epoch (7808 / 50000 train. data). Loss: 1.3741525411605835\n","Training log: 45 epoch (9088 / 50000 train. data). Loss: 1.4932352304458618\n","Training log: 45 epoch (10368 / 50000 train. data). Loss: 1.3871344327926636\n","Training log: 45 epoch (11648 / 50000 train. data). Loss: 1.4063353538513184\n","Training log: 45 epoch (12928 / 50000 train. data). Loss: 1.4205082654953003\n","Training log: 45 epoch (14208 / 50000 train. data). Loss: 1.3407694101333618\n","Training log: 45 epoch (15488 / 50000 train. data). Loss: 1.5662318468093872\n","Training log: 45 epoch (16768 / 50000 train. data). Loss: 1.5567108392715454\n","Training log: 45 epoch (18048 / 50000 train. data). Loss: 1.5172364711761475\n","Training log: 45 epoch (19328 / 50000 train. data). Loss: 1.4376029968261719\n","Training log: 45 epoch (20608 / 50000 train. data). Loss: 1.3701101541519165\n","Training log: 45 epoch (21888 / 50000 train. data). Loss: 1.4509625434875488\n","Training log: 45 epoch (23168 / 50000 train. data). Loss: 1.514080286026001\n","Training log: 45 epoch (24448 / 50000 train. data). Loss: 1.4091342687606812\n","Training log: 45 epoch (25728 / 50000 train. data). Loss: 1.4970725774765015\n","Training log: 45 epoch (27008 / 50000 train. data). Loss: 1.462781310081482\n","Training log: 45 epoch (28288 / 50000 train. data). Loss: 1.435624361038208\n","Training log: 45 epoch (29568 / 50000 train. data). Loss: 1.3747011423110962\n","Training log: 45 epoch (30848 / 50000 train. data). Loss: 1.4175455570220947\n","Training log: 45 epoch (32128 / 50000 train. data). Loss: 1.336260199546814\n","Training log: 45 epoch (33408 / 50000 train. data). Loss: 1.5427721738815308\n","Training log: 45 epoch (34688 / 50000 train. data). Loss: 1.4476430416107178\n","Training log: 45 epoch (35968 / 50000 train. data). Loss: 1.4232227802276611\n","Training log: 45 epoch (37248 / 50000 train. data). Loss: 1.5537151098251343\n","Training log: 45 epoch (38528 / 50000 train. data). Loss: 1.5276768207550049\n","Training log: 45 epoch (39808 / 50000 train. data). Loss: 1.5584644079208374\n","Training log: 45 epoch (41088 / 50000 train. data). Loss: 1.414743185043335\n","Training log: 45 epoch (42368 / 50000 train. data). Loss: 1.4326871633529663\n","Training log: 45 epoch (43648 / 50000 train. data). Loss: 1.4119250774383545\n","Training log: 45 epoch (44928 / 50000 train. data). Loss: 1.5064399242401123\n","Training log: 45 epoch (46208 / 50000 train. data). Loss: 1.402550220489502\n","Training log: 45 epoch (47488 / 50000 train. data). Loss: 1.4821141958236694\n","Training log: 45 epoch (48768 / 50000 train. data). Loss: 1.4992318153381348\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 34.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 45 epoch (50048 / 50000 train. data). Loss: 1.380882978439331\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.30it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.17it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.529700\n","Training log: 46 epoch (128 / 50000 train. data). Loss: 1.512332797050476\n","Training log: 46 epoch (1408 / 50000 train. data). Loss: 1.476403832435608\n","Training log: 46 epoch (2688 / 50000 train. data). Loss: 1.562682867050171\n","Training log: 46 epoch (3968 / 50000 train. data). Loss: 1.495914101600647\n","Training log: 46 epoch (5248 / 50000 train. data). Loss: 1.3924832344055176\n","Training log: 46 epoch (6528 / 50000 train. data). Loss: 1.233508825302124\n","Training log: 46 epoch (7808 / 50000 train. data). Loss: 1.4860082864761353\n","Training log: 46 epoch (9088 / 50000 train. data). Loss: 1.432515025138855\n","Training log: 46 epoch (10368 / 50000 train. data). Loss: 1.3974440097808838\n","Training log: 46 epoch (11648 / 50000 train. data). Loss: 1.3600646257400513\n","Training log: 46 epoch (12928 / 50000 train. data). Loss: 1.4527397155761719\n","Training log: 46 epoch (14208 / 50000 train. data). Loss: 1.3172833919525146\n","Training log: 46 epoch (15488 / 50000 train. data). Loss: 1.4436488151550293\n","Training log: 46 epoch (16768 / 50000 train. data). Loss: 1.3819568157196045\n","Training log: 46 epoch (18048 / 50000 train. data). Loss: 1.467903971672058\n","Training log: 46 epoch (19328 / 50000 train. data). Loss: 1.5373506546020508\n","Training log: 46 epoch (20608 / 50000 train. data). Loss: 1.3104819059371948\n","Training log: 46 epoch (21888 / 50000 train. data). Loss: 1.4237970113754272\n","Training log: 46 epoch (23168 / 50000 train. data). Loss: 1.4744646549224854\n","Training log: 46 epoch (24448 / 50000 train. data). Loss: 1.4152780771255493\n","Training log: 46 epoch (25728 / 50000 train. data). Loss: 1.4693962335586548\n","Training log: 46 epoch (27008 / 50000 train. data). Loss: 1.5024425983428955\n","Training log: 46 epoch (28288 / 50000 train. data). Loss: 1.3662831783294678\n","Training log: 46 epoch (29568 / 50000 train. data). Loss: 1.6415719985961914\n","Training log: 46 epoch (30848 / 50000 train. data). Loss: 1.4582395553588867\n","Training log: 46 epoch (32128 / 50000 train. data). Loss: 1.4114302396774292\n","Training log: 46 epoch (33408 / 50000 train. data). Loss: 1.490952968597412\n","Training log: 46 epoch (34688 / 50000 train. data). Loss: 1.469935655593872\n","Training log: 46 epoch (35968 / 50000 train. data). Loss: 1.397615909576416\n","Training log: 46 epoch (37248 / 50000 train. data). Loss: 1.3308188915252686\n","Training log: 46 epoch (38528 / 50000 train. data). Loss: 1.5251230001449585\n","Training log: 46 epoch (39808 / 50000 train. data). Loss: 1.304728388786316\n","Training log: 46 epoch (41088 / 50000 train. data). Loss: 1.4447166919708252\n","Training log: 46 epoch (42368 / 50000 train. data). Loss: 1.5045735836029053\n","Training log: 46 epoch (43648 / 50000 train. data). Loss: 1.4719141721725464\n","Training log: 46 epoch (44928 / 50000 train. data). Loss: 1.295669436454773\n","Training log: 46 epoch (46208 / 50000 train. data). Loss: 1.3535220623016357\n","Training log: 46 epoch (47488 / 50000 train. data). Loss: 1.4422930479049683\n","Training log: 46 epoch (48768 / 50000 train. data). Loss: 1.419405221939087\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 35.05it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 46 epoch (50048 / 50000 train. data). Loss: 1.3162599802017212\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.00it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.90it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.528100\n","Training log: 47 epoch (128 / 50000 train. data). Loss: 1.3849093914031982\n","Training log: 47 epoch (1408 / 50000 train. data). Loss: 1.4387634992599487\n","Training log: 47 epoch (2688 / 50000 train. data). Loss: 1.433384895324707\n","Training log: 47 epoch (3968 / 50000 train. data). Loss: 1.4198026657104492\n","Training log: 47 epoch (5248 / 50000 train. data). Loss: 1.3654069900512695\n","Training log: 47 epoch (6528 / 50000 train. data). Loss: 1.5150704383850098\n","Training log: 47 epoch (7808 / 50000 train. data). Loss: 1.482757806777954\n","Training log: 47 epoch (9088 / 50000 train. data). Loss: 1.481542944908142\n","Training log: 47 epoch (10368 / 50000 train. data). Loss: 1.4045310020446777\n","Training log: 47 epoch (11648 / 50000 train. data). Loss: 1.3365707397460938\n","Training log: 47 epoch (12928 / 50000 train. data). Loss: 1.4018222093582153\n","Training log: 47 epoch (14208 / 50000 train. data). Loss: 1.3901869058609009\n","Training log: 47 epoch (15488 / 50000 train. data). Loss: 1.4342155456542969\n","Training log: 47 epoch (16768 / 50000 train. data). Loss: 1.5331854820251465\n","Training log: 47 epoch (18048 / 50000 train. data). Loss: 1.4627023935317993\n","Training log: 47 epoch (19328 / 50000 train. data). Loss: 1.5151824951171875\n","Training log: 47 epoch (20608 / 50000 train. data). Loss: 1.497904896736145\n","Training log: 47 epoch (21888 / 50000 train. data). Loss: 1.4532455205917358\n","Training log: 47 epoch (23168 / 50000 train. data). Loss: 1.4654422998428345\n","Training log: 47 epoch (24448 / 50000 train. data). Loss: 1.5163828134536743\n","Training log: 47 epoch (25728 / 50000 train. data). Loss: 1.3160059452056885\n","Training log: 47 epoch (27008 / 50000 train. data). Loss: 1.4136295318603516\n","Training log: 47 epoch (28288 / 50000 train. data). Loss: 1.5486912727355957\n","Training log: 47 epoch (29568 / 50000 train. data). Loss: 1.5484482049942017\n","Training log: 47 epoch (30848 / 50000 train. data). Loss: 1.399478793144226\n","Training log: 47 epoch (32128 / 50000 train. data). Loss: 1.4348139762878418\n","Training log: 47 epoch (33408 / 50000 train. data). Loss: 1.2583664655685425\n","Training log: 47 epoch (34688 / 50000 train. data). Loss: 1.4663612842559814\n","Training log: 47 epoch (35968 / 50000 train. data). Loss: 1.527846097946167\n","Training log: 47 epoch (37248 / 50000 train. data). Loss: 1.515993356704712\n","Training log: 47 epoch (38528 / 50000 train. data). Loss: 1.4990310668945312\n","Training log: 47 epoch (39808 / 50000 train. data). Loss: 1.56493079662323\n","Training log: 47 epoch (41088 / 50000 train. data). Loss: 1.4140456914901733\n","Training log: 47 epoch (42368 / 50000 train. data). Loss: 1.3966320753097534\n","Training log: 47 epoch (43648 / 50000 train. data). Loss: 1.520267128944397\n","Training log: 47 epoch (44928 / 50000 train. data). Loss: 1.391894817352295\n","Training log: 47 epoch (46208 / 50000 train. data). Loss: 1.4503917694091797\n","Training log: 47 epoch (47488 / 50000 train. data). Loss: 1.4326279163360596\n","Training log: 47 epoch (48768 / 50000 train. data). Loss: 1.2527287006378174\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:12, 31.45it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 47 epoch (50048 / 50000 train. data). Loss: 1.4985345602035522\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.39it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.92it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.531000\n","Training log: 48 epoch (128 / 50000 train. data). Loss: 1.4093655347824097\n","Training log: 48 epoch (1408 / 50000 train. data). Loss: 1.6190770864486694\n","Training log: 48 epoch (2688 / 50000 train. data). Loss: 1.401757836341858\n","Training log: 48 epoch (3968 / 50000 train. data). Loss: 1.3862292766571045\n","Training log: 48 epoch (5248 / 50000 train. data). Loss: 1.4905940294265747\n","Training log: 48 epoch (6528 / 50000 train. data). Loss: 1.4979758262634277\n","Training log: 48 epoch (7808 / 50000 train. data). Loss: 1.372778058052063\n","Training log: 48 epoch (9088 / 50000 train. data). Loss: 1.4785035848617554\n","Training log: 48 epoch (10368 / 50000 train. data). Loss: 1.3119395971298218\n","Training log: 48 epoch (11648 / 50000 train. data). Loss: 1.4648852348327637\n","Training log: 48 epoch (12928 / 50000 train. data). Loss: 1.3753732442855835\n","Training log: 48 epoch (14208 / 50000 train. data). Loss: 1.4012742042541504\n","Training log: 48 epoch (15488 / 50000 train. data). Loss: 1.4378982782363892\n","Training log: 48 epoch (16768 / 50000 train. data). Loss: 1.41219162940979\n","Training log: 48 epoch (18048 / 50000 train. data). Loss: 1.5973197221755981\n","Training log: 48 epoch (19328 / 50000 train. data). Loss: 1.3550188541412354\n","Training log: 48 epoch (20608 / 50000 train. data). Loss: 1.672136902809143\n","Training log: 48 epoch (21888 / 50000 train. data). Loss: 1.419405221939087\n","Training log: 48 epoch (23168 / 50000 train. data). Loss: 1.5391900539398193\n","Training log: 48 epoch (24448 / 50000 train. data). Loss: 1.3945071697235107\n","Training log: 48 epoch (25728 / 50000 train. data). Loss: 1.3592685461044312\n","Training log: 48 epoch (27008 / 50000 train. data). Loss: 1.3596101999282837\n","Training log: 48 epoch (28288 / 50000 train. data). Loss: 1.3743923902511597\n","Training log: 48 epoch (29568 / 50000 train. data). Loss: 1.3426885604858398\n","Training log: 48 epoch (30848 / 50000 train. data). Loss: 1.4118030071258545\n","Training log: 48 epoch (32128 / 50000 train. data). Loss: 1.2608764171600342\n","Training log: 48 epoch (33408 / 50000 train. data). Loss: 1.409258246421814\n","Training log: 48 epoch (34688 / 50000 train. data). Loss: 1.324617862701416\n","Training log: 48 epoch (35968 / 50000 train. data). Loss: 1.3306466341018677\n","Training log: 48 epoch (37248 / 50000 train. data). Loss: 1.4124953746795654\n","Training log: 48 epoch (38528 / 50000 train. data). Loss: 1.4360276460647583\n","Training log: 48 epoch (39808 / 50000 train. data). Loss: 1.569097638130188\n","Training log: 48 epoch (41088 / 50000 train. data). Loss: 1.4691829681396484\n","Training log: 48 epoch (42368 / 50000 train. data). Loss: 1.2997797727584839\n","Training log: 48 epoch (43648 / 50000 train. data). Loss: 1.4057610034942627\n","Training log: 48 epoch (44928 / 50000 train. data). Loss: 1.467826247215271\n","Training log: 48 epoch (46208 / 50000 train. data). Loss: 1.3624354600906372\n","Training log: 48 epoch (47488 / 50000 train. data). Loss: 1.4106056690216064\n","Training log: 48 epoch (48768 / 50000 train. data). Loss: 1.517811894416809\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 3/391 [00:00<00:13, 29.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 48 epoch (50048 / 50000 train. data). Loss: 1.5747379064559937\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.90it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.61it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.531700\n","Training log: 49 epoch (128 / 50000 train. data). Loss: 1.5147147178649902\n","Training log: 49 epoch (1408 / 50000 train. data). Loss: 1.4930022954940796\n","Training log: 49 epoch (2688 / 50000 train. data). Loss: 1.4261341094970703\n","Training log: 49 epoch (3968 / 50000 train. data). Loss: 1.5408880710601807\n","Training log: 49 epoch (5248 / 50000 train. data). Loss: 1.413583517074585\n","Training log: 49 epoch (6528 / 50000 train. data). Loss: 1.3766976594924927\n","Training log: 49 epoch (7808 / 50000 train. data). Loss: 1.5382683277130127\n","Training log: 49 epoch (9088 / 50000 train. data). Loss: 1.5219451189041138\n","Training log: 49 epoch (10368 / 50000 train. data). Loss: 1.3961098194122314\n","Training log: 49 epoch (11648 / 50000 train. data). Loss: 1.3851423263549805\n","Training log: 49 epoch (12928 / 50000 train. data). Loss: 1.4033085107803345\n","Training log: 49 epoch (14208 / 50000 train. data). Loss: 1.6132415533065796\n","Training log: 49 epoch (15488 / 50000 train. data). Loss: 1.4389070272445679\n","Training log: 49 epoch (16768 / 50000 train. data). Loss: 1.2866846323013306\n","Training log: 49 epoch (18048 / 50000 train. data). Loss: 1.4218732118606567\n","Training log: 49 epoch (19328 / 50000 train. data). Loss: 1.4537297487258911\n","Training log: 49 epoch (20608 / 50000 train. data). Loss: 1.4169567823410034\n","Training log: 49 epoch (21888 / 50000 train. data). Loss: 1.433688759803772\n","Training log: 49 epoch (23168 / 50000 train. data). Loss: 1.5267040729522705\n","Training log: 49 epoch (24448 / 50000 train. data). Loss: 1.4409875869750977\n","Training log: 49 epoch (25728 / 50000 train. data). Loss: 1.492173194885254\n","Training log: 49 epoch (27008 / 50000 train. data). Loss: 1.4502300024032593\n","Training log: 49 epoch (28288 / 50000 train. data). Loss: 1.4975634813308716\n","Training log: 49 epoch (29568 / 50000 train. data). Loss: 1.4481050968170166\n","Training log: 49 epoch (30848 / 50000 train. data). Loss: 1.4573872089385986\n","Training log: 49 epoch (32128 / 50000 train. data). Loss: 1.4997648000717163\n","Training log: 49 epoch (33408 / 50000 train. data). Loss: 1.3625001907348633\n","Training log: 49 epoch (34688 / 50000 train. data). Loss: 1.4710012674331665\n","Training log: 49 epoch (35968 / 50000 train. data). Loss: 1.4317810535430908\n","Training log: 49 epoch (37248 / 50000 train. data). Loss: 1.4558055400848389\n","Training log: 49 epoch (38528 / 50000 train. data). Loss: 1.3802634477615356\n","Training log: 49 epoch (39808 / 50000 train. data). Loss: 1.389474630355835\n","Training log: 49 epoch (41088 / 50000 train. data). Loss: 1.305047631263733\n","Training log: 49 epoch (42368 / 50000 train. data). Loss: 1.3372114896774292\n","Training log: 49 epoch (43648 / 50000 train. data). Loss: 1.3965297937393188\n","Training log: 49 epoch (44928 / 50000 train. data). Loss: 1.3955906629562378\n","Training log: 49 epoch (46208 / 50000 train. data). Loss: 1.3508474826812744\n","Training log: 49 epoch (47488 / 50000 train. data). Loss: 1.3470388650894165\n","Training log: 49 epoch (48768 / 50000 train. data). Loss: 1.46212637424469\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 49 epoch (50048 / 50000 train. data). Loss: 1.577517032623291\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.06it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.59it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.533200\n","Training log: 50 epoch (128 / 50000 train. data). Loss: 1.457040786743164\n","Training log: 50 epoch (1408 / 50000 train. data). Loss: 1.3810805082321167\n","Training log: 50 epoch (2688 / 50000 train. data). Loss: 1.482508659362793\n","Training log: 50 epoch (3968 / 50000 train. data). Loss: 1.5466654300689697\n","Training log: 50 epoch (5248 / 50000 train. data). Loss: 1.454360008239746\n","Training log: 50 epoch (6528 / 50000 train. data). Loss: 1.6539642810821533\n","Training log: 50 epoch (7808 / 50000 train. data). Loss: 1.5248559713363647\n","Training log: 50 epoch (9088 / 50000 train. data). Loss: 1.397268533706665\n","Training log: 50 epoch (10368 / 50000 train. data). Loss: 1.4221664667129517\n","Training log: 50 epoch (11648 / 50000 train. data). Loss: 1.4320780038833618\n","Training log: 50 epoch (12928 / 50000 train. data). Loss: 1.4179986715316772\n","Training log: 50 epoch (14208 / 50000 train. data). Loss: 1.4498130083084106\n","Training log: 50 epoch (15488 / 50000 train. data). Loss: 1.5102038383483887\n","Training log: 50 epoch (16768 / 50000 train. data). Loss: 1.4955672025680542\n","Training log: 50 epoch (18048 / 50000 train. data). Loss: 1.3975752592086792\n","Training log: 50 epoch (19328 / 50000 train. data). Loss: 1.4490886926651\n","Training log: 50 epoch (20608 / 50000 train. data). Loss: 1.4448045492172241\n","Training log: 50 epoch (21888 / 50000 train. data). Loss: 1.5152310132980347\n","Training log: 50 epoch (23168 / 50000 train. data). Loss: 1.5160590410232544\n","Training log: 50 epoch (24448 / 50000 train. data). Loss: 1.3616005182266235\n","Training log: 50 epoch (25728 / 50000 train. data). Loss: 1.3444406986236572\n","Training log: 50 epoch (27008 / 50000 train. data). Loss: 1.464112401008606\n","Training log: 50 epoch (28288 / 50000 train. data). Loss: 1.630831241607666\n","Training log: 50 epoch (29568 / 50000 train. data). Loss: 1.432157278060913\n","Training log: 50 epoch (30848 / 50000 train. data). Loss: 1.4056659936904907\n","Training log: 50 epoch (32128 / 50000 train. data). Loss: 1.3949089050292969\n","Training log: 50 epoch (33408 / 50000 train. data). Loss: 1.574735403060913\n","Training log: 50 epoch (34688 / 50000 train. data). Loss: 1.5143851041793823\n","Training log: 50 epoch (35968 / 50000 train. data). Loss: 1.3916850090026855\n","Training log: 50 epoch (37248 / 50000 train. data). Loss: 1.3752620220184326\n","Training log: 50 epoch (38528 / 50000 train. data). Loss: 1.4750454425811768\n","Training log: 50 epoch (39808 / 50000 train. data). Loss: 1.4617127180099487\n","Training log: 50 epoch (41088 / 50000 train. data). Loss: 1.368632197380066\n","Training log: 50 epoch (42368 / 50000 train. data). Loss: 1.4016122817993164\n","Training log: 50 epoch (43648 / 50000 train. data). Loss: 1.413002848625183\n","Training log: 50 epoch (44928 / 50000 train. data). Loss: 1.3505765199661255\n","Training log: 50 epoch (46208 / 50000 train. data). Loss: 1.3715169429779053\n","Training log: 50 epoch (47488 / 50000 train. data). Loss: 1.5242717266082764\n","Training log: 50 epoch (48768 / 50000 train. data). Loss: 1.456085443496704\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 50 epoch (50048 / 50000 train. data). Loss: 1.1254174709320068\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.71it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.26it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.539000\n","Training log: 51 epoch (128 / 50000 train. data). Loss: 1.3746405839920044\n","Training log: 51 epoch (1408 / 50000 train. data). Loss: 1.2873258590698242\n","Training log: 51 epoch (2688 / 50000 train. data). Loss: 1.5074286460876465\n","Training log: 51 epoch (3968 / 50000 train. data). Loss: 1.3956904411315918\n","Training log: 51 epoch (5248 / 50000 train. data). Loss: 1.456626296043396\n","Training log: 51 epoch (6528 / 50000 train. data). Loss: 1.4925146102905273\n","Training log: 51 epoch (7808 / 50000 train. data). Loss: 1.3795273303985596\n","Training log: 51 epoch (9088 / 50000 train. data). Loss: 1.3499819040298462\n","Training log: 51 epoch (10368 / 50000 train. data). Loss: 1.3768587112426758\n","Training log: 51 epoch (11648 / 50000 train. data). Loss: 1.4364197254180908\n","Training log: 51 epoch (12928 / 50000 train. data). Loss: 1.4477908611297607\n","Training log: 51 epoch (14208 / 50000 train. data). Loss: 1.439568281173706\n","Training log: 51 epoch (15488 / 50000 train. data). Loss: 1.4829612970352173\n","Training log: 51 epoch (16768 / 50000 train. data). Loss: 1.3623381853103638\n","Training log: 51 epoch (18048 / 50000 train. data). Loss: 1.3507883548736572\n","Training log: 51 epoch (19328 / 50000 train. data). Loss: 1.3585084676742554\n","Training log: 51 epoch (20608 / 50000 train. data). Loss: 1.4637726545333862\n","Training log: 51 epoch (21888 / 50000 train. data). Loss: 1.436219334602356\n","Training log: 51 epoch (23168 / 50000 train. data). Loss: 1.4938794374465942\n","Training log: 51 epoch (24448 / 50000 train. data). Loss: 1.3865957260131836\n","Training log: 51 epoch (25728 / 50000 train. data). Loss: 1.4319891929626465\n","Training log: 51 epoch (27008 / 50000 train. data). Loss: 1.4005008935928345\n","Training log: 51 epoch (28288 / 50000 train. data). Loss: 1.5187407732009888\n","Training log: 51 epoch (29568 / 50000 train. data). Loss: 1.3564399480819702\n","Training log: 51 epoch (30848 / 50000 train. data). Loss: 1.2947933673858643\n","Training log: 51 epoch (32128 / 50000 train. data). Loss: 1.3925861120224\n","Training log: 51 epoch (33408 / 50000 train. data). Loss: 1.446765422821045\n","Training log: 51 epoch (34688 / 50000 train. data). Loss: 1.401581048965454\n","Training log: 51 epoch (35968 / 50000 train. data). Loss: 1.5470306873321533\n","Training log: 51 epoch (37248 / 50000 train. data). Loss: 1.3577830791473389\n","Training log: 51 epoch (38528 / 50000 train. data). Loss: 1.3501275777816772\n","Training log: 51 epoch (39808 / 50000 train. data). Loss: 1.4893882274627686\n","Training log: 51 epoch (41088 / 50000 train. data). Loss: 1.4371389150619507\n","Training log: 51 epoch (42368 / 50000 train. data). Loss: 1.496138095855713\n","Training log: 51 epoch (43648 / 50000 train. data). Loss: 1.5291177034378052\n","Training log: 51 epoch (44928 / 50000 train. data). Loss: 1.4776901006698608\n","Training log: 51 epoch (46208 / 50000 train. data). Loss: 1.3043752908706665\n","Training log: 51 epoch (47488 / 50000 train. data). Loss: 1.3789302110671997\n","Training log: 51 epoch (48768 / 50000 train. data). Loss: 1.3802847862243652\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 38.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 51 epoch (50048 / 50000 train. data). Loss: 1.371060848236084\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.34it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.28it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.544000\n","Training log: 52 epoch (128 / 50000 train. data). Loss: 1.3872158527374268\n","Training log: 52 epoch (1408 / 50000 train. data). Loss: 1.5067341327667236\n","Training log: 52 epoch (2688 / 50000 train. data). Loss: 1.425907015800476\n","Training log: 52 epoch (3968 / 50000 train. data). Loss: 1.3454252481460571\n","Training log: 52 epoch (5248 / 50000 train. data). Loss: 1.436999797821045\n","Training log: 52 epoch (6528 / 50000 train. data). Loss: 1.5565634965896606\n","Training log: 52 epoch (7808 / 50000 train. data). Loss: 1.4038037061691284\n","Training log: 52 epoch (9088 / 50000 train. data). Loss: 1.3861963748931885\n","Training log: 52 epoch (10368 / 50000 train. data). Loss: 1.3545851707458496\n","Training log: 52 epoch (11648 / 50000 train. data). Loss: 1.3396806716918945\n","Training log: 52 epoch (12928 / 50000 train. data). Loss: 1.4412161111831665\n","Training log: 52 epoch (14208 / 50000 train. data). Loss: 1.4110831022262573\n","Training log: 52 epoch (15488 / 50000 train. data). Loss: 1.3429067134857178\n","Training log: 52 epoch (16768 / 50000 train. data). Loss: 1.5314708948135376\n","Training log: 52 epoch (18048 / 50000 train. data). Loss: 1.2370693683624268\n","Training log: 52 epoch (19328 / 50000 train. data). Loss: 1.46934974193573\n","Training log: 52 epoch (20608 / 50000 train. data). Loss: 1.4943643808364868\n","Training log: 52 epoch (21888 / 50000 train. data). Loss: 1.4051188230514526\n","Training log: 52 epoch (23168 / 50000 train. data). Loss: 1.5852221250534058\n","Training log: 52 epoch (24448 / 50000 train. data). Loss: 1.4104926586151123\n","Training log: 52 epoch (25728 / 50000 train. data). Loss: 1.369356632232666\n","Training log: 52 epoch (27008 / 50000 train. data). Loss: 1.2463068962097168\n","Training log: 52 epoch (28288 / 50000 train. data). Loss: 1.2825549840927124\n","Training log: 52 epoch (29568 / 50000 train. data). Loss: 1.421839952468872\n","Training log: 52 epoch (30848 / 50000 train. data). Loss: 1.5394107103347778\n","Training log: 52 epoch (32128 / 50000 train. data). Loss: 1.3299428224563599\n","Training log: 52 epoch (33408 / 50000 train. data). Loss: 1.3575977087020874\n","Training log: 52 epoch (34688 / 50000 train. data). Loss: 1.4099082946777344\n","Training log: 52 epoch (35968 / 50000 train. data). Loss: 1.4670732021331787\n","Training log: 52 epoch (37248 / 50000 train. data). Loss: 1.3700677156448364\n","Training log: 52 epoch (38528 / 50000 train. data). Loss: 1.595866322517395\n","Training log: 52 epoch (39808 / 50000 train. data). Loss: 1.4207054376602173\n","Training log: 52 epoch (41088 / 50000 train. data). Loss: 1.5244231224060059\n","Training log: 52 epoch (42368 / 50000 train. data). Loss: 1.4030505418777466\n","Training log: 52 epoch (43648 / 50000 train. data). Loss: 1.425986409187317\n","Training log: 52 epoch (44928 / 50000 train. data). Loss: 1.5359530448913574\n","Training log: 52 epoch (46208 / 50000 train. data). Loss: 1.4216015338897705\n","Training log: 52 epoch (47488 / 50000 train. data). Loss: 1.3373029232025146\n","Training log: 52 epoch (48768 / 50000 train. data). Loss: 1.356640100479126\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 52 epoch (50048 / 50000 train. data). Loss: 1.3852307796478271\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.62it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.541400\n","Training log: 53 epoch (128 / 50000 train. data). Loss: 1.469923496246338\n","Training log: 53 epoch (1408 / 50000 train. data). Loss: 1.4631654024124146\n","Training log: 53 epoch (2688 / 50000 train. data). Loss: 1.4470372200012207\n","Training log: 53 epoch (3968 / 50000 train. data). Loss: 1.5515189170837402\n","Training log: 53 epoch (5248 / 50000 train. data). Loss: 1.3878196477890015\n","Training log: 53 epoch (6528 / 50000 train. data). Loss: 1.4448528289794922\n","Training log: 53 epoch (7808 / 50000 train. data). Loss: 1.627918004989624\n","Training log: 53 epoch (9088 / 50000 train. data). Loss: 1.3877156972885132\n","Training log: 53 epoch (10368 / 50000 train. data). Loss: 1.4388624429702759\n","Training log: 53 epoch (11648 / 50000 train. data). Loss: 1.326491355895996\n","Training log: 53 epoch (12928 / 50000 train. data). Loss: 1.4774600267410278\n","Training log: 53 epoch (14208 / 50000 train. data). Loss: 1.305911660194397\n","Training log: 53 epoch (15488 / 50000 train. data). Loss: 1.4746309518814087\n","Training log: 53 epoch (16768 / 50000 train. data). Loss: 1.4468252658843994\n","Training log: 53 epoch (18048 / 50000 train. data). Loss: 1.4173184633255005\n","Training log: 53 epoch (19328 / 50000 train. data). Loss: 1.4595075845718384\n","Training log: 53 epoch (20608 / 50000 train. data). Loss: 1.6971492767333984\n","Training log: 53 epoch (21888 / 50000 train. data). Loss: 1.4410525560379028\n","Training log: 53 epoch (23168 / 50000 train. data). Loss: 1.5663058757781982\n","Training log: 53 epoch (24448 / 50000 train. data). Loss: 1.3736664056777954\n","Training log: 53 epoch (25728 / 50000 train. data). Loss: 1.4442552328109741\n","Training log: 53 epoch (27008 / 50000 train. data). Loss: 1.4871116876602173\n","Training log: 53 epoch (28288 / 50000 train. data). Loss: 1.4659701585769653\n","Training log: 53 epoch (29568 / 50000 train. data). Loss: 1.4686164855957031\n","Training log: 53 epoch (30848 / 50000 train. data). Loss: 1.3558224439620972\n","Training log: 53 epoch (32128 / 50000 train. data). Loss: 1.4099514484405518\n","Training log: 53 epoch (33408 / 50000 train. data). Loss: 1.4812430143356323\n","Training log: 53 epoch (34688 / 50000 train. data). Loss: 1.4989304542541504\n","Training log: 53 epoch (35968 / 50000 train. data). Loss: 1.501497507095337\n","Training log: 53 epoch (37248 / 50000 train. data). Loss: 1.5065289735794067\n","Training log: 53 epoch (38528 / 50000 train. data). Loss: 1.538170576095581\n","Training log: 53 epoch (39808 / 50000 train. data). Loss: 1.4635505676269531\n","Training log: 53 epoch (41088 / 50000 train. data). Loss: 1.3671072721481323\n","Training log: 53 epoch (42368 / 50000 train. data). Loss: 1.4235259294509888\n","Training log: 53 epoch (43648 / 50000 train. data). Loss: 1.5158145427703857\n","Training log: 53 epoch (44928 / 50000 train. data). Loss: 1.475826621055603\n","Training log: 53 epoch (46208 / 50000 train. data). Loss: 1.4117082357406616\n","Training log: 53 epoch (47488 / 50000 train. data). Loss: 1.5120799541473389\n","Training log: 53 epoch (48768 / 50000 train. data). Loss: 1.343921184539795\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.22it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 53 epoch (50048 / 50000 train. data). Loss: 1.3908507823944092\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.63it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.58it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.547300\n","Training log: 54 epoch (128 / 50000 train. data). Loss: 1.4469150304794312\n","Training log: 54 epoch (1408 / 50000 train. data). Loss: 1.5723848342895508\n","Training log: 54 epoch (2688 / 50000 train. data). Loss: 1.4140193462371826\n","Training log: 54 epoch (3968 / 50000 train. data). Loss: 1.3658679723739624\n","Training log: 54 epoch (5248 / 50000 train. data). Loss: 1.2408183813095093\n","Training log: 54 epoch (6528 / 50000 train. data). Loss: 1.4379147291183472\n","Training log: 54 epoch (7808 / 50000 train. data). Loss: 1.4865766763687134\n","Training log: 54 epoch (9088 / 50000 train. data). Loss: 1.4343456029891968\n","Training log: 54 epoch (10368 / 50000 train. data). Loss: 1.2832218408584595\n","Training log: 54 epoch (11648 / 50000 train. data). Loss: 1.4096800088882446\n","Training log: 54 epoch (12928 / 50000 train. data). Loss: 1.4505563974380493\n","Training log: 54 epoch (14208 / 50000 train. data). Loss: 1.378538727760315\n","Training log: 54 epoch (15488 / 50000 train. data). Loss: 1.3690917491912842\n","Training log: 54 epoch (16768 / 50000 train. data). Loss: 1.4787126779556274\n","Training log: 54 epoch (18048 / 50000 train. data). Loss: 1.2137507200241089\n","Training log: 54 epoch (19328 / 50000 train. data). Loss: 1.561743974685669\n","Training log: 54 epoch (20608 / 50000 train. data). Loss: 1.5126861333847046\n","Training log: 54 epoch (21888 / 50000 train. data). Loss: 1.4540095329284668\n","Training log: 54 epoch (23168 / 50000 train. data). Loss: 1.3358224630355835\n","Training log: 54 epoch (24448 / 50000 train. data). Loss: 1.3734803199768066\n","Training log: 54 epoch (25728 / 50000 train. data). Loss: 1.5736424922943115\n","Training log: 54 epoch (27008 / 50000 train. data). Loss: 1.3212798833847046\n","Training log: 54 epoch (28288 / 50000 train. data). Loss: 1.340639352798462\n","Training log: 54 epoch (29568 / 50000 train. data). Loss: 1.5860037803649902\n","Training log: 54 epoch (30848 / 50000 train. data). Loss: 1.4445459842681885\n","Training log: 54 epoch (32128 / 50000 train. data). Loss: 1.471130132675171\n","Training log: 54 epoch (33408 / 50000 train. data). Loss: 1.4618526697158813\n","Training log: 54 epoch (34688 / 50000 train. data). Loss: 1.4337983131408691\n","Training log: 54 epoch (35968 / 50000 train. data). Loss: 1.4495282173156738\n","Training log: 54 epoch (37248 / 50000 train. data). Loss: 1.6331950426101685\n","Training log: 54 epoch (38528 / 50000 train. data). Loss: 1.4939333200454712\n","Training log: 54 epoch (39808 / 50000 train. data). Loss: 1.3339812755584717\n","Training log: 54 epoch (41088 / 50000 train. data). Loss: 1.3885302543640137\n","Training log: 54 epoch (42368 / 50000 train. data). Loss: 1.5607452392578125\n","Training log: 54 epoch (43648 / 50000 train. data). Loss: 1.4482440948486328\n","Training log: 54 epoch (44928 / 50000 train. data). Loss: 1.2965208292007446\n","Training log: 54 epoch (46208 / 50000 train. data). Loss: 1.4445395469665527\n","Training log: 54 epoch (47488 / 50000 train. data). Loss: 1.4424718618392944\n","Training log: 54 epoch (48768 / 50000 train. data). Loss: 1.2478585243225098\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 54 epoch (50048 / 50000 train. data). Loss: 1.5086537599563599\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.94it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.66it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.544600\n","Training log: 55 epoch (128 / 50000 train. data). Loss: 1.5493922233581543\n","Training log: 55 epoch (1408 / 50000 train. data). Loss: 1.314735770225525\n","Training log: 55 epoch (2688 / 50000 train. data). Loss: 1.3184354305267334\n","Training log: 55 epoch (3968 / 50000 train. data). Loss: 1.702675700187683\n","Training log: 55 epoch (5248 / 50000 train. data). Loss: 1.554283618927002\n","Training log: 55 epoch (6528 / 50000 train. data). Loss: 1.4053339958190918\n","Training log: 55 epoch (7808 / 50000 train. data). Loss: 1.367476224899292\n","Training log: 55 epoch (9088 / 50000 train. data). Loss: 1.409764051437378\n","Training log: 55 epoch (10368 / 50000 train. data). Loss: 1.2850151062011719\n","Training log: 55 epoch (11648 / 50000 train. data). Loss: 1.4016343355178833\n","Training log: 55 epoch (12928 / 50000 train. data). Loss: 1.4554369449615479\n","Training log: 55 epoch (14208 / 50000 train. data). Loss: 1.354854702949524\n","Training log: 55 epoch (15488 / 50000 train. data). Loss: 1.3483314514160156\n","Training log: 55 epoch (16768 / 50000 train. data). Loss: 1.3107463121414185\n","Training log: 55 epoch (18048 / 50000 train. data). Loss: 1.4205505847930908\n","Training log: 55 epoch (19328 / 50000 train. data). Loss: 1.4141039848327637\n","Training log: 55 epoch (20608 / 50000 train. data). Loss: 1.3659617900848389\n","Training log: 55 epoch (21888 / 50000 train. data). Loss: 1.3539695739746094\n","Training log: 55 epoch (23168 / 50000 train. data). Loss: 1.3205492496490479\n","Training log: 55 epoch (24448 / 50000 train. data). Loss: 1.2962076663970947\n","Training log: 55 epoch (25728 / 50000 train. data). Loss: 1.3867570161819458\n","Training log: 55 epoch (27008 / 50000 train. data). Loss: 1.256476640701294\n","Training log: 55 epoch (28288 / 50000 train. data). Loss: 1.509447455406189\n","Training log: 55 epoch (29568 / 50000 train. data). Loss: 1.5221303701400757\n","Training log: 55 epoch (30848 / 50000 train. data). Loss: 1.4546635150909424\n","Training log: 55 epoch (32128 / 50000 train. data). Loss: 1.2861069440841675\n","Training log: 55 epoch (33408 / 50000 train. data). Loss: 1.4218112230300903\n","Training log: 55 epoch (34688 / 50000 train. data). Loss: 1.2908216714859009\n","Training log: 55 epoch (35968 / 50000 train. data). Loss: 1.2856370210647583\n","Training log: 55 epoch (37248 / 50000 train. data). Loss: 1.3260629177093506\n","Training log: 55 epoch (38528 / 50000 train. data). Loss: 1.4359647035598755\n","Training log: 55 epoch (39808 / 50000 train. data). Loss: 1.3719334602355957\n","Training log: 55 epoch (41088 / 50000 train. data). Loss: 1.3015472888946533\n","Training log: 55 epoch (42368 / 50000 train. data). Loss: 1.4044893980026245\n","Training log: 55 epoch (43648 / 50000 train. data). Loss: 1.4201841354370117\n","Training log: 55 epoch (44928 / 50000 train. data). Loss: 1.4762312173843384\n","Training log: 55 epoch (46208 / 50000 train. data). Loss: 1.3645521402359009\n","Training log: 55 epoch (47488 / 50000 train. data). Loss: 1.348526954650879\n","Training log: 55 epoch (48768 / 50000 train. data). Loss: 1.2760297060012817\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 3/391 [00:00<00:13, 29.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 55 epoch (50048 / 50000 train. data). Loss: 1.4642459154129028\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.99it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.56it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.551100\n","Training log: 56 epoch (128 / 50000 train. data). Loss: 1.3448302745819092\n","Training log: 56 epoch (1408 / 50000 train. data). Loss: 1.3807835578918457\n","Training log: 56 epoch (2688 / 50000 train. data). Loss: 1.377382516860962\n","Training log: 56 epoch (3968 / 50000 train. data). Loss: 1.5101850032806396\n","Training log: 56 epoch (5248 / 50000 train. data). Loss: 1.4115538597106934\n","Training log: 56 epoch (6528 / 50000 train. data). Loss: 1.5192183256149292\n","Training log: 56 epoch (7808 / 50000 train. data). Loss: 1.4051729440689087\n","Training log: 56 epoch (9088 / 50000 train. data). Loss: 1.534731149673462\n","Training log: 56 epoch (10368 / 50000 train. data). Loss: 1.3133447170257568\n","Training log: 56 epoch (11648 / 50000 train. data). Loss: 1.2855110168457031\n","Training log: 56 epoch (12928 / 50000 train. data). Loss: 1.6563758850097656\n","Training log: 56 epoch (14208 / 50000 train. data). Loss: 1.2835813760757446\n","Training log: 56 epoch (15488 / 50000 train. data). Loss: 1.3985496759414673\n","Training log: 56 epoch (16768 / 50000 train. data). Loss: 1.4082629680633545\n","Training log: 56 epoch (18048 / 50000 train. data). Loss: 1.3387982845306396\n","Training log: 56 epoch (19328 / 50000 train. data). Loss: 1.454315185546875\n","Training log: 56 epoch (20608 / 50000 train. data). Loss: 1.3616727590560913\n","Training log: 56 epoch (21888 / 50000 train. data). Loss: 1.3326835632324219\n","Training log: 56 epoch (23168 / 50000 train. data). Loss: 1.6316882371902466\n","Training log: 56 epoch (24448 / 50000 train. data). Loss: 1.386176586151123\n","Training log: 56 epoch (25728 / 50000 train. data). Loss: 1.4904024600982666\n","Training log: 56 epoch (27008 / 50000 train. data). Loss: 1.2492848634719849\n","Training log: 56 epoch (28288 / 50000 train. data). Loss: 1.526574730873108\n","Training log: 56 epoch (29568 / 50000 train. data). Loss: 1.4019105434417725\n","Training log: 56 epoch (30848 / 50000 train. data). Loss: 1.4230655431747437\n","Training log: 56 epoch (32128 / 50000 train. data). Loss: 1.3864219188690186\n","Training log: 56 epoch (33408 / 50000 train. data). Loss: 1.369673728942871\n","Training log: 56 epoch (34688 / 50000 train. data). Loss: 1.4111257791519165\n","Training log: 56 epoch (35968 / 50000 train. data). Loss: 1.4918168783187866\n","Training log: 56 epoch (37248 / 50000 train. data). Loss: 1.3678396940231323\n","Training log: 56 epoch (38528 / 50000 train. data). Loss: 1.294411301612854\n","Training log: 56 epoch (39808 / 50000 train. data). Loss: 1.35099196434021\n","Training log: 56 epoch (41088 / 50000 train. data). Loss: 1.4049968719482422\n","Training log: 56 epoch (42368 / 50000 train. data). Loss: 1.4645408391952515\n","Training log: 56 epoch (43648 / 50000 train. data). Loss: 1.3059258460998535\n","Training log: 56 epoch (44928 / 50000 train. data). Loss: 1.411799430847168\n","Training log: 56 epoch (46208 / 50000 train. data). Loss: 1.4933264255523682\n","Training log: 56 epoch (47488 / 50000 train. data). Loss: 1.300700306892395\n","Training log: 56 epoch (48768 / 50000 train. data). Loss: 1.2710814476013184\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 38.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 56 epoch (50048 / 50000 train. data). Loss: 1.2131623029708862\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.74it/s]\n","100%|██████████| 79/79 [00:02<00:00, 37.16it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.551700\n","Training log: 57 epoch (128 / 50000 train. data). Loss: 1.3841599225997925\n","Training log: 57 epoch (1408 / 50000 train. data). Loss: 1.2935795783996582\n","Training log: 57 epoch (2688 / 50000 train. data). Loss: 1.281460165977478\n","Training log: 57 epoch (3968 / 50000 train. data). Loss: 1.3886810541152954\n","Training log: 57 epoch (5248 / 50000 train. data). Loss: 1.3058863878250122\n","Training log: 57 epoch (6528 / 50000 train. data). Loss: 1.3012646436691284\n","Training log: 57 epoch (7808 / 50000 train. data). Loss: 1.3016866445541382\n","Training log: 57 epoch (9088 / 50000 train. data). Loss: 1.3455359935760498\n","Training log: 57 epoch (10368 / 50000 train. data). Loss: 1.4309388399124146\n","Training log: 57 epoch (11648 / 50000 train. data). Loss: 1.546119213104248\n","Training log: 57 epoch (12928 / 50000 train. data). Loss: 1.4899041652679443\n","Training log: 57 epoch (14208 / 50000 train. data). Loss: 1.42453932762146\n","Training log: 57 epoch (15488 / 50000 train. data). Loss: 1.3924280405044556\n","Training log: 57 epoch (16768 / 50000 train. data). Loss: 1.5339512825012207\n","Training log: 57 epoch (18048 / 50000 train. data). Loss: 1.444294810295105\n","Training log: 57 epoch (19328 / 50000 train. data). Loss: 1.4252638816833496\n","Training log: 57 epoch (20608 / 50000 train. data). Loss: 1.3996763229370117\n","Training log: 57 epoch (21888 / 50000 train. data). Loss: 1.3804795742034912\n","Training log: 57 epoch (23168 / 50000 train. data). Loss: 1.6163936853408813\n","Training log: 57 epoch (24448 / 50000 train. data). Loss: 1.4593226909637451\n","Training log: 57 epoch (25728 / 50000 train. data). Loss: 1.4227887392044067\n","Training log: 57 epoch (27008 / 50000 train. data). Loss: 1.2685472965240479\n","Training log: 57 epoch (28288 / 50000 train. data). Loss: 1.3371559381484985\n","Training log: 57 epoch (29568 / 50000 train. data). Loss: 1.3773480653762817\n","Training log: 57 epoch (30848 / 50000 train. data). Loss: 1.3147733211517334\n","Training log: 57 epoch (32128 / 50000 train. data). Loss: 1.3957009315490723\n","Training log: 57 epoch (33408 / 50000 train. data). Loss: 1.2694718837738037\n","Training log: 57 epoch (34688 / 50000 train. data). Loss: 1.437264084815979\n","Training log: 57 epoch (35968 / 50000 train. data). Loss: 1.5042897462844849\n","Training log: 57 epoch (37248 / 50000 train. data). Loss: 1.5032581090927124\n","Training log: 57 epoch (38528 / 50000 train. data). Loss: 1.4624465703964233\n","Training log: 57 epoch (39808 / 50000 train. data). Loss: 1.4251487255096436\n","Training log: 57 epoch (41088 / 50000 train. data). Loss: 1.520444631576538\n","Training log: 57 epoch (42368 / 50000 train. data). Loss: 1.5100617408752441\n","Training log: 57 epoch (43648 / 50000 train. data). Loss: 1.3171707391738892\n","Training log: 57 epoch (44928 / 50000 train. data). Loss: 1.4143450260162354\n","Training log: 57 epoch (46208 / 50000 train. data). Loss: 1.459588646888733\n","Training log: 57 epoch (47488 / 50000 train. data). Loss: 1.412202000617981\n","Training log: 57 epoch (48768 / 50000 train. data). Loss: 1.3894128799438477\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 34.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 57 epoch (50048 / 50000 train. data). Loss: 1.2943298816680908\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.73it/s]\n","100%|██████████| 79/79 [00:02<00:00, 37.37it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.550700\n","Training log: 58 epoch (128 / 50000 train. data). Loss: 1.556384801864624\n","Training log: 58 epoch (1408 / 50000 train. data). Loss: 1.518820881843567\n","Training log: 58 epoch (2688 / 50000 train. data). Loss: 1.5374250411987305\n","Training log: 58 epoch (3968 / 50000 train. data). Loss: 1.6065998077392578\n","Training log: 58 epoch (5248 / 50000 train. data). Loss: 1.373939037322998\n","Training log: 58 epoch (6528 / 50000 train. data). Loss: 1.348172903060913\n","Training log: 58 epoch (7808 / 50000 train. data). Loss: 1.3675708770751953\n","Training log: 58 epoch (9088 / 50000 train. data). Loss: 1.4640002250671387\n","Training log: 58 epoch (10368 / 50000 train. data). Loss: 1.2250088453292847\n","Training log: 58 epoch (11648 / 50000 train. data). Loss: 1.4924014806747437\n","Training log: 58 epoch (12928 / 50000 train. data). Loss: 1.3038830757141113\n","Training log: 58 epoch (14208 / 50000 train. data). Loss: 1.3501451015472412\n","Training log: 58 epoch (15488 / 50000 train. data). Loss: 1.6761033535003662\n","Training log: 58 epoch (16768 / 50000 train. data). Loss: 1.5133605003356934\n","Training log: 58 epoch (18048 / 50000 train. data). Loss: 1.4292829036712646\n","Training log: 58 epoch (19328 / 50000 train. data). Loss: 1.2124911546707153\n","Training log: 58 epoch (20608 / 50000 train. data). Loss: 1.2620774507522583\n","Training log: 58 epoch (21888 / 50000 train. data). Loss: 1.4787042140960693\n","Training log: 58 epoch (23168 / 50000 train. data). Loss: 1.3873437643051147\n","Training log: 58 epoch (24448 / 50000 train. data). Loss: 1.2642345428466797\n","Training log: 58 epoch (25728 / 50000 train. data). Loss: 1.3670930862426758\n","Training log: 58 epoch (27008 / 50000 train. data). Loss: 1.3321391344070435\n","Training log: 58 epoch (28288 / 50000 train. data). Loss: 1.3007315397262573\n","Training log: 58 epoch (29568 / 50000 train. data). Loss: 1.3700780868530273\n","Training log: 58 epoch (30848 / 50000 train. data). Loss: 1.2681283950805664\n","Training log: 58 epoch (32128 / 50000 train. data). Loss: 1.4230434894561768\n","Training log: 58 epoch (33408 / 50000 train. data). Loss: 1.339168906211853\n","Training log: 58 epoch (34688 / 50000 train. data). Loss: 1.2595853805541992\n","Training log: 58 epoch (35968 / 50000 train. data). Loss: 1.4499918222427368\n","Training log: 58 epoch (37248 / 50000 train. data). Loss: 1.3062041997909546\n","Training log: 58 epoch (38528 / 50000 train. data). Loss: 1.4011342525482178\n","Training log: 58 epoch (39808 / 50000 train. data). Loss: 1.4577029943466187\n","Training log: 58 epoch (41088 / 50000 train. data). Loss: 1.5213242769241333\n","Training log: 58 epoch (42368 / 50000 train. data). Loss: 1.3834534883499146\n","Training log: 58 epoch (43648 / 50000 train. data). Loss: 1.3312292098999023\n","Training log: 58 epoch (44928 / 50000 train. data). Loss: 1.3896466493606567\n","Training log: 58 epoch (46208 / 50000 train. data). Loss: 1.3522074222564697\n","Training log: 58 epoch (47488 / 50000 train. data). Loss: 1.4479045867919922\n","Training log: 58 epoch (48768 / 50000 train. data). Loss: 1.4333149194717407\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 34.05it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 58 epoch (50048 / 50000 train. data). Loss: 1.3795032501220703\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.67it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.40it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.553800\n","Training log: 59 epoch (128 / 50000 train. data). Loss: 1.533270239830017\n","Training log: 59 epoch (1408 / 50000 train. data). Loss: 1.3477288484573364\n","Training log: 59 epoch (2688 / 50000 train. data). Loss: 1.4087659120559692\n","Training log: 59 epoch (3968 / 50000 train. data). Loss: 1.4436231851577759\n","Training log: 59 epoch (5248 / 50000 train. data). Loss: 1.320469856262207\n","Training log: 59 epoch (6528 / 50000 train. data). Loss: 1.3602769374847412\n","Training log: 59 epoch (7808 / 50000 train. data). Loss: 1.446567416191101\n","Training log: 59 epoch (9088 / 50000 train. data). Loss: 1.300220251083374\n","Training log: 59 epoch (10368 / 50000 train. data). Loss: 1.4741206169128418\n","Training log: 59 epoch (11648 / 50000 train. data). Loss: 1.4120830297470093\n","Training log: 59 epoch (12928 / 50000 train. data). Loss: 1.3169844150543213\n","Training log: 59 epoch (14208 / 50000 train. data). Loss: 1.3731982707977295\n","Training log: 59 epoch (15488 / 50000 train. data). Loss: 1.3365801572799683\n","Training log: 59 epoch (16768 / 50000 train. data). Loss: 1.3913958072662354\n","Training log: 59 epoch (18048 / 50000 train. data). Loss: 1.3304823637008667\n","Training log: 59 epoch (19328 / 50000 train. data). Loss: 1.3054603338241577\n","Training log: 59 epoch (20608 / 50000 train. data). Loss: 1.266939640045166\n","Training log: 59 epoch (21888 / 50000 train. data). Loss: 1.3538106679916382\n","Training log: 59 epoch (23168 / 50000 train. data). Loss: 1.4679676294326782\n","Training log: 59 epoch (24448 / 50000 train. data). Loss: 1.547248363494873\n","Training log: 59 epoch (25728 / 50000 train. data). Loss: 1.4101684093475342\n","Training log: 59 epoch (27008 / 50000 train. data). Loss: 1.4372965097427368\n","Training log: 59 epoch (28288 / 50000 train. data). Loss: 1.4380298852920532\n","Training log: 59 epoch (29568 / 50000 train. data). Loss: 1.3816931247711182\n","Training log: 59 epoch (30848 / 50000 train. data). Loss: 1.3781137466430664\n","Training log: 59 epoch (32128 / 50000 train. data). Loss: 1.3729215860366821\n","Training log: 59 epoch (33408 / 50000 train. data). Loss: 1.4571163654327393\n","Training log: 59 epoch (34688 / 50000 train. data). Loss: 1.3794045448303223\n","Training log: 59 epoch (35968 / 50000 train. data). Loss: 1.39844810962677\n","Training log: 59 epoch (37248 / 50000 train. data). Loss: 1.3370213508605957\n","Training log: 59 epoch (38528 / 50000 train. data). Loss: 1.4522560834884644\n","Training log: 59 epoch (39808 / 50000 train. data). Loss: 1.4650206565856934\n","Training log: 59 epoch (41088 / 50000 train. data). Loss: 1.1793667078018188\n","Training log: 59 epoch (42368 / 50000 train. data). Loss: 1.2723051309585571\n","Training log: 59 epoch (43648 / 50000 train. data). Loss: 1.3462154865264893\n","Training log: 59 epoch (44928 / 50000 train. data). Loss: 1.4043790102005005\n","Training log: 59 epoch (46208 / 50000 train. data). Loss: 1.5361974239349365\n","Training log: 59 epoch (47488 / 50000 train. data). Loss: 1.2245365381240845\n","Training log: 59 epoch (48768 / 50000 train. data). Loss: 1.4736868143081665\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 59 epoch (50048 / 50000 train. data). Loss: 1.2694175243377686\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.10it/s]\n","100%|██████████| 79/79 [00:02<00:00, 32.08it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.558600\n","Training log: 60 epoch (128 / 50000 train. data). Loss: 1.3619033098220825\n","Training log: 60 epoch (1408 / 50000 train. data). Loss: 1.3352775573730469\n","Training log: 60 epoch (2688 / 50000 train. data). Loss: 1.2969125509262085\n","Training log: 60 epoch (3968 / 50000 train. data). Loss: 1.475689172744751\n","Training log: 60 epoch (5248 / 50000 train. data). Loss: 1.454805850982666\n","Training log: 60 epoch (6528 / 50000 train. data). Loss: 1.422008991241455\n","Training log: 60 epoch (7808 / 50000 train. data). Loss: 1.4629490375518799\n","Training log: 60 epoch (9088 / 50000 train. data). Loss: 1.4817396402359009\n","Training log: 60 epoch (10368 / 50000 train. data). Loss: 1.4976781606674194\n","Training log: 60 epoch (11648 / 50000 train. data). Loss: 1.3632169961929321\n","Training log: 60 epoch (12928 / 50000 train. data). Loss: 1.4145236015319824\n","Training log: 60 epoch (14208 / 50000 train. data). Loss: 1.4610095024108887\n","Training log: 60 epoch (15488 / 50000 train. data). Loss: 1.3652955293655396\n","Training log: 60 epoch (16768 / 50000 train. data). Loss: 1.5536189079284668\n","Training log: 60 epoch (18048 / 50000 train. data). Loss: 1.4856938123703003\n","Training log: 60 epoch (19328 / 50000 train. data). Loss: 1.5187160968780518\n","Training log: 60 epoch (20608 / 50000 train. data). Loss: 1.5472239255905151\n","Training log: 60 epoch (21888 / 50000 train. data). Loss: 1.4896159172058105\n","Training log: 60 epoch (23168 / 50000 train. data). Loss: 1.3602255582809448\n","Training log: 60 epoch (24448 / 50000 train. data). Loss: 1.3929249048233032\n","Training log: 60 epoch (25728 / 50000 train. data). Loss: 1.410038948059082\n","Training log: 60 epoch (27008 / 50000 train. data). Loss: 1.4725713729858398\n","Training log: 60 epoch (28288 / 50000 train. data). Loss: 1.3644086122512817\n","Training log: 60 epoch (29568 / 50000 train. data). Loss: 1.286327600479126\n","Training log: 60 epoch (30848 / 50000 train. data). Loss: 1.4800477027893066\n","Training log: 60 epoch (32128 / 50000 train. data). Loss: 1.3705272674560547\n","Training log: 60 epoch (33408 / 50000 train. data). Loss: 1.324407935142517\n","Training log: 60 epoch (34688 / 50000 train. data). Loss: 1.396424651145935\n","Training log: 60 epoch (35968 / 50000 train. data). Loss: 1.2763631343841553\n","Training log: 60 epoch (37248 / 50000 train. data). Loss: 1.298012375831604\n","Training log: 60 epoch (38528 / 50000 train. data). Loss: 1.407341480255127\n","Training log: 60 epoch (39808 / 50000 train. data). Loss: 1.2605974674224854\n","Training log: 60 epoch (41088 / 50000 train. data). Loss: 1.3400815725326538\n","Training log: 60 epoch (42368 / 50000 train. data). Loss: 1.3288042545318604\n","Training log: 60 epoch (43648 / 50000 train. data). Loss: 1.4292799234390259\n","Training log: 60 epoch (44928 / 50000 train. data). Loss: 1.423789620399475\n","Training log: 60 epoch (46208 / 50000 train. data). Loss: 1.4867230653762817\n","Training log: 60 epoch (47488 / 50000 train. data). Loss: 1.37471604347229\n","Training log: 60 epoch (48768 / 50000 train. data). Loss: 1.3880834579467773\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 33.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 60 epoch (50048 / 50000 train. data). Loss: 1.2390556335449219\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.22it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.31it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.558500\n","Training log: 61 epoch (128 / 50000 train. data). Loss: 1.198677897453308\n","Training log: 61 epoch (1408 / 50000 train. data). Loss: 1.338984727859497\n","Training log: 61 epoch (2688 / 50000 train. data). Loss: 1.3186355829238892\n","Training log: 61 epoch (3968 / 50000 train. data). Loss: 1.2944650650024414\n","Training log: 61 epoch (5248 / 50000 train. data). Loss: 1.3444839715957642\n","Training log: 61 epoch (6528 / 50000 train. data). Loss: 1.3624626398086548\n","Training log: 61 epoch (7808 / 50000 train. data). Loss: 1.2988955974578857\n","Training log: 61 epoch (9088 / 50000 train. data). Loss: 1.5355006456375122\n","Training log: 61 epoch (10368 / 50000 train. data). Loss: 1.3679207563400269\n","Training log: 61 epoch (11648 / 50000 train. data). Loss: 1.536402702331543\n","Training log: 61 epoch (12928 / 50000 train. data). Loss: 1.5429930686950684\n","Training log: 61 epoch (14208 / 50000 train. data). Loss: 1.456925630569458\n","Training log: 61 epoch (15488 / 50000 train. data). Loss: 1.4142481088638306\n","Training log: 61 epoch (16768 / 50000 train. data). Loss: 1.439697027206421\n","Training log: 61 epoch (18048 / 50000 train. data). Loss: 1.28257155418396\n","Training log: 61 epoch (19328 / 50000 train. data). Loss: 1.4543495178222656\n","Training log: 61 epoch (20608 / 50000 train. data). Loss: 1.2959129810333252\n","Training log: 61 epoch (21888 / 50000 train. data). Loss: 1.5065263509750366\n","Training log: 61 epoch (23168 / 50000 train. data). Loss: 1.3509061336517334\n","Training log: 61 epoch (24448 / 50000 train. data). Loss: 1.344274878501892\n","Training log: 61 epoch (25728 / 50000 train. data). Loss: 1.4471319913864136\n","Training log: 61 epoch (27008 / 50000 train. data). Loss: 1.212092399597168\n","Training log: 61 epoch (28288 / 50000 train. data). Loss: 1.2815911769866943\n","Training log: 61 epoch (29568 / 50000 train. data). Loss: 1.3148974180221558\n","Training log: 61 epoch (30848 / 50000 train. data). Loss: 1.2526934146881104\n","Training log: 61 epoch (32128 / 50000 train. data). Loss: 1.2763359546661377\n","Training log: 61 epoch (33408 / 50000 train. data). Loss: 1.308706521987915\n","Training log: 61 epoch (34688 / 50000 train. data). Loss: 1.4585771560668945\n","Training log: 61 epoch (35968 / 50000 train. data). Loss: 1.3711531162261963\n","Training log: 61 epoch (37248 / 50000 train. data). Loss: 1.4313020706176758\n","Training log: 61 epoch (38528 / 50000 train. data). Loss: 1.410750389099121\n","Training log: 61 epoch (39808 / 50000 train. data). Loss: 1.4275916814804077\n","Training log: 61 epoch (41088 / 50000 train. data). Loss: 1.47303307056427\n","Training log: 61 epoch (42368 / 50000 train. data). Loss: 1.345242977142334\n","Training log: 61 epoch (43648 / 50000 train. data). Loss: 1.386899471282959\n","Training log: 61 epoch (44928 / 50000 train. data). Loss: 1.358846664428711\n","Training log: 61 epoch (46208 / 50000 train. data). Loss: 1.2843977212905884\n","Training log: 61 epoch (47488 / 50000 train. data). Loss: 1.4209734201431274\n","Training log: 61 epoch (48768 / 50000 train. data). Loss: 1.26945960521698\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.47it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 61 epoch (50048 / 50000 train. data). Loss: 1.5219852924346924\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.96it/s]\n","100%|██████████| 79/79 [00:02<00:00, 32.54it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.565800\n","Training log: 62 epoch (128 / 50000 train. data). Loss: 1.3024734258651733\n","Training log: 62 epoch (1408 / 50000 train. data). Loss: 1.5361146926879883\n","Training log: 62 epoch (2688 / 50000 train. data). Loss: 1.343611478805542\n","Training log: 62 epoch (3968 / 50000 train. data). Loss: 1.5971927642822266\n","Training log: 62 epoch (5248 / 50000 train. data). Loss: 1.241579294204712\n","Training log: 62 epoch (6528 / 50000 train. data). Loss: 1.3670542240142822\n","Training log: 62 epoch (7808 / 50000 train. data). Loss: 1.5245695114135742\n","Training log: 62 epoch (9088 / 50000 train. data). Loss: 1.4167474508285522\n","Training log: 62 epoch (10368 / 50000 train. data). Loss: 1.349368929862976\n","Training log: 62 epoch (11648 / 50000 train. data). Loss: 1.3380985260009766\n","Training log: 62 epoch (12928 / 50000 train. data). Loss: 1.3368884325027466\n","Training log: 62 epoch (14208 / 50000 train. data). Loss: 1.3568713665008545\n","Training log: 62 epoch (15488 / 50000 train. data). Loss: 1.3860020637512207\n","Training log: 62 epoch (16768 / 50000 train. data). Loss: 1.4006731510162354\n","Training log: 62 epoch (18048 / 50000 train. data). Loss: 1.3626354932785034\n","Training log: 62 epoch (19328 / 50000 train. data). Loss: 1.2315559387207031\n","Training log: 62 epoch (20608 / 50000 train. data). Loss: 1.4030095338821411\n","Training log: 62 epoch (21888 / 50000 train. data). Loss: 1.4259581565856934\n","Training log: 62 epoch (23168 / 50000 train. data). Loss: 1.4927451610565186\n","Training log: 62 epoch (24448 / 50000 train. data). Loss: 1.3921860456466675\n","Training log: 62 epoch (25728 / 50000 train. data). Loss: 1.3917884826660156\n","Training log: 62 epoch (27008 / 50000 train. data). Loss: 1.5680381059646606\n","Training log: 62 epoch (28288 / 50000 train. data). Loss: 1.325199842453003\n","Training log: 62 epoch (29568 / 50000 train. data). Loss: 1.4705767631530762\n","Training log: 62 epoch (30848 / 50000 train. data). Loss: 1.23189377784729\n","Training log: 62 epoch (32128 / 50000 train. data). Loss: 1.4051496982574463\n","Training log: 62 epoch (33408 / 50000 train. data). Loss: 1.2930629253387451\n","Training log: 62 epoch (34688 / 50000 train. data). Loss: 1.203198790550232\n","Training log: 62 epoch (35968 / 50000 train. data). Loss: 1.3686994314193726\n","Training log: 62 epoch (37248 / 50000 train. data). Loss: 1.2748335599899292\n","Training log: 62 epoch (38528 / 50000 train. data). Loss: 1.4153196811676025\n","Training log: 62 epoch (39808 / 50000 train. data). Loss: 1.3353393077850342\n","Training log: 62 epoch (41088 / 50000 train. data). Loss: 1.4223638772964478\n","Training log: 62 epoch (42368 / 50000 train. data). Loss: 1.123962640762329\n","Training log: 62 epoch (43648 / 50000 train. data). Loss: 1.3358665704727173\n","Training log: 62 epoch (44928 / 50000 train. data). Loss: 1.3554543256759644\n","Training log: 62 epoch (46208 / 50000 train. data). Loss: 1.2606546878814697\n","Training log: 62 epoch (47488 / 50000 train. data). Loss: 1.3299859762191772\n","Training log: 62 epoch (48768 / 50000 train. data). Loss: 1.3551703691482544\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 33.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 62 epoch (50048 / 50000 train. data). Loss: 1.297539472579956\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.07it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.37it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.565500\n","Training log: 63 epoch (128 / 50000 train. data). Loss: 1.2321902513504028\n","Training log: 63 epoch (1408 / 50000 train. data). Loss: 1.227387547492981\n","Training log: 63 epoch (2688 / 50000 train. data). Loss: 1.5919228792190552\n","Training log: 63 epoch (3968 / 50000 train. data). Loss: 1.4125767946243286\n","Training log: 63 epoch (5248 / 50000 train. data). Loss: 1.2430663108825684\n","Training log: 63 epoch (6528 / 50000 train. data). Loss: 1.4518407583236694\n","Training log: 63 epoch (7808 / 50000 train. data). Loss: 1.289730429649353\n","Training log: 63 epoch (9088 / 50000 train. data). Loss: 1.3335891962051392\n","Training log: 63 epoch (10368 / 50000 train. data). Loss: 1.4490739107131958\n","Training log: 63 epoch (11648 / 50000 train. data). Loss: 1.409692645072937\n","Training log: 63 epoch (12928 / 50000 train. data). Loss: 1.486973524093628\n","Training log: 63 epoch (14208 / 50000 train. data). Loss: 1.2965519428253174\n","Training log: 63 epoch (15488 / 50000 train. data). Loss: 1.2392075061798096\n","Training log: 63 epoch (16768 / 50000 train. data). Loss: 1.375967264175415\n","Training log: 63 epoch (18048 / 50000 train. data). Loss: 1.3858184814453125\n","Training log: 63 epoch (19328 / 50000 train. data). Loss: 1.3182586431503296\n","Training log: 63 epoch (20608 / 50000 train. data). Loss: 1.3290833234786987\n","Training log: 63 epoch (21888 / 50000 train. data). Loss: 1.3633992671966553\n","Training log: 63 epoch (23168 / 50000 train. data). Loss: 1.4697030782699585\n","Training log: 63 epoch (24448 / 50000 train. data). Loss: 1.2868003845214844\n","Training log: 63 epoch (25728 / 50000 train. data). Loss: 1.415986180305481\n","Training log: 63 epoch (27008 / 50000 train. data). Loss: 1.4423021078109741\n","Training log: 63 epoch (28288 / 50000 train. data). Loss: 1.2773470878601074\n","Training log: 63 epoch (29568 / 50000 train. data). Loss: 1.3161437511444092\n","Training log: 63 epoch (30848 / 50000 train. data). Loss: 1.4258042573928833\n","Training log: 63 epoch (32128 / 50000 train. data). Loss: 1.5757559537887573\n","Training log: 63 epoch (33408 / 50000 train. data). Loss: 1.3680527210235596\n","Training log: 63 epoch (34688 / 50000 train. data). Loss: 1.3161237239837646\n","Training log: 63 epoch (35968 / 50000 train. data). Loss: 1.32099187374115\n","Training log: 63 epoch (37248 / 50000 train. data). Loss: 1.3313084840774536\n","Training log: 63 epoch (38528 / 50000 train. data). Loss: 1.3278396129608154\n","Training log: 63 epoch (39808 / 50000 train. data). Loss: 1.3954691886901855\n","Training log: 63 epoch (41088 / 50000 train. data). Loss: 1.448036789894104\n","Training log: 63 epoch (42368 / 50000 train. data). Loss: 1.6742318868637085\n","Training log: 63 epoch (43648 / 50000 train. data). Loss: 1.3834257125854492\n","Training log: 63 epoch (44928 / 50000 train. data). Loss: 1.4001132249832153\n","Training log: 63 epoch (46208 / 50000 train. data). Loss: 1.293083906173706\n","Training log: 63 epoch (47488 / 50000 train. data). Loss: 1.2683240175247192\n","Training log: 63 epoch (48768 / 50000 train. data). Loss: 1.4859800338745117\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.10it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 63 epoch (50048 / 50000 train. data). Loss: 1.431732177734375\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.98it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.40it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.561700\n","Training log: 64 epoch (128 / 50000 train. data). Loss: 1.3317646980285645\n","Training log: 64 epoch (1408 / 50000 train. data). Loss: 1.4414606094360352\n","Training log: 64 epoch (2688 / 50000 train. data). Loss: 1.5081878900527954\n","Training log: 64 epoch (3968 / 50000 train. data). Loss: 1.4360548257827759\n","Training log: 64 epoch (5248 / 50000 train. data). Loss: 1.4373598098754883\n","Training log: 64 epoch (6528 / 50000 train. data). Loss: 1.3903844356536865\n","Training log: 64 epoch (7808 / 50000 train. data). Loss: 1.3637962341308594\n","Training log: 64 epoch (9088 / 50000 train. data). Loss: 1.5081862211227417\n","Training log: 64 epoch (10368 / 50000 train. data). Loss: 1.4295929670333862\n","Training log: 64 epoch (11648 / 50000 train. data). Loss: 1.5130685567855835\n","Training log: 64 epoch (12928 / 50000 train. data). Loss: 1.2145355939865112\n","Training log: 64 epoch (14208 / 50000 train. data). Loss: 1.382485270500183\n","Training log: 64 epoch (15488 / 50000 train. data). Loss: 1.3730491399765015\n","Training log: 64 epoch (16768 / 50000 train. data). Loss: 1.2463151216506958\n","Training log: 64 epoch (18048 / 50000 train. data). Loss: 1.2817845344543457\n","Training log: 64 epoch (19328 / 50000 train. data). Loss: 1.4797333478927612\n","Training log: 64 epoch (20608 / 50000 train. data). Loss: 1.5647964477539062\n","Training log: 64 epoch (21888 / 50000 train. data). Loss: 1.363844871520996\n","Training log: 64 epoch (23168 / 50000 train. data). Loss: 1.4023170471191406\n","Training log: 64 epoch (24448 / 50000 train. data). Loss: 1.3036524057388306\n","Training log: 64 epoch (25728 / 50000 train. data). Loss: 1.376006841659546\n","Training log: 64 epoch (27008 / 50000 train. data). Loss: 1.2241454124450684\n","Training log: 64 epoch (28288 / 50000 train. data). Loss: 1.3985815048217773\n","Training log: 64 epoch (29568 / 50000 train. data). Loss: 1.4044651985168457\n","Training log: 64 epoch (30848 / 50000 train. data). Loss: 1.2834913730621338\n","Training log: 64 epoch (32128 / 50000 train. data). Loss: 1.211717128753662\n","Training log: 64 epoch (33408 / 50000 train. data). Loss: 1.3643879890441895\n","Training log: 64 epoch (34688 / 50000 train. data). Loss: 1.2927770614624023\n","Training log: 64 epoch (35968 / 50000 train. data). Loss: 1.3770087957382202\n","Training log: 64 epoch (37248 / 50000 train. data). Loss: 1.5501022338867188\n","Training log: 64 epoch (38528 / 50000 train. data). Loss: 1.265379548072815\n","Training log: 64 epoch (39808 / 50000 train. data). Loss: 1.2780671119689941\n","Training log: 64 epoch (41088 / 50000 train. data). Loss: 1.4321130514144897\n","Training log: 64 epoch (42368 / 50000 train. data). Loss: 1.569061517715454\n","Training log: 64 epoch (43648 / 50000 train. data). Loss: 1.1883491277694702\n","Training log: 64 epoch (44928 / 50000 train. data). Loss: 1.3619307279586792\n","Training log: 64 epoch (46208 / 50000 train. data). Loss: 1.3183876276016235\n","Training log: 64 epoch (47488 / 50000 train. data). Loss: 1.3117355108261108\n","Training log: 64 epoch (48768 / 50000 train. data). Loss: 1.2348871231079102\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 64 epoch (50048 / 50000 train. data). Loss: 1.2484647035598755\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.22it/s]\n","100%|██████████| 79/79 [00:02<00:00, 32.17it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.566800\n","Training log: 65 epoch (128 / 50000 train. data). Loss: 1.3899235725402832\n","Training log: 65 epoch (1408 / 50000 train. data). Loss: 1.394115924835205\n","Training log: 65 epoch (2688 / 50000 train. data). Loss: 1.3449064493179321\n","Training log: 65 epoch (3968 / 50000 train. data). Loss: 1.3062574863433838\n","Training log: 65 epoch (5248 / 50000 train. data). Loss: 1.4073739051818848\n","Training log: 65 epoch (6528 / 50000 train. data). Loss: 1.452459454536438\n","Training log: 65 epoch (7808 / 50000 train. data). Loss: 1.3235670328140259\n","Training log: 65 epoch (9088 / 50000 train. data). Loss: 1.4759801626205444\n","Training log: 65 epoch (10368 / 50000 train. data). Loss: 1.277296543121338\n","Training log: 65 epoch (11648 / 50000 train. data). Loss: 1.2803187370300293\n","Training log: 65 epoch (12928 / 50000 train. data). Loss: 1.1982067823410034\n","Training log: 65 epoch (14208 / 50000 train. data). Loss: 1.3672707080841064\n","Training log: 65 epoch (15488 / 50000 train. data). Loss: 1.2597200870513916\n","Training log: 65 epoch (16768 / 50000 train. data). Loss: 1.332403540611267\n","Training log: 65 epoch (18048 / 50000 train. data). Loss: 1.2399239540100098\n","Training log: 65 epoch (19328 / 50000 train. data). Loss: 1.397450566291809\n","Training log: 65 epoch (20608 / 50000 train. data). Loss: 1.332399606704712\n","Training log: 65 epoch (21888 / 50000 train. data). Loss: 1.4714164733886719\n","Training log: 65 epoch (23168 / 50000 train. data). Loss: 1.4802347421646118\n","Training log: 65 epoch (24448 / 50000 train. data). Loss: 1.3525288105010986\n","Training log: 65 epoch (25728 / 50000 train. data). Loss: 1.4364657402038574\n","Training log: 65 epoch (27008 / 50000 train. data). Loss: 1.3280309438705444\n","Training log: 65 epoch (28288 / 50000 train. data). Loss: 1.2655560970306396\n","Training log: 65 epoch (29568 / 50000 train. data). Loss: 1.2649948596954346\n","Training log: 65 epoch (30848 / 50000 train. data). Loss: 1.2947940826416016\n","Training log: 65 epoch (32128 / 50000 train. data). Loss: 1.320752739906311\n","Training log: 65 epoch (33408 / 50000 train. data). Loss: 1.2532895803451538\n","Training log: 65 epoch (34688 / 50000 train. data). Loss: 1.6456629037857056\n","Training log: 65 epoch (35968 / 50000 train. data). Loss: 1.2505096197128296\n","Training log: 65 epoch (37248 / 50000 train. data). Loss: 1.5746797323226929\n","Training log: 65 epoch (38528 / 50000 train. data). Loss: 1.192861795425415\n","Training log: 65 epoch (39808 / 50000 train. data). Loss: 1.1612887382507324\n","Training log: 65 epoch (41088 / 50000 train. data). Loss: 1.3686825037002563\n","Training log: 65 epoch (42368 / 50000 train. data). Loss: 1.2213318347930908\n","Training log: 65 epoch (43648 / 50000 train. data). Loss: 1.400285005569458\n","Training log: 65 epoch (44928 / 50000 train. data). Loss: 1.3414925336837769\n","Training log: 65 epoch (46208 / 50000 train. data). Loss: 1.2675143480300903\n","Training log: 65 epoch (47488 / 50000 train. data). Loss: 1.242969036102295\n","Training log: 65 epoch (48768 / 50000 train. data). Loss: 1.2984672784805298\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 65 epoch (50048 / 50000 train. data). Loss: 1.3751128911972046\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.69it/s]\n","100%|██████████| 79/79 [00:02<00:00, 32.71it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.567000\n","Training log: 66 epoch (128 / 50000 train. data). Loss: 1.4799365997314453\n","Training log: 66 epoch (1408 / 50000 train. data). Loss: 1.3442047834396362\n","Training log: 66 epoch (2688 / 50000 train. data). Loss: 1.3524540662765503\n","Training log: 66 epoch (3968 / 50000 train. data). Loss: 1.3393805027008057\n","Training log: 66 epoch (5248 / 50000 train. data). Loss: 1.1614246368408203\n","Training log: 66 epoch (6528 / 50000 train. data). Loss: 1.3981170654296875\n","Training log: 66 epoch (7808 / 50000 train. data). Loss: 1.3568124771118164\n","Training log: 66 epoch (9088 / 50000 train. data). Loss: 1.3953416347503662\n","Training log: 66 epoch (10368 / 50000 train. data). Loss: 1.317230463027954\n","Training log: 66 epoch (11648 / 50000 train. data). Loss: 1.2656068801879883\n","Training log: 66 epoch (12928 / 50000 train. data). Loss: 1.2405130863189697\n","Training log: 66 epoch (14208 / 50000 train. data). Loss: 1.350268006324768\n","Training log: 66 epoch (15488 / 50000 train. data). Loss: 1.4561291933059692\n","Training log: 66 epoch (16768 / 50000 train. data). Loss: 1.409517526626587\n","Training log: 66 epoch (18048 / 50000 train. data). Loss: 1.294189453125\n","Training log: 66 epoch (19328 / 50000 train. data). Loss: 1.4815210103988647\n","Training log: 66 epoch (20608 / 50000 train. data). Loss: 1.449657917022705\n","Training log: 66 epoch (21888 / 50000 train. data). Loss: 1.3745561838150024\n","Training log: 66 epoch (23168 / 50000 train. data). Loss: 1.260434627532959\n","Training log: 66 epoch (24448 / 50000 train. data). Loss: 1.4463307857513428\n","Training log: 66 epoch (25728 / 50000 train. data). Loss: 1.3069061040878296\n","Training log: 66 epoch (27008 / 50000 train. data). Loss: 1.3479859828948975\n","Training log: 66 epoch (28288 / 50000 train. data). Loss: 1.4137569665908813\n","Training log: 66 epoch (29568 / 50000 train. data). Loss: 1.408308744430542\n","Training log: 66 epoch (30848 / 50000 train. data). Loss: 1.2090190649032593\n","Training log: 66 epoch (32128 / 50000 train. data). Loss: 1.2724003791809082\n","Training log: 66 epoch (33408 / 50000 train. data). Loss: 1.2890712022781372\n","Training log: 66 epoch (34688 / 50000 train. data). Loss: 1.3953015804290771\n","Training log: 66 epoch (35968 / 50000 train. data). Loss: 1.1953837871551514\n","Training log: 66 epoch (37248 / 50000 train. data). Loss: 1.2009646892547607\n","Training log: 66 epoch (38528 / 50000 train. data). Loss: 1.4552630186080933\n","Training log: 66 epoch (39808 / 50000 train. data). Loss: 1.372099757194519\n","Training log: 66 epoch (41088 / 50000 train. data). Loss: 1.3358345031738281\n","Training log: 66 epoch (42368 / 50000 train. data). Loss: 1.3314099311828613\n","Training log: 66 epoch (43648 / 50000 train. data). Loss: 1.3147361278533936\n","Training log: 66 epoch (44928 / 50000 train. data). Loss: 1.4985040426254272\n","Training log: 66 epoch (46208 / 50000 train. data). Loss: 1.3827956914901733\n","Training log: 66 epoch (47488 / 50000 train. data). Loss: 1.2416690587997437\n","Training log: 66 epoch (48768 / 50000 train. data). Loss: 1.5026755332946777\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 66 epoch (50048 / 50000 train. data). Loss: 1.492244005203247\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.07it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.94it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.567000\n","Training log: 67 epoch (128 / 50000 train. data). Loss: 1.3758476972579956\n","Training log: 67 epoch (1408 / 50000 train. data). Loss: 1.407626986503601\n","Training log: 67 epoch (2688 / 50000 train. data). Loss: 1.3667669296264648\n","Training log: 67 epoch (3968 / 50000 train. data). Loss: 1.4418853521347046\n","Training log: 67 epoch (5248 / 50000 train. data). Loss: 1.3052177429199219\n","Training log: 67 epoch (6528 / 50000 train. data). Loss: 1.5137877464294434\n","Training log: 67 epoch (7808 / 50000 train. data). Loss: 1.3778959512710571\n","Training log: 67 epoch (9088 / 50000 train. data). Loss: 1.3344253301620483\n","Training log: 67 epoch (10368 / 50000 train. data). Loss: 1.3751572370529175\n","Training log: 67 epoch (11648 / 50000 train. data). Loss: 1.3894596099853516\n","Training log: 67 epoch (12928 / 50000 train. data). Loss: 1.5715973377227783\n","Training log: 67 epoch (14208 / 50000 train. data). Loss: 1.2961598634719849\n","Training log: 67 epoch (15488 / 50000 train. data). Loss: 1.3084310293197632\n","Training log: 67 epoch (16768 / 50000 train. data). Loss: 1.4685897827148438\n","Training log: 67 epoch (18048 / 50000 train. data). Loss: 1.366059422492981\n","Training log: 67 epoch (19328 / 50000 train. data). Loss: 1.4685165882110596\n","Training log: 67 epoch (20608 / 50000 train. data). Loss: 1.2446906566619873\n","Training log: 67 epoch (21888 / 50000 train. data). Loss: 1.4072835445404053\n","Training log: 67 epoch (23168 / 50000 train. data). Loss: 1.3032947778701782\n","Training log: 67 epoch (24448 / 50000 train. data). Loss: 1.2245886325836182\n","Training log: 67 epoch (25728 / 50000 train. data). Loss: 1.4337947368621826\n","Training log: 67 epoch (27008 / 50000 train. data). Loss: 1.28139066696167\n","Training log: 67 epoch (28288 / 50000 train. data). Loss: 1.3644920587539673\n","Training log: 67 epoch (29568 / 50000 train. data). Loss: 1.2685325145721436\n","Training log: 67 epoch (30848 / 50000 train. data). Loss: 1.2787606716156006\n","Training log: 67 epoch (32128 / 50000 train. data). Loss: 1.300876498222351\n","Training log: 67 epoch (33408 / 50000 train. data). Loss: 1.3764253854751587\n","Training log: 67 epoch (34688 / 50000 train. data). Loss: 1.3857771158218384\n","Training log: 67 epoch (35968 / 50000 train. data). Loss: 1.290023922920227\n","Training log: 67 epoch (37248 / 50000 train. data). Loss: 1.286512017250061\n","Training log: 67 epoch (38528 / 50000 train. data). Loss: 1.4734901189804077\n","Training log: 67 epoch (39808 / 50000 train. data). Loss: 1.26372230052948\n","Training log: 67 epoch (41088 / 50000 train. data). Loss: 1.3782674074172974\n","Training log: 67 epoch (42368 / 50000 train. data). Loss: 1.3310686349868774\n","Training log: 67 epoch (43648 / 50000 train. data). Loss: 1.3998897075653076\n","Training log: 67 epoch (44928 / 50000 train. data). Loss: 1.2738243341445923\n","Training log: 67 epoch (46208 / 50000 train. data). Loss: 1.5271327495574951\n","Training log: 67 epoch (47488 / 50000 train. data). Loss: 1.2905962467193604\n","Training log: 67 epoch (48768 / 50000 train. data). Loss: 1.3751903772354126\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 67 epoch (50048 / 50000 train. data). Loss: 1.3523083925247192\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.64it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.66it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.570500\n","Training log: 68 epoch (128 / 50000 train. data). Loss: 1.4041545391082764\n","Training log: 68 epoch (1408 / 50000 train. data). Loss: 1.309465765953064\n","Training log: 68 epoch (2688 / 50000 train. data). Loss: 1.4328505992889404\n","Training log: 68 epoch (3968 / 50000 train. data). Loss: 1.2835965156555176\n","Training log: 68 epoch (5248 / 50000 train. data). Loss: 1.2930967807769775\n","Training log: 68 epoch (6528 / 50000 train. data). Loss: 1.5331159830093384\n","Training log: 68 epoch (7808 / 50000 train. data). Loss: 1.4869670867919922\n","Training log: 68 epoch (9088 / 50000 train. data). Loss: 1.567430853843689\n","Training log: 68 epoch (10368 / 50000 train. data). Loss: 1.1368142366409302\n","Training log: 68 epoch (11648 / 50000 train. data). Loss: 1.5113914012908936\n","Training log: 68 epoch (12928 / 50000 train. data). Loss: 1.2764174938201904\n","Training log: 68 epoch (14208 / 50000 train. data). Loss: 1.3557989597320557\n","Training log: 68 epoch (15488 / 50000 train. data). Loss: 1.1979461908340454\n","Training log: 68 epoch (16768 / 50000 train. data). Loss: 1.2688422203063965\n","Training log: 68 epoch (18048 / 50000 train. data). Loss: 1.3521615266799927\n","Training log: 68 epoch (19328 / 50000 train. data). Loss: 1.3175079822540283\n","Training log: 68 epoch (20608 / 50000 train. data). Loss: 1.3591595888137817\n","Training log: 68 epoch (21888 / 50000 train. data). Loss: 1.3594858646392822\n","Training log: 68 epoch (23168 / 50000 train. data). Loss: 1.4102630615234375\n","Training log: 68 epoch (24448 / 50000 train. data). Loss: 1.5774078369140625\n","Training log: 68 epoch (25728 / 50000 train. data). Loss: 1.2262248992919922\n","Training log: 68 epoch (27008 / 50000 train. data). Loss: 1.4058414697647095\n","Training log: 68 epoch (28288 / 50000 train. data). Loss: 1.253982663154602\n","Training log: 68 epoch (29568 / 50000 train. data). Loss: 1.42111337184906\n","Training log: 68 epoch (30848 / 50000 train. data). Loss: 1.333216905593872\n","Training log: 68 epoch (32128 / 50000 train. data). Loss: 1.4882118701934814\n","Training log: 68 epoch (33408 / 50000 train. data). Loss: 1.226078987121582\n","Training log: 68 epoch (34688 / 50000 train. data). Loss: 1.3506824970245361\n","Training log: 68 epoch (35968 / 50000 train. data). Loss: 1.3207448720932007\n","Training log: 68 epoch (37248 / 50000 train. data). Loss: 1.3627370595932007\n","Training log: 68 epoch (38528 / 50000 train. data). Loss: 1.3869588375091553\n","Training log: 68 epoch (39808 / 50000 train. data). Loss: 1.40004563331604\n","Training log: 68 epoch (41088 / 50000 train. data). Loss: 1.4187003374099731\n","Training log: 68 epoch (42368 / 50000 train. data). Loss: 1.34956955909729\n","Training log: 68 epoch (43648 / 50000 train. data). Loss: 1.3995774984359741\n","Training log: 68 epoch (44928 / 50000 train. data). Loss: 1.4357349872589111\n","Training log: 68 epoch (46208 / 50000 train. data). Loss: 1.2754156589508057\n","Training log: 68 epoch (47488 / 50000 train. data). Loss: 1.289902687072754\n","Training log: 68 epoch (48768 / 50000 train. data). Loss: 1.1982730627059937\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.15it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 68 epoch (50048 / 50000 train. data). Loss: 1.4840867519378662\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.97it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.02it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.572500\n","Training log: 69 epoch (128 / 50000 train. data). Loss: 1.3569526672363281\n","Training log: 69 epoch (1408 / 50000 train. data). Loss: 1.3823935985565186\n","Training log: 69 epoch (2688 / 50000 train. data). Loss: 1.2819980382919312\n","Training log: 69 epoch (3968 / 50000 train. data). Loss: 1.3943017721176147\n","Training log: 69 epoch (5248 / 50000 train. data). Loss: 1.2913049459457397\n","Training log: 69 epoch (6528 / 50000 train. data). Loss: 1.2835304737091064\n","Training log: 69 epoch (7808 / 50000 train. data). Loss: 1.3499000072479248\n","Training log: 69 epoch (9088 / 50000 train. data). Loss: 1.3313474655151367\n","Training log: 69 epoch (10368 / 50000 train. data). Loss: 1.558815598487854\n","Training log: 69 epoch (11648 / 50000 train. data). Loss: 1.2672560214996338\n","Training log: 69 epoch (12928 / 50000 train. data). Loss: 1.3026776313781738\n","Training log: 69 epoch (14208 / 50000 train. data). Loss: 1.4710384607315063\n","Training log: 69 epoch (15488 / 50000 train. data). Loss: 1.4273126125335693\n","Training log: 69 epoch (16768 / 50000 train. data). Loss: 1.4117995500564575\n","Training log: 69 epoch (18048 / 50000 train. data). Loss: 1.4868438243865967\n","Training log: 69 epoch (19328 / 50000 train. data). Loss: 1.489217758178711\n","Training log: 69 epoch (20608 / 50000 train. data). Loss: 1.3579838275909424\n","Training log: 69 epoch (21888 / 50000 train. data). Loss: 1.333822250366211\n","Training log: 69 epoch (23168 / 50000 train. data). Loss: 1.3943946361541748\n","Training log: 69 epoch (24448 / 50000 train. data). Loss: 1.362310767173767\n","Training log: 69 epoch (25728 / 50000 train. data). Loss: 1.2178593873977661\n","Training log: 69 epoch (27008 / 50000 train. data). Loss: 1.220686912536621\n","Training log: 69 epoch (28288 / 50000 train. data). Loss: 1.3278602361679077\n","Training log: 69 epoch (29568 / 50000 train. data). Loss: 1.6135929822921753\n","Training log: 69 epoch (30848 / 50000 train. data). Loss: 1.3705781698226929\n","Training log: 69 epoch (32128 / 50000 train. data). Loss: 1.3801742792129517\n","Training log: 69 epoch (33408 / 50000 train. data). Loss: 1.4449596405029297\n","Training log: 69 epoch (34688 / 50000 train. data). Loss: 1.2284401655197144\n","Training log: 69 epoch (35968 / 50000 train. data). Loss: 1.2963943481445312\n","Training log: 69 epoch (37248 / 50000 train. data). Loss: 1.348762035369873\n","Training log: 69 epoch (38528 / 50000 train. data). Loss: 1.2955654859542847\n","Training log: 69 epoch (39808 / 50000 train. data). Loss: 1.289872169494629\n","Training log: 69 epoch (41088 / 50000 train. data). Loss: 1.3849493265151978\n","Training log: 69 epoch (42368 / 50000 train. data). Loss: 1.3598145246505737\n","Training log: 69 epoch (43648 / 50000 train. data). Loss: 1.4909875392913818\n","Training log: 69 epoch (44928 / 50000 train. data). Loss: 1.3121695518493652\n","Training log: 69 epoch (46208 / 50000 train. data). Loss: 1.3350470066070557\n","Training log: 69 epoch (47488 / 50000 train. data). Loss: 1.3987233638763428\n","Training log: 69 epoch (48768 / 50000 train. data). Loss: 1.295195460319519\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 69 epoch (50048 / 50000 train. data). Loss: 1.4155621528625488\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.76it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.13it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.569400\n","Training log: 70 epoch (128 / 50000 train. data). Loss: 1.2788982391357422\n","Training log: 70 epoch (1408 / 50000 train. data). Loss: 1.279426097869873\n","Training log: 70 epoch (2688 / 50000 train. data). Loss: 1.3605321645736694\n","Training log: 70 epoch (3968 / 50000 train. data). Loss: 1.3209936618804932\n","Training log: 70 epoch (5248 / 50000 train. data). Loss: 1.3596887588500977\n","Training log: 70 epoch (6528 / 50000 train. data). Loss: 1.3532636165618896\n","Training log: 70 epoch (7808 / 50000 train. data). Loss: 1.2423923015594482\n","Training log: 70 epoch (9088 / 50000 train. data). Loss: 1.4371217489242554\n","Training log: 70 epoch (10368 / 50000 train. data). Loss: 1.280790090560913\n","Training log: 70 epoch (11648 / 50000 train. data). Loss: 1.2222859859466553\n","Training log: 70 epoch (12928 / 50000 train. data). Loss: 1.3734558820724487\n","Training log: 70 epoch (14208 / 50000 train. data). Loss: 1.3098390102386475\n","Training log: 70 epoch (15488 / 50000 train. data). Loss: 1.3506927490234375\n","Training log: 70 epoch (16768 / 50000 train. data). Loss: 1.334855556488037\n","Training log: 70 epoch (18048 / 50000 train. data). Loss: 1.3865845203399658\n","Training log: 70 epoch (19328 / 50000 train. data). Loss: 1.3171414136886597\n","Training log: 70 epoch (20608 / 50000 train. data). Loss: 1.5443564653396606\n","Training log: 70 epoch (21888 / 50000 train. data). Loss: 1.3999154567718506\n","Training log: 70 epoch (23168 / 50000 train. data). Loss: 1.4365367889404297\n","Training log: 70 epoch (24448 / 50000 train. data). Loss: 1.2018461227416992\n","Training log: 70 epoch (25728 / 50000 train. data). Loss: 1.4143638610839844\n","Training log: 70 epoch (27008 / 50000 train. data). Loss: 1.303252100944519\n","Training log: 70 epoch (28288 / 50000 train. data). Loss: 1.4376651048660278\n","Training log: 70 epoch (29568 / 50000 train. data). Loss: 1.5149120092391968\n","Training log: 70 epoch (30848 / 50000 train. data). Loss: 1.3485790491104126\n","Training log: 70 epoch (32128 / 50000 train. data). Loss: 1.4366239309310913\n","Training log: 70 epoch (33408 / 50000 train. data). Loss: 1.4385851621627808\n","Training log: 70 epoch (34688 / 50000 train. data). Loss: 1.328568696975708\n","Training log: 70 epoch (35968 / 50000 train. data). Loss: 1.5156428813934326\n","Training log: 70 epoch (37248 / 50000 train. data). Loss: 1.3357605934143066\n","Training log: 70 epoch (38528 / 50000 train. data). Loss: 1.4320542812347412\n","Training log: 70 epoch (39808 / 50000 train. data). Loss: 1.3469104766845703\n","Training log: 70 epoch (41088 / 50000 train. data). Loss: 1.417935848236084\n","Training log: 70 epoch (42368 / 50000 train. data). Loss: 1.375877857208252\n","Training log: 70 epoch (43648 / 50000 train. data). Loss: 1.4071266651153564\n","Training log: 70 epoch (44928 / 50000 train. data). Loss: 1.3731889724731445\n","Training log: 70 epoch (46208 / 50000 train. data). Loss: 1.3299639225006104\n","Training log: 70 epoch (47488 / 50000 train. data). Loss: 1.413496971130371\n","Training log: 70 epoch (48768 / 50000 train. data). Loss: 1.3463263511657715\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.27it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 70 epoch (50048 / 50000 train. data). Loss: 1.214706540107727\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.13it/s]\n","100%|██████████| 79/79 [00:02<00:00, 32.65it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.572700\n","Training log: 71 epoch (128 / 50000 train. data). Loss: 1.3055118322372437\n","Training log: 71 epoch (1408 / 50000 train. data). Loss: 1.4631152153015137\n","Training log: 71 epoch (2688 / 50000 train. data). Loss: 1.3679890632629395\n","Training log: 71 epoch (3968 / 50000 train. data). Loss: 1.3296549320220947\n","Training log: 71 epoch (5248 / 50000 train. data). Loss: 1.28532874584198\n","Training log: 71 epoch (6528 / 50000 train. data). Loss: 1.3337101936340332\n","Training log: 71 epoch (7808 / 50000 train. data). Loss: 1.4377936124801636\n","Training log: 71 epoch (9088 / 50000 train. data). Loss: 1.6057312488555908\n","Training log: 71 epoch (10368 / 50000 train. data). Loss: 1.3840059041976929\n","Training log: 71 epoch (11648 / 50000 train. data). Loss: 1.190040946006775\n","Training log: 71 epoch (12928 / 50000 train. data). Loss: 1.439531683921814\n","Training log: 71 epoch (14208 / 50000 train. data). Loss: 1.2930914163589478\n","Training log: 71 epoch (15488 / 50000 train. data). Loss: 1.3155373334884644\n","Training log: 71 epoch (16768 / 50000 train. data). Loss: 1.4753594398498535\n","Training log: 71 epoch (18048 / 50000 train. data). Loss: 1.3488502502441406\n","Training log: 71 epoch (19328 / 50000 train. data). Loss: 1.3073686361312866\n","Training log: 71 epoch (20608 / 50000 train. data). Loss: 1.459938883781433\n","Training log: 71 epoch (21888 / 50000 train. data). Loss: 1.5003634691238403\n","Training log: 71 epoch (23168 / 50000 train. data). Loss: 1.3373956680297852\n","Training log: 71 epoch (24448 / 50000 train. data). Loss: 1.37881338596344\n","Training log: 71 epoch (25728 / 50000 train. data). Loss: 1.3619277477264404\n","Training log: 71 epoch (27008 / 50000 train. data). Loss: 1.5071942806243896\n","Training log: 71 epoch (28288 / 50000 train. data). Loss: 1.3983769416809082\n","Training log: 71 epoch (29568 / 50000 train. data). Loss: 1.4100126028060913\n","Training log: 71 epoch (30848 / 50000 train. data). Loss: 1.5834009647369385\n","Training log: 71 epoch (32128 / 50000 train. data). Loss: 1.3906673192977905\n","Training log: 71 epoch (33408 / 50000 train. data). Loss: 1.2890676259994507\n","Training log: 71 epoch (34688 / 50000 train. data). Loss: 1.3845138549804688\n","Training log: 71 epoch (35968 / 50000 train. data). Loss: 1.3049075603485107\n","Training log: 71 epoch (37248 / 50000 train. data). Loss: 1.2399873733520508\n","Training log: 71 epoch (38528 / 50000 train. data). Loss: 1.3645689487457275\n","Training log: 71 epoch (39808 / 50000 train. data). Loss: 1.341694951057434\n","Training log: 71 epoch (41088 / 50000 train. data). Loss: 1.3618518114089966\n","Training log: 71 epoch (42368 / 50000 train. data). Loss: 1.3495595455169678\n","Training log: 71 epoch (43648 / 50000 train. data). Loss: 1.4566075801849365\n","Training log: 71 epoch (44928 / 50000 train. data). Loss: 1.2961474657058716\n","Training log: 71 epoch (46208 / 50000 train. data). Loss: 1.402370810508728\n","Training log: 71 epoch (47488 / 50000 train. data). Loss: 1.4134454727172852\n","Training log: 71 epoch (48768 / 50000 train. data). Loss: 1.2662760019302368\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 33.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 71 epoch (50048 / 50000 train. data). Loss: 1.3478463888168335\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.22it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.576500\n","Training log: 72 epoch (128 / 50000 train. data). Loss: 1.2775535583496094\n","Training log: 72 epoch (1408 / 50000 train. data). Loss: 1.4406133890151978\n","Training log: 72 epoch (2688 / 50000 train. data). Loss: 1.1673890352249146\n","Training log: 72 epoch (3968 / 50000 train. data). Loss: 1.4102283716201782\n","Training log: 72 epoch (5248 / 50000 train. data). Loss: 1.2932716608047485\n","Training log: 72 epoch (6528 / 50000 train. data). Loss: 1.2961453199386597\n","Training log: 72 epoch (7808 / 50000 train. data). Loss: 1.407939076423645\n","Training log: 72 epoch (9088 / 50000 train. data). Loss: 1.3314138650894165\n","Training log: 72 epoch (10368 / 50000 train. data). Loss: 1.320479393005371\n","Training log: 72 epoch (11648 / 50000 train. data). Loss: 1.1860510110855103\n","Training log: 72 epoch (12928 / 50000 train. data). Loss: 1.289210557937622\n","Training log: 72 epoch (14208 / 50000 train. data). Loss: 1.315861463546753\n","Training log: 72 epoch (15488 / 50000 train. data). Loss: 1.4518643617630005\n","Training log: 72 epoch (16768 / 50000 train. data). Loss: 1.3378708362579346\n","Training log: 72 epoch (18048 / 50000 train. data). Loss: 1.575394868850708\n","Training log: 72 epoch (19328 / 50000 train. data). Loss: 1.3673255443572998\n","Training log: 72 epoch (20608 / 50000 train. data). Loss: 1.4967478513717651\n","Training log: 72 epoch (21888 / 50000 train. data). Loss: 1.3932280540466309\n","Training log: 72 epoch (23168 / 50000 train. data). Loss: 1.5733469724655151\n","Training log: 72 epoch (24448 / 50000 train. data). Loss: 1.5105384588241577\n","Training log: 72 epoch (25728 / 50000 train. data). Loss: 1.4161293506622314\n","Training log: 72 epoch (27008 / 50000 train. data). Loss: 1.4704210758209229\n","Training log: 72 epoch (28288 / 50000 train. data). Loss: 1.458445429801941\n","Training log: 72 epoch (29568 / 50000 train. data). Loss: 1.3268791437149048\n","Training log: 72 epoch (30848 / 50000 train. data). Loss: 1.384833812713623\n","Training log: 72 epoch (32128 / 50000 train. data). Loss: 1.231104850769043\n","Training log: 72 epoch (33408 / 50000 train. data). Loss: 1.434963583946228\n","Training log: 72 epoch (34688 / 50000 train. data). Loss: 1.318780541419983\n","Training log: 72 epoch (35968 / 50000 train. data). Loss: 1.3205657005310059\n","Training log: 72 epoch (37248 / 50000 train. data). Loss: 1.4661036729812622\n","Training log: 72 epoch (38528 / 50000 train. data). Loss: 1.3418105840682983\n","Training log: 72 epoch (39808 / 50000 train. data). Loss: 1.333754062652588\n","Training log: 72 epoch (41088 / 50000 train. data). Loss: 1.298016905784607\n","Training log: 72 epoch (42368 / 50000 train. data). Loss: 1.2889207601547241\n","Training log: 72 epoch (43648 / 50000 train. data). Loss: 1.422203540802002\n","Training log: 72 epoch (44928 / 50000 train. data). Loss: 1.3485771417617798\n","Training log: 72 epoch (46208 / 50000 train. data). Loss: 1.4903744459152222\n","Training log: 72 epoch (47488 / 50000 train. data). Loss: 1.2966197729110718\n","Training log: 72 epoch (48768 / 50000 train. data). Loss: 1.3333097696304321\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 72 epoch (50048 / 50000 train. data). Loss: 1.362613558769226\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.40it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.42it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.572800\n","Training log: 73 epoch (128 / 50000 train. data). Loss: 1.3508492708206177\n","Training log: 73 epoch (1408 / 50000 train. data). Loss: 1.2410252094268799\n","Training log: 73 epoch (2688 / 50000 train. data). Loss: 1.205020546913147\n","Training log: 73 epoch (3968 / 50000 train. data). Loss: 1.4220483303070068\n","Training log: 73 epoch (5248 / 50000 train. data). Loss: 1.4153878688812256\n","Training log: 73 epoch (6528 / 50000 train. data). Loss: 1.295385479927063\n","Training log: 73 epoch (7808 / 50000 train. data). Loss: 1.2002960443496704\n","Training log: 73 epoch (9088 / 50000 train. data). Loss: 1.311418890953064\n","Training log: 73 epoch (10368 / 50000 train. data). Loss: 1.2616876363754272\n","Training log: 73 epoch (11648 / 50000 train. data). Loss: 1.2855935096740723\n","Training log: 73 epoch (12928 / 50000 train. data). Loss: 1.2546043395996094\n","Training log: 73 epoch (14208 / 50000 train. data). Loss: 1.266827940940857\n","Training log: 73 epoch (15488 / 50000 train. data). Loss: 1.449409008026123\n","Training log: 73 epoch (16768 / 50000 train. data). Loss: 1.4881789684295654\n","Training log: 73 epoch (18048 / 50000 train. data). Loss: 1.4871022701263428\n","Training log: 73 epoch (19328 / 50000 train. data). Loss: 1.169824481010437\n","Training log: 73 epoch (20608 / 50000 train. data). Loss: 1.108063817024231\n","Training log: 73 epoch (21888 / 50000 train. data). Loss: 1.433670163154602\n","Training log: 73 epoch (23168 / 50000 train. data). Loss: 1.1827431917190552\n","Training log: 73 epoch (24448 / 50000 train. data). Loss: 1.352346420288086\n","Training log: 73 epoch (25728 / 50000 train. data). Loss: 1.388296127319336\n","Training log: 73 epoch (27008 / 50000 train. data). Loss: 1.2044751644134521\n","Training log: 73 epoch (28288 / 50000 train. data). Loss: 1.3483268022537231\n","Training log: 73 epoch (29568 / 50000 train. data). Loss: 1.3590915203094482\n","Training log: 73 epoch (30848 / 50000 train. data). Loss: 1.2758787870407104\n","Training log: 73 epoch (32128 / 50000 train. data). Loss: 1.2341227531433105\n","Training log: 73 epoch (33408 / 50000 train. data). Loss: 1.2279822826385498\n","Training log: 73 epoch (34688 / 50000 train. data). Loss: 1.4269744157791138\n","Training log: 73 epoch (35968 / 50000 train. data). Loss: 1.3484067916870117\n","Training log: 73 epoch (37248 / 50000 train. data). Loss: 1.2975447177886963\n","Training log: 73 epoch (38528 / 50000 train. data). Loss: 1.3200125694274902\n","Training log: 73 epoch (39808 / 50000 train. data). Loss: 1.418112874031067\n","Training log: 73 epoch (41088 / 50000 train. data). Loss: 1.416500449180603\n","Training log: 73 epoch (42368 / 50000 train. data). Loss: 1.204724907875061\n","Training log: 73 epoch (43648 / 50000 train. data). Loss: 1.416244387626648\n","Training log: 73 epoch (44928 / 50000 train. data). Loss: 1.31631600856781\n","Training log: 73 epoch (46208 / 50000 train. data). Loss: 1.1611642837524414\n","Training log: 73 epoch (47488 / 50000 train. data). Loss: 1.4955146312713623\n","Training log: 73 epoch (48768 / 50000 train. data). Loss: 1.372857928276062\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 34.72it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 73 epoch (50048 / 50000 train. data). Loss: 1.267470121383667\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.83it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.34it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.575900\n","Training log: 74 epoch (128 / 50000 train. data). Loss: 1.1413403749465942\n","Training log: 74 epoch (1408 / 50000 train. data). Loss: 1.1720093488693237\n","Training log: 74 epoch (2688 / 50000 train. data). Loss: 1.3413587808609009\n","Training log: 74 epoch (3968 / 50000 train. data). Loss: 1.5496755838394165\n","Training log: 74 epoch (5248 / 50000 train. data). Loss: 1.4880311489105225\n","Training log: 74 epoch (6528 / 50000 train. data). Loss: 1.3566128015518188\n","Training log: 74 epoch (7808 / 50000 train. data). Loss: 1.204939842224121\n","Training log: 74 epoch (9088 / 50000 train. data). Loss: 1.3999314308166504\n","Training log: 74 epoch (10368 / 50000 train. data). Loss: 1.361132025718689\n","Training log: 74 epoch (11648 / 50000 train. data). Loss: 1.4249646663665771\n","Training log: 74 epoch (12928 / 50000 train. data). Loss: 1.4054925441741943\n","Training log: 74 epoch (14208 / 50000 train. data). Loss: 1.3524271249771118\n","Training log: 74 epoch (15488 / 50000 train. data). Loss: 1.1374952793121338\n","Training log: 74 epoch (16768 / 50000 train. data). Loss: 1.3676910400390625\n","Training log: 74 epoch (18048 / 50000 train. data). Loss: 1.1559624671936035\n","Training log: 74 epoch (19328 / 50000 train. data). Loss: 1.3798167705535889\n","Training log: 74 epoch (20608 / 50000 train. data). Loss: 1.403175711631775\n","Training log: 74 epoch (21888 / 50000 train. data). Loss: 1.3188773393630981\n","Training log: 74 epoch (23168 / 50000 train. data). Loss: 1.2144620418548584\n","Training log: 74 epoch (24448 / 50000 train. data). Loss: 1.379279613494873\n","Training log: 74 epoch (25728 / 50000 train. data). Loss: 1.4038386344909668\n","Training log: 74 epoch (27008 / 50000 train. data). Loss: 1.2861887216567993\n","Training log: 74 epoch (28288 / 50000 train. data). Loss: 1.2283101081848145\n","Training log: 74 epoch (29568 / 50000 train. data). Loss: 1.3082771301269531\n","Training log: 74 epoch (30848 / 50000 train. data). Loss: 1.4372419118881226\n","Training log: 74 epoch (32128 / 50000 train. data). Loss: 1.2471675872802734\n","Training log: 74 epoch (33408 / 50000 train. data). Loss: 1.6975935697555542\n","Training log: 74 epoch (34688 / 50000 train. data). Loss: 1.2797043323516846\n","Training log: 74 epoch (35968 / 50000 train. data). Loss: 1.2725886106491089\n","Training log: 74 epoch (37248 / 50000 train. data). Loss: 1.2252799272537231\n","Training log: 74 epoch (38528 / 50000 train. data). Loss: 1.3129686117172241\n","Training log: 74 epoch (39808 / 50000 train. data). Loss: 1.4920320510864258\n","Training log: 74 epoch (41088 / 50000 train. data). Loss: 1.316500186920166\n","Training log: 74 epoch (42368 / 50000 train. data). Loss: 1.4098646640777588\n","Training log: 74 epoch (43648 / 50000 train. data). Loss: 1.4856922626495361\n","Training log: 74 epoch (44928 / 50000 train. data). Loss: 1.4284238815307617\n","Training log: 74 epoch (46208 / 50000 train. data). Loss: 1.34605872631073\n","Training log: 74 epoch (47488 / 50000 train. data). Loss: 1.3480515480041504\n","Training log: 74 epoch (48768 / 50000 train. data). Loss: 1.2231523990631104\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 3/391 [00:00<00:15, 25.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 74 epoch (50048 / 50000 train. data). Loss: 1.2041375637054443\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.40it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.04it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.578800\n","Training log: 75 epoch (128 / 50000 train. data). Loss: 1.447931170463562\n","Training log: 75 epoch (1408 / 50000 train. data). Loss: 1.29400634765625\n","Training log: 75 epoch (2688 / 50000 train. data). Loss: 1.3667151927947998\n","Training log: 75 epoch (3968 / 50000 train. data). Loss: 1.3431707620620728\n","Training log: 75 epoch (5248 / 50000 train. data). Loss: 1.2624213695526123\n","Training log: 75 epoch (6528 / 50000 train. data). Loss: 1.3798571825027466\n","Training log: 75 epoch (7808 / 50000 train. data). Loss: 1.4363429546356201\n","Training log: 75 epoch (9088 / 50000 train. data). Loss: 1.2832605838775635\n","Training log: 75 epoch (10368 / 50000 train. data). Loss: 1.3198142051696777\n","Training log: 75 epoch (11648 / 50000 train. data). Loss: 1.4424411058425903\n","Training log: 75 epoch (12928 / 50000 train. data). Loss: 1.361040472984314\n","Training log: 75 epoch (14208 / 50000 train. data). Loss: 1.471189260482788\n","Training log: 75 epoch (15488 / 50000 train. data). Loss: 1.3122118711471558\n","Training log: 75 epoch (16768 / 50000 train. data). Loss: 1.397704839706421\n","Training log: 75 epoch (18048 / 50000 train. data). Loss: 1.1997525691986084\n","Training log: 75 epoch (19328 / 50000 train. data). Loss: 1.3166704177856445\n","Training log: 75 epoch (20608 / 50000 train. data). Loss: 1.3924872875213623\n","Training log: 75 epoch (21888 / 50000 train. data). Loss: 1.5196646451950073\n","Training log: 75 epoch (23168 / 50000 train. data). Loss: 1.35152006149292\n","Training log: 75 epoch (24448 / 50000 train. data). Loss: 1.4442625045776367\n","Training log: 75 epoch (25728 / 50000 train. data). Loss: 1.4585164785385132\n","Training log: 75 epoch (27008 / 50000 train. data). Loss: 1.3273142576217651\n","Training log: 75 epoch (28288 / 50000 train. data). Loss: 1.227454423904419\n","Training log: 75 epoch (29568 / 50000 train. data). Loss: 1.419631004333496\n","Training log: 75 epoch (30848 / 50000 train. data). Loss: 1.4790918827056885\n","Training log: 75 epoch (32128 / 50000 train. data). Loss: 1.3620411157608032\n","Training log: 75 epoch (33408 / 50000 train. data). Loss: 1.3289809226989746\n","Training log: 75 epoch (34688 / 50000 train. data). Loss: 1.4087843894958496\n","Training log: 75 epoch (35968 / 50000 train. data). Loss: 1.4604859352111816\n","Training log: 75 epoch (37248 / 50000 train. data). Loss: 1.5036383867263794\n","Training log: 75 epoch (38528 / 50000 train. data). Loss: 1.3516054153442383\n","Training log: 75 epoch (39808 / 50000 train. data). Loss: 1.3558337688446045\n","Training log: 75 epoch (41088 / 50000 train. data). Loss: 1.426637887954712\n","Training log: 75 epoch (42368 / 50000 train. data). Loss: 1.2896448373794556\n","Training log: 75 epoch (43648 / 50000 train. data). Loss: 1.2428641319274902\n","Training log: 75 epoch (44928 / 50000 train. data). Loss: 1.2558764219284058\n","Training log: 75 epoch (46208 / 50000 train. data). Loss: 1.1748182773590088\n","Training log: 75 epoch (47488 / 50000 train. data). Loss: 1.4894764423370361\n","Training log: 75 epoch (48768 / 50000 train. data). Loss: 1.2849282026290894\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:12, 30.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 75 epoch (50048 / 50000 train. data). Loss: 1.3882663249969482\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.60it/s]\n","100%|██████████| 79/79 [00:02<00:00, 32.08it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.583200\n","Training log: 76 epoch (128 / 50000 train. data). Loss: 1.302693247795105\n","Training log: 76 epoch (1408 / 50000 train. data). Loss: 1.2594410181045532\n","Training log: 76 epoch (2688 / 50000 train. data). Loss: 1.5436115264892578\n","Training log: 76 epoch (3968 / 50000 train. data). Loss: 1.419832706451416\n","Training log: 76 epoch (5248 / 50000 train. data). Loss: 1.4111062288284302\n","Training log: 76 epoch (6528 / 50000 train. data). Loss: 1.3358274698257446\n","Training log: 76 epoch (7808 / 50000 train. data). Loss: 1.5835585594177246\n","Training log: 76 epoch (9088 / 50000 train. data). Loss: 1.279831886291504\n","Training log: 76 epoch (10368 / 50000 train. data). Loss: 1.3621480464935303\n","Training log: 76 epoch (11648 / 50000 train. data). Loss: 1.410725474357605\n","Training log: 76 epoch (12928 / 50000 train. data). Loss: 1.3819235563278198\n","Training log: 76 epoch (14208 / 50000 train. data). Loss: 1.4371639490127563\n","Training log: 76 epoch (15488 / 50000 train. data). Loss: 1.2731603384017944\n","Training log: 76 epoch (16768 / 50000 train. data). Loss: 1.3517159223556519\n","Training log: 76 epoch (18048 / 50000 train. data). Loss: 1.2901363372802734\n","Training log: 76 epoch (19328 / 50000 train. data). Loss: 1.3840651512145996\n","Training log: 76 epoch (20608 / 50000 train. data). Loss: 1.3875466585159302\n","Training log: 76 epoch (21888 / 50000 train. data). Loss: 1.3731369972229004\n","Training log: 76 epoch (23168 / 50000 train. data). Loss: 1.3264976739883423\n","Training log: 76 epoch (24448 / 50000 train. data). Loss: 1.3328934907913208\n","Training log: 76 epoch (25728 / 50000 train. data). Loss: 1.3878562450408936\n","Training log: 76 epoch (27008 / 50000 train. data). Loss: 1.378739595413208\n","Training log: 76 epoch (28288 / 50000 train. data). Loss: 1.407423973083496\n","Training log: 76 epoch (29568 / 50000 train. data). Loss: 1.2741087675094604\n","Training log: 76 epoch (30848 / 50000 train. data). Loss: 1.4681408405303955\n","Training log: 76 epoch (32128 / 50000 train. data). Loss: 1.343885898590088\n","Training log: 76 epoch (33408 / 50000 train. data). Loss: 1.408081293106079\n","Training log: 76 epoch (34688 / 50000 train. data). Loss: 1.1898293495178223\n","Training log: 76 epoch (35968 / 50000 train. data). Loss: 1.332725167274475\n","Training log: 76 epoch (37248 / 50000 train. data). Loss: 1.1640877723693848\n","Training log: 76 epoch (38528 / 50000 train. data). Loss: 1.2800824642181396\n","Training log: 76 epoch (39808 / 50000 train. data). Loss: 1.478692650794983\n","Training log: 76 epoch (41088 / 50000 train. data). Loss: 1.3807727098464966\n","Training log: 76 epoch (42368 / 50000 train. data). Loss: 1.3552342653274536\n","Training log: 76 epoch (43648 / 50000 train. data). Loss: 1.2617671489715576\n","Training log: 76 epoch (44928 / 50000 train. data). Loss: 1.28305184841156\n","Training log: 76 epoch (46208 / 50000 train. data). Loss: 1.305031657218933\n","Training log: 76 epoch (47488 / 50000 train. data). Loss: 1.3241840600967407\n","Training log: 76 epoch (48768 / 50000 train. data). Loss: 1.485205054283142\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 3/391 [00:00<00:13, 28.05it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 76 epoch (50048 / 50000 train. data). Loss: 1.5631624460220337\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.58it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.23it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.583900\n","Training log: 77 epoch (128 / 50000 train. data). Loss: 1.379290223121643\n","Training log: 77 epoch (1408 / 50000 train. data). Loss: 1.5205529928207397\n","Training log: 77 epoch (2688 / 50000 train. data). Loss: 1.4247205257415771\n","Training log: 77 epoch (3968 / 50000 train. data). Loss: 1.4673740863800049\n","Training log: 77 epoch (5248 / 50000 train. data). Loss: 1.3710566759109497\n","Training log: 77 epoch (6528 / 50000 train. data). Loss: 1.3816453218460083\n","Training log: 77 epoch (7808 / 50000 train. data). Loss: 1.2373770475387573\n","Training log: 77 epoch (9088 / 50000 train. data). Loss: 1.3008301258087158\n","Training log: 77 epoch (10368 / 50000 train. data). Loss: 1.3079047203063965\n","Training log: 77 epoch (11648 / 50000 train. data). Loss: 1.4491828680038452\n","Training log: 77 epoch (12928 / 50000 train. data). Loss: 1.3002839088439941\n","Training log: 77 epoch (14208 / 50000 train. data). Loss: 1.3601094484329224\n","Training log: 77 epoch (15488 / 50000 train. data). Loss: 1.3489925861358643\n","Training log: 77 epoch (16768 / 50000 train. data). Loss: 1.2731835842132568\n","Training log: 77 epoch (18048 / 50000 train. data). Loss: 1.249329924583435\n","Training log: 77 epoch (19328 / 50000 train. data). Loss: 1.4059648513793945\n","Training log: 77 epoch (20608 / 50000 train. data). Loss: 1.5599606037139893\n","Training log: 77 epoch (21888 / 50000 train. data). Loss: 1.18729567527771\n","Training log: 77 epoch (23168 / 50000 train. data). Loss: 1.3289096355438232\n","Training log: 77 epoch (24448 / 50000 train. data). Loss: 1.363877534866333\n","Training log: 77 epoch (25728 / 50000 train. data). Loss: 1.319120168685913\n","Training log: 77 epoch (27008 / 50000 train. data). Loss: 1.4278850555419922\n","Training log: 77 epoch (28288 / 50000 train. data). Loss: 1.342738389968872\n","Training log: 77 epoch (29568 / 50000 train. data). Loss: 1.1423991918563843\n","Training log: 77 epoch (30848 / 50000 train. data). Loss: 1.5457396507263184\n","Training log: 77 epoch (32128 / 50000 train. data). Loss: 1.3619494438171387\n","Training log: 77 epoch (33408 / 50000 train. data). Loss: 1.33656907081604\n","Training log: 77 epoch (34688 / 50000 train. data). Loss: 1.348171591758728\n","Training log: 77 epoch (35968 / 50000 train. data). Loss: 1.293593168258667\n","Training log: 77 epoch (37248 / 50000 train. data). Loss: 1.444151520729065\n","Training log: 77 epoch (38528 / 50000 train. data). Loss: 1.3224116563796997\n","Training log: 77 epoch (39808 / 50000 train. data). Loss: 1.4473090171813965\n","Training log: 77 epoch (41088 / 50000 train. data). Loss: 1.2582708597183228\n","Training log: 77 epoch (42368 / 50000 train. data). Loss: 1.280155897140503\n","Training log: 77 epoch (43648 / 50000 train. data). Loss: 1.4288945198059082\n","Training log: 77 epoch (44928 / 50000 train. data). Loss: 1.3506108522415161\n","Training log: 77 epoch (46208 / 50000 train. data). Loss: 1.345598816871643\n","Training log: 77 epoch (47488 / 50000 train. data). Loss: 1.3453034162521362\n","Training log: 77 epoch (48768 / 50000 train. data). Loss: 1.2299754619598389\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 77 epoch (50048 / 50000 train. data). Loss: 1.1942923069000244\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.74it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.87it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.583400\n","Training log: 78 epoch (128 / 50000 train. data). Loss: 1.3518929481506348\n","Training log: 78 epoch (1408 / 50000 train. data). Loss: 1.3114503622055054\n","Training log: 78 epoch (2688 / 50000 train. data). Loss: 1.2405542135238647\n","Training log: 78 epoch (3968 / 50000 train. data). Loss: 1.411208987236023\n","Training log: 78 epoch (5248 / 50000 train. data). Loss: 1.3676888942718506\n","Training log: 78 epoch (6528 / 50000 train. data). Loss: 1.3283542394638062\n","Training log: 78 epoch (7808 / 50000 train. data). Loss: 1.4335787296295166\n","Training log: 78 epoch (9088 / 50000 train. data). Loss: 1.3309006690979004\n","Training log: 78 epoch (10368 / 50000 train. data). Loss: 1.3213682174682617\n","Training log: 78 epoch (11648 / 50000 train. data). Loss: 1.3494926691055298\n","Training log: 78 epoch (12928 / 50000 train. data). Loss: 1.2332805395126343\n","Training log: 78 epoch (14208 / 50000 train. data). Loss: 1.4260917901992798\n","Training log: 78 epoch (15488 / 50000 train. data). Loss: 1.4152581691741943\n","Training log: 78 epoch (16768 / 50000 train. data). Loss: 1.403557538986206\n","Training log: 78 epoch (18048 / 50000 train. data). Loss: 1.267856240272522\n","Training log: 78 epoch (19328 / 50000 train. data). Loss: 1.2405776977539062\n","Training log: 78 epoch (20608 / 50000 train. data). Loss: 1.42463219165802\n","Training log: 78 epoch (21888 / 50000 train. data). Loss: 1.3954089879989624\n","Training log: 78 epoch (23168 / 50000 train. data). Loss: 1.3851913213729858\n","Training log: 78 epoch (24448 / 50000 train. data). Loss: 1.3098081350326538\n","Training log: 78 epoch (25728 / 50000 train. data). Loss: 1.3850665092468262\n","Training log: 78 epoch (27008 / 50000 train. data). Loss: 1.3396384716033936\n","Training log: 78 epoch (28288 / 50000 train. data). Loss: 1.2951148748397827\n","Training log: 78 epoch (29568 / 50000 train. data). Loss: 1.2180182933807373\n","Training log: 78 epoch (30848 / 50000 train. data). Loss: 1.1656641960144043\n","Training log: 78 epoch (32128 / 50000 train. data). Loss: 1.2569013833999634\n","Training log: 78 epoch (33408 / 50000 train. data). Loss: 1.6342308521270752\n","Training log: 78 epoch (34688 / 50000 train. data). Loss: 1.214888572692871\n","Training log: 78 epoch (35968 / 50000 train. data). Loss: 1.3476200103759766\n","Training log: 78 epoch (37248 / 50000 train. data). Loss: 1.4017871618270874\n","Training log: 78 epoch (38528 / 50000 train. data). Loss: 1.443124532699585\n","Training log: 78 epoch (39808 / 50000 train. data). Loss: 1.4853582382202148\n","Training log: 78 epoch (41088 / 50000 train. data). Loss: 1.2863670587539673\n","Training log: 78 epoch (42368 / 50000 train. data). Loss: 1.25130033493042\n","Training log: 78 epoch (43648 / 50000 train. data). Loss: 1.3371033668518066\n","Training log: 78 epoch (44928 / 50000 train. data). Loss: 1.2674957513809204\n","Training log: 78 epoch (46208 / 50000 train. data). Loss: 1.3690773248672485\n","Training log: 78 epoch (47488 / 50000 train. data). Loss: 1.2152918577194214\n","Training log: 78 epoch (48768 / 50000 train. data). Loss: 1.3773717880249023\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.14it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 78 epoch (50048 / 50000 train. data). Loss: 1.3340365886688232\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.41it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.23it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.586100\n","Training log: 79 epoch (128 / 50000 train. data). Loss: 1.2642929553985596\n","Training log: 79 epoch (1408 / 50000 train. data). Loss: 1.4295825958251953\n","Training log: 79 epoch (2688 / 50000 train. data). Loss: 1.3088328838348389\n","Training log: 79 epoch (3968 / 50000 train. data). Loss: 1.3401761054992676\n","Training log: 79 epoch (5248 / 50000 train. data). Loss: 1.303213119506836\n","Training log: 79 epoch (6528 / 50000 train. data). Loss: 1.3385179042816162\n","Training log: 79 epoch (7808 / 50000 train. data). Loss: 1.2790191173553467\n","Training log: 79 epoch (9088 / 50000 train. data). Loss: 1.31621253490448\n","Training log: 79 epoch (10368 / 50000 train. data). Loss: 1.4078128337860107\n","Training log: 79 epoch (11648 / 50000 train. data). Loss: 1.288148283958435\n","Training log: 79 epoch (12928 / 50000 train. data). Loss: 1.1661345958709717\n","Training log: 79 epoch (14208 / 50000 train. data). Loss: 1.2667078971862793\n","Training log: 79 epoch (15488 / 50000 train. data). Loss: 1.2033214569091797\n","Training log: 79 epoch (16768 / 50000 train. data). Loss: 1.3280253410339355\n","Training log: 79 epoch (18048 / 50000 train. data). Loss: 1.1719732284545898\n","Training log: 79 epoch (19328 / 50000 train. data). Loss: 1.243215799331665\n","Training log: 79 epoch (20608 / 50000 train. data). Loss: 1.36139714717865\n","Training log: 79 epoch (21888 / 50000 train. data). Loss: 1.2789629697799683\n","Training log: 79 epoch (23168 / 50000 train. data). Loss: 1.3976961374282837\n","Training log: 79 epoch (24448 / 50000 train. data). Loss: 1.294554352760315\n","Training log: 79 epoch (25728 / 50000 train. data). Loss: 1.4011894464492798\n","Training log: 79 epoch (27008 / 50000 train. data). Loss: 1.368963599205017\n","Training log: 79 epoch (28288 / 50000 train. data). Loss: 1.3414592742919922\n","Training log: 79 epoch (29568 / 50000 train. data). Loss: 1.4787601232528687\n","Training log: 79 epoch (30848 / 50000 train. data). Loss: 1.250874638557434\n","Training log: 79 epoch (32128 / 50000 train. data). Loss: 1.3163683414459229\n","Training log: 79 epoch (33408 / 50000 train. data). Loss: 1.3541022539138794\n","Training log: 79 epoch (34688 / 50000 train. data). Loss: 1.3450599908828735\n","Training log: 79 epoch (35968 / 50000 train. data). Loss: 1.3009949922561646\n","Training log: 79 epoch (37248 / 50000 train. data). Loss: 1.3562159538269043\n","Training log: 79 epoch (38528 / 50000 train. data). Loss: 1.384137749671936\n","Training log: 79 epoch (39808 / 50000 train. data). Loss: 1.2438410520553589\n","Training log: 79 epoch (41088 / 50000 train. data). Loss: 1.2876986265182495\n","Training log: 79 epoch (42368 / 50000 train. data). Loss: 1.3997879028320312\n","Training log: 79 epoch (43648 / 50000 train. data). Loss: 1.4105424880981445\n","Training log: 79 epoch (44928 / 50000 train. data). Loss: 1.175700306892395\n","Training log: 79 epoch (46208 / 50000 train. data). Loss: 1.4645822048187256\n","Training log: 79 epoch (47488 / 50000 train. data). Loss: 1.365503191947937\n","Training log: 79 epoch (48768 / 50000 train. data). Loss: 1.5020718574523926\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 79 epoch (50048 / 50000 train. data). Loss: 1.1926956176757812\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.85it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.59it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.577400\n","Training log: 80 epoch (128 / 50000 train. data). Loss: 1.3205703496932983\n","Training log: 80 epoch (1408 / 50000 train. data). Loss: 1.4146828651428223\n","Training log: 80 epoch (2688 / 50000 train. data). Loss: 1.4850640296936035\n","Training log: 80 epoch (3968 / 50000 train. data). Loss: 1.5408408641815186\n","Training log: 80 epoch (5248 / 50000 train. data). Loss: 1.433152675628662\n","Training log: 80 epoch (6528 / 50000 train. data). Loss: 1.5024107694625854\n","Training log: 80 epoch (7808 / 50000 train. data). Loss: 1.185405969619751\n","Training log: 80 epoch (9088 / 50000 train. data). Loss: 1.198397159576416\n","Training log: 80 epoch (10368 / 50000 train. data). Loss: 1.277894139289856\n","Training log: 80 epoch (11648 / 50000 train. data). Loss: 1.3362350463867188\n","Training log: 80 epoch (12928 / 50000 train. data). Loss: 1.3384267091751099\n","Training log: 80 epoch (14208 / 50000 train. data). Loss: 1.2608472108840942\n","Training log: 80 epoch (15488 / 50000 train. data). Loss: 1.2982158660888672\n","Training log: 80 epoch (16768 / 50000 train. data). Loss: 1.2978614568710327\n","Training log: 80 epoch (18048 / 50000 train. data). Loss: 1.3411870002746582\n","Training log: 80 epoch (19328 / 50000 train. data). Loss: 1.2364305257797241\n","Training log: 80 epoch (20608 / 50000 train. data). Loss: 1.4325759410858154\n","Training log: 80 epoch (21888 / 50000 train. data). Loss: 1.3890959024429321\n","Training log: 80 epoch (23168 / 50000 train. data). Loss: 1.470457911491394\n","Training log: 80 epoch (24448 / 50000 train. data). Loss: 1.3087623119354248\n","Training log: 80 epoch (25728 / 50000 train. data). Loss: 1.2698153257369995\n","Training log: 80 epoch (27008 / 50000 train. data). Loss: 1.2276771068572998\n","Training log: 80 epoch (28288 / 50000 train. data). Loss: 1.4352521896362305\n","Training log: 80 epoch (29568 / 50000 train. data). Loss: 1.417150616645813\n","Training log: 80 epoch (30848 / 50000 train. data). Loss: 1.1688560247421265\n","Training log: 80 epoch (32128 / 50000 train. data). Loss: 1.222389817237854\n","Training log: 80 epoch (33408 / 50000 train. data). Loss: 1.335376501083374\n","Training log: 80 epoch (34688 / 50000 train. data). Loss: 1.4727439880371094\n","Training log: 80 epoch (35968 / 50000 train. data). Loss: 1.3295475244522095\n","Training log: 80 epoch (37248 / 50000 train. data). Loss: 1.482160210609436\n","Training log: 80 epoch (38528 / 50000 train. data). Loss: 1.4288716316223145\n","Training log: 80 epoch (39808 / 50000 train. data). Loss: 1.2359766960144043\n","Training log: 80 epoch (41088 / 50000 train. data). Loss: 1.3708916902542114\n","Training log: 80 epoch (42368 / 50000 train. data). Loss: 1.279401183128357\n","Training log: 80 epoch (43648 / 50000 train. data). Loss: 1.2742300033569336\n","Training log: 80 epoch (44928 / 50000 train. data). Loss: 1.4582808017730713\n","Training log: 80 epoch (46208 / 50000 train. data). Loss: 1.4003204107284546\n","Training log: 80 epoch (47488 / 50000 train. data). Loss: 1.3920189142227173\n","Training log: 80 epoch (48768 / 50000 train. data). Loss: 1.4664762020111084\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 80 epoch (50048 / 50000 train. data). Loss: 1.4457370042800903\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.33it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.23it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.586000\n","Training log: 81 epoch (128 / 50000 train. data). Loss: 1.2496157884597778\n","Training log: 81 epoch (1408 / 50000 train. data). Loss: 1.2585958242416382\n","Training log: 81 epoch (2688 / 50000 train. data). Loss: 1.497897744178772\n","Training log: 81 epoch (3968 / 50000 train. data). Loss: 1.3084269762039185\n","Training log: 81 epoch (5248 / 50000 train. data). Loss: 1.263493537902832\n","Training log: 81 epoch (6528 / 50000 train. data). Loss: 1.3115854263305664\n","Training log: 81 epoch (7808 / 50000 train. data). Loss: 1.4137119054794312\n","Training log: 81 epoch (9088 / 50000 train. data). Loss: 1.3417351245880127\n","Training log: 81 epoch (10368 / 50000 train. data). Loss: 1.1710166931152344\n","Training log: 81 epoch (11648 / 50000 train. data). Loss: 1.2642654180526733\n","Training log: 81 epoch (12928 / 50000 train. data). Loss: 1.2957981824874878\n","Training log: 81 epoch (14208 / 50000 train. data). Loss: 1.3829231262207031\n","Training log: 81 epoch (15488 / 50000 train. data). Loss: 1.4367191791534424\n","Training log: 81 epoch (16768 / 50000 train. data). Loss: 1.4062237739562988\n","Training log: 81 epoch (18048 / 50000 train. data). Loss: 1.349780797958374\n","Training log: 81 epoch (19328 / 50000 train. data). Loss: 1.408835530281067\n","Training log: 81 epoch (20608 / 50000 train. data). Loss: 1.25735604763031\n","Training log: 81 epoch (21888 / 50000 train. data). Loss: 1.291420817375183\n","Training log: 81 epoch (23168 / 50000 train. data). Loss: 1.31040358543396\n","Training log: 81 epoch (24448 / 50000 train. data). Loss: 1.3092252016067505\n","Training log: 81 epoch (25728 / 50000 train. data). Loss: 1.3912783861160278\n","Training log: 81 epoch (27008 / 50000 train. data). Loss: 1.2119728326797485\n","Training log: 81 epoch (28288 / 50000 train. data). Loss: 1.518093228340149\n","Training log: 81 epoch (29568 / 50000 train. data). Loss: 1.5069117546081543\n","Training log: 81 epoch (30848 / 50000 train. data). Loss: 1.4358171224594116\n","Training log: 81 epoch (32128 / 50000 train. data). Loss: 1.2453161478042603\n","Training log: 81 epoch (33408 / 50000 train. data). Loss: 1.4129455089569092\n","Training log: 81 epoch (34688 / 50000 train. data). Loss: 1.2873433828353882\n","Training log: 81 epoch (35968 / 50000 train. data). Loss: 1.2941372394561768\n","Training log: 81 epoch (37248 / 50000 train. data). Loss: 1.2569539546966553\n","Training log: 81 epoch (38528 / 50000 train. data). Loss: 1.4211266040802002\n","Training log: 81 epoch (39808 / 50000 train. data). Loss: 1.3941190242767334\n","Training log: 81 epoch (41088 / 50000 train. data). Loss: 1.3304191827774048\n","Training log: 81 epoch (42368 / 50000 train. data). Loss: 1.2398117780685425\n","Training log: 81 epoch (43648 / 50000 train. data). Loss: 1.3481019735336304\n","Training log: 81 epoch (44928 / 50000 train. data). Loss: 1.3886643648147583\n","Training log: 81 epoch (46208 / 50000 train. data). Loss: 1.3298903703689575\n","Training log: 81 epoch (47488 / 50000 train. data). Loss: 1.237536072731018\n","Training log: 81 epoch (48768 / 50000 train. data). Loss: 1.3202683925628662\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 81 epoch (50048 / 50000 train. data). Loss: 1.3855953216552734\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.18it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.01it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.585100\n","Training log: 82 epoch (128 / 50000 train. data). Loss: 1.2465431690216064\n","Training log: 82 epoch (1408 / 50000 train. data). Loss: 1.3854460716247559\n","Training log: 82 epoch (2688 / 50000 train. data). Loss: 1.4244004487991333\n","Training log: 82 epoch (3968 / 50000 train. data). Loss: 1.4090203046798706\n","Training log: 82 epoch (5248 / 50000 train. data). Loss: 1.1685938835144043\n","Training log: 82 epoch (6528 / 50000 train. data). Loss: 1.2547049522399902\n","Training log: 82 epoch (7808 / 50000 train. data). Loss: 1.2736496925354004\n","Training log: 82 epoch (9088 / 50000 train. data). Loss: 1.2708086967468262\n","Training log: 82 epoch (10368 / 50000 train. data). Loss: 1.4302012920379639\n","Training log: 82 epoch (11648 / 50000 train. data). Loss: 1.5119930505752563\n","Training log: 82 epoch (12928 / 50000 train. data). Loss: 1.248009204864502\n","Training log: 82 epoch (14208 / 50000 train. data). Loss: 1.5646865367889404\n","Training log: 82 epoch (15488 / 50000 train. data). Loss: 1.2974694967269897\n","Training log: 82 epoch (16768 / 50000 train. data). Loss: 1.2781314849853516\n","Training log: 82 epoch (18048 / 50000 train. data). Loss: 1.4247459173202515\n","Training log: 82 epoch (19328 / 50000 train. data). Loss: 1.3865340948104858\n","Training log: 82 epoch (20608 / 50000 train. data). Loss: 1.455427885055542\n","Training log: 82 epoch (21888 / 50000 train. data). Loss: 1.1643213033676147\n","Training log: 82 epoch (23168 / 50000 train. data). Loss: 1.495300531387329\n","Training log: 82 epoch (24448 / 50000 train. data). Loss: 1.1890920400619507\n","Training log: 82 epoch (25728 / 50000 train. data). Loss: 1.2325258255004883\n","Training log: 82 epoch (27008 / 50000 train. data). Loss: 1.243910312652588\n","Training log: 82 epoch (28288 / 50000 train. data). Loss: 1.2432527542114258\n","Training log: 82 epoch (29568 / 50000 train. data). Loss: 1.3127493858337402\n","Training log: 82 epoch (30848 / 50000 train. data). Loss: 1.4443514347076416\n","Training log: 82 epoch (32128 / 50000 train. data). Loss: 1.2243798971176147\n","Training log: 82 epoch (33408 / 50000 train. data). Loss: 1.3250768184661865\n","Training log: 82 epoch (34688 / 50000 train. data). Loss: 1.2563282251358032\n","Training log: 82 epoch (35968 / 50000 train. data). Loss: 1.2722764015197754\n","Training log: 82 epoch (37248 / 50000 train. data). Loss: 1.2178977727890015\n","Training log: 82 epoch (38528 / 50000 train. data). Loss: 1.4100879430770874\n","Training log: 82 epoch (39808 / 50000 train. data). Loss: 1.2016233205795288\n","Training log: 82 epoch (41088 / 50000 train. data). Loss: 1.2185338735580444\n","Training log: 82 epoch (42368 / 50000 train. data). Loss: 1.3330879211425781\n","Training log: 82 epoch (43648 / 50000 train. data). Loss: 1.165622353553772\n","Training log: 82 epoch (44928 / 50000 train. data). Loss: 1.2715630531311035\n","Training log: 82 epoch (46208 / 50000 train. data). Loss: 1.4218204021453857\n","Training log: 82 epoch (47488 / 50000 train. data). Loss: 1.3045316934585571\n","Training log: 82 epoch (48768 / 50000 train. data). Loss: 1.4435278177261353\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 3/391 [00:00<00:13, 28.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 82 epoch (50048 / 50000 train. data). Loss: 1.3272111415863037\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.12it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.96it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.584400\n","Training log: 83 epoch (128 / 50000 train. data). Loss: 1.3075058460235596\n","Training log: 83 epoch (1408 / 50000 train. data). Loss: 1.2706639766693115\n","Training log: 83 epoch (2688 / 50000 train. data). Loss: 1.3322319984436035\n","Training log: 83 epoch (3968 / 50000 train. data). Loss: 1.3244636058807373\n","Training log: 83 epoch (5248 / 50000 train. data). Loss: 1.131977915763855\n","Training log: 83 epoch (6528 / 50000 train. data). Loss: 1.3774495124816895\n","Training log: 83 epoch (7808 / 50000 train. data). Loss: 1.3103618621826172\n","Training log: 83 epoch (9088 / 50000 train. data). Loss: 1.3151074647903442\n","Training log: 83 epoch (10368 / 50000 train. data). Loss: 1.3420687913894653\n","Training log: 83 epoch (11648 / 50000 train. data). Loss: 1.4079047441482544\n","Training log: 83 epoch (12928 / 50000 train. data). Loss: 1.4097400903701782\n","Training log: 83 epoch (14208 / 50000 train. data). Loss: 1.1940343379974365\n","Training log: 83 epoch (15488 / 50000 train. data). Loss: 1.2561473846435547\n","Training log: 83 epoch (16768 / 50000 train. data). Loss: 1.4065301418304443\n","Training log: 83 epoch (18048 / 50000 train. data). Loss: 1.2898675203323364\n","Training log: 83 epoch (19328 / 50000 train. data). Loss: 1.3251198530197144\n","Training log: 83 epoch (20608 / 50000 train. data). Loss: 1.312090516090393\n","Training log: 83 epoch (21888 / 50000 train. data). Loss: 1.342185139656067\n","Training log: 83 epoch (23168 / 50000 train. data). Loss: 1.2747435569763184\n","Training log: 83 epoch (24448 / 50000 train. data). Loss: 1.3941378593444824\n","Training log: 83 epoch (25728 / 50000 train. data). Loss: 1.4558550119400024\n","Training log: 83 epoch (27008 / 50000 train. data). Loss: 1.2769191265106201\n","Training log: 83 epoch (28288 / 50000 train. data). Loss: 1.2958271503448486\n","Training log: 83 epoch (29568 / 50000 train. data). Loss: 1.255791187286377\n","Training log: 83 epoch (30848 / 50000 train. data). Loss: 1.3422961235046387\n","Training log: 83 epoch (32128 / 50000 train. data). Loss: 1.343984842300415\n","Training log: 83 epoch (33408 / 50000 train. data). Loss: 1.3050289154052734\n","Training log: 83 epoch (34688 / 50000 train. data). Loss: 1.2360776662826538\n","Training log: 83 epoch (35968 / 50000 train. data). Loss: 1.3516080379486084\n","Training log: 83 epoch (37248 / 50000 train. data). Loss: 1.3225750923156738\n","Training log: 83 epoch (38528 / 50000 train. data). Loss: 1.1902430057525635\n","Training log: 83 epoch (39808 / 50000 train. data). Loss: 1.3304039239883423\n","Training log: 83 epoch (41088 / 50000 train. data). Loss: 1.286278247833252\n","Training log: 83 epoch (42368 / 50000 train. data). Loss: 1.3675134181976318\n","Training log: 83 epoch (43648 / 50000 train. data). Loss: 1.472596287727356\n","Training log: 83 epoch (44928 / 50000 train. data). Loss: 1.391286015510559\n","Training log: 83 epoch (46208 / 50000 train. data). Loss: 1.2529995441436768\n","Training log: 83 epoch (47488 / 50000 train. data). Loss: 1.2818254232406616\n","Training log: 83 epoch (48768 / 50000 train. data). Loss: 1.153415322303772\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 33.22it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 83 epoch (50048 / 50000 train. data). Loss: 1.5376760959625244\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.38it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.64it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.586600\n","Training log: 84 epoch (128 / 50000 train. data). Loss: 1.1847282648086548\n","Training log: 84 epoch (1408 / 50000 train. data). Loss: 1.1541423797607422\n","Training log: 84 epoch (2688 / 50000 train. data). Loss: 1.3565990924835205\n","Training log: 84 epoch (3968 / 50000 train. data). Loss: 1.2377206087112427\n","Training log: 84 epoch (5248 / 50000 train. data). Loss: 1.3046873807907104\n","Training log: 84 epoch (6528 / 50000 train. data). Loss: 1.4067738056182861\n","Training log: 84 epoch (7808 / 50000 train. data). Loss: 1.4236774444580078\n","Training log: 84 epoch (9088 / 50000 train. data). Loss: 1.290770411491394\n","Training log: 84 epoch (10368 / 50000 train. data). Loss: 1.3757376670837402\n","Training log: 84 epoch (11648 / 50000 train. data). Loss: 1.2757372856140137\n","Training log: 84 epoch (12928 / 50000 train. data). Loss: 1.1753309965133667\n","Training log: 84 epoch (14208 / 50000 train. data). Loss: 1.3101447820663452\n","Training log: 84 epoch (15488 / 50000 train. data). Loss: 1.4451512098312378\n","Training log: 84 epoch (16768 / 50000 train. data). Loss: 1.2666804790496826\n","Training log: 84 epoch (18048 / 50000 train. data). Loss: 1.1672688722610474\n","Training log: 84 epoch (19328 / 50000 train. data). Loss: 1.2500003576278687\n","Training log: 84 epoch (20608 / 50000 train. data). Loss: 1.3319270610809326\n","Training log: 84 epoch (21888 / 50000 train. data). Loss: 1.3382889032363892\n","Training log: 84 epoch (23168 / 50000 train. data). Loss: 1.5129172801971436\n","Training log: 84 epoch (24448 / 50000 train. data). Loss: 1.313606858253479\n","Training log: 84 epoch (25728 / 50000 train. data). Loss: 1.2430508136749268\n","Training log: 84 epoch (27008 / 50000 train. data). Loss: 1.1882245540618896\n","Training log: 84 epoch (28288 / 50000 train. data). Loss: 1.4253565073013306\n","Training log: 84 epoch (29568 / 50000 train. data). Loss: 1.293820858001709\n","Training log: 84 epoch (30848 / 50000 train. data). Loss: 1.3931015729904175\n","Training log: 84 epoch (32128 / 50000 train. data). Loss: 1.2149784564971924\n","Training log: 84 epoch (33408 / 50000 train. data). Loss: 1.2929575443267822\n","Training log: 84 epoch (34688 / 50000 train. data). Loss: 1.3961951732635498\n","Training log: 84 epoch (35968 / 50000 train. data). Loss: 1.3846900463104248\n","Training log: 84 epoch (37248 / 50000 train. data). Loss: 1.3360028266906738\n","Training log: 84 epoch (38528 / 50000 train. data). Loss: 1.49281644821167\n","Training log: 84 epoch (39808 / 50000 train. data). Loss: 1.3281269073486328\n","Training log: 84 epoch (41088 / 50000 train. data). Loss: 1.2866290807724\n","Training log: 84 epoch (42368 / 50000 train. data). Loss: 1.4710372686386108\n","Training log: 84 epoch (43648 / 50000 train. data). Loss: 1.1840169429779053\n","Training log: 84 epoch (44928 / 50000 train. data). Loss: 1.2179350852966309\n","Training log: 84 epoch (46208 / 50000 train. data). Loss: 1.4020472764968872\n","Training log: 84 epoch (47488 / 50000 train. data). Loss: 1.2867066860198975\n","Training log: 84 epoch (48768 / 50000 train. data). Loss: 1.43559992313385\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.08it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 84 epoch (50048 / 50000 train. data). Loss: 1.4614198207855225\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.65it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.76it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.586000\n","Training log: 85 epoch (128 / 50000 train. data). Loss: 1.2914658784866333\n","Training log: 85 epoch (1408 / 50000 train. data). Loss: 1.2611439228057861\n","Training log: 85 epoch (2688 / 50000 train. data). Loss: 1.35213303565979\n","Training log: 85 epoch (3968 / 50000 train. data). Loss: 1.279478669166565\n","Training log: 85 epoch (5248 / 50000 train. data). Loss: 1.239248275756836\n","Training log: 85 epoch (6528 / 50000 train. data). Loss: 1.3572087287902832\n","Training log: 85 epoch (7808 / 50000 train. data). Loss: 1.1915593147277832\n","Training log: 85 epoch (9088 / 50000 train. data). Loss: 1.354986548423767\n","Training log: 85 epoch (10368 / 50000 train. data). Loss: 1.5018885135650635\n","Training log: 85 epoch (11648 / 50000 train. data). Loss: 1.2036564350128174\n","Training log: 85 epoch (12928 / 50000 train. data). Loss: 1.3160386085510254\n","Training log: 85 epoch (14208 / 50000 train. data). Loss: 1.453602910041809\n","Training log: 85 epoch (15488 / 50000 train. data). Loss: 1.230281114578247\n","Training log: 85 epoch (16768 / 50000 train. data). Loss: 1.468003749847412\n","Training log: 85 epoch (18048 / 50000 train. data). Loss: 1.33049476146698\n","Training log: 85 epoch (19328 / 50000 train. data). Loss: 1.2044541835784912\n","Training log: 85 epoch (20608 / 50000 train. data). Loss: 1.3159376382827759\n","Training log: 85 epoch (21888 / 50000 train. data). Loss: 1.255680799484253\n","Training log: 85 epoch (23168 / 50000 train. data). Loss: 1.3113936185836792\n","Training log: 85 epoch (24448 / 50000 train. data). Loss: 1.218569040298462\n","Training log: 85 epoch (25728 / 50000 train. data). Loss: 1.3265119791030884\n","Training log: 85 epoch (27008 / 50000 train. data). Loss: 1.3347117900848389\n","Training log: 85 epoch (28288 / 50000 train. data). Loss: 1.2629352807998657\n","Training log: 85 epoch (29568 / 50000 train. data). Loss: 1.366821527481079\n","Training log: 85 epoch (30848 / 50000 train. data). Loss: 1.28218412399292\n","Training log: 85 epoch (32128 / 50000 train. data). Loss: 1.1973093748092651\n","Training log: 85 epoch (33408 / 50000 train. data). Loss: 1.3655253648757935\n","Training log: 85 epoch (34688 / 50000 train. data). Loss: 1.3392258882522583\n","Training log: 85 epoch (35968 / 50000 train. data). Loss: 1.18080735206604\n","Training log: 85 epoch (37248 / 50000 train. data). Loss: 1.2659215927124023\n","Training log: 85 epoch (38528 / 50000 train. data). Loss: 1.2740751504898071\n","Training log: 85 epoch (39808 / 50000 train. data). Loss: 1.4291129112243652\n","Training log: 85 epoch (41088 / 50000 train. data). Loss: 1.4661320447921753\n","Training log: 85 epoch (42368 / 50000 train. data). Loss: 1.3659968376159668\n","Training log: 85 epoch (43648 / 50000 train. data). Loss: 1.3118610382080078\n","Training log: 85 epoch (44928 / 50000 train. data). Loss: 1.4690219163894653\n","Training log: 85 epoch (46208 / 50000 train. data). Loss: 1.3738659620285034\n","Training log: 85 epoch (47488 / 50000 train. data). Loss: 1.1809509992599487\n","Training log: 85 epoch (48768 / 50000 train. data). Loss: 1.377240538597107\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 35.15it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 85 epoch (50048 / 50000 train. data). Loss: 1.208770751953125\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.90it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.590300\n","Training log: 86 epoch (128 / 50000 train. data). Loss: 1.3103220462799072\n","Training log: 86 epoch (1408 / 50000 train. data). Loss: 1.389077067375183\n","Training log: 86 epoch (2688 / 50000 train. data). Loss: 1.2321511507034302\n","Training log: 86 epoch (3968 / 50000 train. data). Loss: 1.3056118488311768\n","Training log: 86 epoch (5248 / 50000 train. data). Loss: 1.2246992588043213\n","Training log: 86 epoch (6528 / 50000 train. data). Loss: 1.405517578125\n","Training log: 86 epoch (7808 / 50000 train. data). Loss: 1.3347561359405518\n","Training log: 86 epoch (9088 / 50000 train. data). Loss: 1.352565050125122\n","Training log: 86 epoch (10368 / 50000 train. data). Loss: 1.3072683811187744\n","Training log: 86 epoch (11648 / 50000 train. data). Loss: 1.2951236963272095\n","Training log: 86 epoch (12928 / 50000 train. data). Loss: 1.0617103576660156\n","Training log: 86 epoch (14208 / 50000 train. data). Loss: 1.1983683109283447\n","Training log: 86 epoch (15488 / 50000 train. data). Loss: 1.2593660354614258\n","Training log: 86 epoch (16768 / 50000 train. data). Loss: 1.4076950550079346\n","Training log: 86 epoch (18048 / 50000 train. data). Loss: 1.1531902551651\n","Training log: 86 epoch (19328 / 50000 train. data). Loss: 1.1922364234924316\n","Training log: 86 epoch (20608 / 50000 train. data). Loss: 1.4166955947875977\n","Training log: 86 epoch (21888 / 50000 train. data). Loss: 1.2867636680603027\n","Training log: 86 epoch (23168 / 50000 train. data). Loss: 1.3064459562301636\n","Training log: 86 epoch (24448 / 50000 train. data). Loss: 1.3430802822113037\n","Training log: 86 epoch (25728 / 50000 train. data). Loss: 1.2789491415023804\n","Training log: 86 epoch (27008 / 50000 train. data). Loss: 1.3094879388809204\n","Training log: 86 epoch (28288 / 50000 train. data). Loss: 1.3849893808364868\n","Training log: 86 epoch (29568 / 50000 train. data). Loss: 1.4672918319702148\n","Training log: 86 epoch (30848 / 50000 train. data). Loss: 1.4587457180023193\n","Training log: 86 epoch (32128 / 50000 train. data). Loss: 1.2153505086898804\n","Training log: 86 epoch (33408 / 50000 train. data). Loss: 1.2942732572555542\n","Training log: 86 epoch (34688 / 50000 train. data). Loss: 1.2312922477722168\n","Training log: 86 epoch (35968 / 50000 train. data). Loss: 1.2955825328826904\n","Training log: 86 epoch (37248 / 50000 train. data). Loss: 1.4559745788574219\n","Training log: 86 epoch (38528 / 50000 train. data). Loss: 1.262822151184082\n","Training log: 86 epoch (39808 / 50000 train. data). Loss: 1.3115546703338623\n","Training log: 86 epoch (41088 / 50000 train. data). Loss: 1.245977759361267\n","Training log: 86 epoch (42368 / 50000 train. data). Loss: 1.4228343963623047\n","Training log: 86 epoch (43648 / 50000 train. data). Loss: 1.2965986728668213\n","Training log: 86 epoch (44928 / 50000 train. data). Loss: 1.266876220703125\n","Training log: 86 epoch (46208 / 50000 train. data). Loss: 1.242267370223999\n","Training log: 86 epoch (47488 / 50000 train. data). Loss: 1.4554023742675781\n","Training log: 86 epoch (48768 / 50000 train. data). Loss: 1.3293263912200928\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 3/391 [00:00<00:13, 28.74it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 86 epoch (50048 / 50000 train. data). Loss: 1.4341520071029663\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 32.80it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.98it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.589600\n","Training log: 87 epoch (128 / 50000 train. data). Loss: 1.1260817050933838\n","Training log: 87 epoch (1408 / 50000 train. data). Loss: 1.295549988746643\n","Training log: 87 epoch (2688 / 50000 train. data). Loss: 1.2068737745285034\n","Training log: 87 epoch (3968 / 50000 train. data). Loss: 1.2215816974639893\n","Training log: 87 epoch (5248 / 50000 train. data). Loss: 1.3183048963546753\n","Training log: 87 epoch (6528 / 50000 train. data). Loss: 1.4638161659240723\n","Training log: 87 epoch (7808 / 50000 train. data). Loss: 1.3934228420257568\n","Training log: 87 epoch (9088 / 50000 train. data). Loss: 1.2244248390197754\n","Training log: 87 epoch (10368 / 50000 train. data). Loss: 1.2810189723968506\n","Training log: 87 epoch (11648 / 50000 train. data). Loss: 1.3057717084884644\n","Training log: 87 epoch (12928 / 50000 train. data). Loss: 1.3141539096832275\n","Training log: 87 epoch (14208 / 50000 train. data). Loss: 1.295728087425232\n","Training log: 87 epoch (15488 / 50000 train. data). Loss: 1.1027615070343018\n","Training log: 87 epoch (16768 / 50000 train. data). Loss: 1.327499270439148\n","Training log: 87 epoch (18048 / 50000 train. data). Loss: 1.3046274185180664\n","Training log: 87 epoch (19328 / 50000 train. data). Loss: 1.2887905836105347\n","Training log: 87 epoch (20608 / 50000 train. data). Loss: 1.1920665502548218\n","Training log: 87 epoch (21888 / 50000 train. data). Loss: 1.2639001607894897\n","Training log: 87 epoch (23168 / 50000 train. data). Loss: 1.2281063795089722\n","Training log: 87 epoch (24448 / 50000 train. data). Loss: 1.262049674987793\n","Training log: 87 epoch (25728 / 50000 train. data). Loss: 1.3522635698318481\n","Training log: 87 epoch (27008 / 50000 train. data). Loss: 1.2773752212524414\n","Training log: 87 epoch (28288 / 50000 train. data). Loss: 1.342640995979309\n","Training log: 87 epoch (29568 / 50000 train. data). Loss: 1.2966735363006592\n","Training log: 87 epoch (30848 / 50000 train. data). Loss: 1.193581461906433\n","Training log: 87 epoch (32128 / 50000 train. data). Loss: 1.4161338806152344\n","Training log: 87 epoch (33408 / 50000 train. data). Loss: 1.2262338399887085\n","Training log: 87 epoch (34688 / 50000 train. data). Loss: 1.1060981750488281\n","Training log: 87 epoch (35968 / 50000 train. data). Loss: 1.3313761949539185\n","Training log: 87 epoch (37248 / 50000 train. data). Loss: 1.2818294763565063\n","Training log: 87 epoch (38528 / 50000 train. data). Loss: 1.345078468322754\n","Training log: 87 epoch (39808 / 50000 train. data). Loss: 1.2099215984344482\n","Training log: 87 epoch (41088 / 50000 train. data). Loss: 1.3369996547698975\n","Training log: 87 epoch (42368 / 50000 train. data). Loss: 1.1732475757598877\n","Training log: 87 epoch (43648 / 50000 train. data). Loss: 1.4370183944702148\n","Training log: 87 epoch (44928 / 50000 train. data). Loss: 1.3402258157730103\n","Training log: 87 epoch (46208 / 50000 train. data). Loss: 1.3969604969024658\n","Training log: 87 epoch (47488 / 50000 train. data). Loss: 1.242627501487732\n","Training log: 87 epoch (48768 / 50000 train. data). Loss: 1.2430421113967896\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.38it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 87 epoch (50048 / 50000 train. data). Loss: 1.1567611694335938\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.68it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.72it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.587400\n","Training log: 88 epoch (128 / 50000 train. data). Loss: 1.1196733713150024\n","Training log: 88 epoch (1408 / 50000 train. data). Loss: 1.2162379026412964\n","Training log: 88 epoch (2688 / 50000 train. data). Loss: 1.2156836986541748\n","Training log: 88 epoch (3968 / 50000 train. data). Loss: 1.4188655614852905\n","Training log: 88 epoch (5248 / 50000 train. data). Loss: 1.2744383811950684\n","Training log: 88 epoch (6528 / 50000 train. data). Loss: 1.2541470527648926\n","Training log: 88 epoch (7808 / 50000 train. data). Loss: 1.2442978620529175\n","Training log: 88 epoch (9088 / 50000 train. data). Loss: 1.5858062505722046\n","Training log: 88 epoch (10368 / 50000 train. data). Loss: 1.4032634496688843\n","Training log: 88 epoch (11648 / 50000 train. data). Loss: 1.3183095455169678\n","Training log: 88 epoch (12928 / 50000 train. data). Loss: 1.32833731174469\n","Training log: 88 epoch (14208 / 50000 train. data). Loss: 1.2352160215377808\n","Training log: 88 epoch (15488 / 50000 train. data). Loss: 1.2110131978988647\n","Training log: 88 epoch (16768 / 50000 train. data). Loss: 1.3247153759002686\n","Training log: 88 epoch (18048 / 50000 train. data). Loss: 1.2262400388717651\n","Training log: 88 epoch (19328 / 50000 train. data). Loss: 1.360101342201233\n","Training log: 88 epoch (20608 / 50000 train. data). Loss: 1.2397652864456177\n","Training log: 88 epoch (21888 / 50000 train. data). Loss: 1.4508694410324097\n","Training log: 88 epoch (23168 / 50000 train. data). Loss: 1.3310457468032837\n","Training log: 88 epoch (24448 / 50000 train. data). Loss: 1.476699948310852\n","Training log: 88 epoch (25728 / 50000 train. data). Loss: 1.2796604633331299\n","Training log: 88 epoch (27008 / 50000 train. data). Loss: 1.3506195545196533\n","Training log: 88 epoch (28288 / 50000 train. data). Loss: 1.3110496997833252\n","Training log: 88 epoch (29568 / 50000 train. data). Loss: 1.2519031763076782\n","Training log: 88 epoch (30848 / 50000 train. data). Loss: 1.3086600303649902\n","Training log: 88 epoch (32128 / 50000 train. data). Loss: 1.348563313484192\n","Training log: 88 epoch (33408 / 50000 train. data). Loss: 1.1615103483200073\n","Training log: 88 epoch (34688 / 50000 train. data). Loss: 1.2877262830734253\n","Training log: 88 epoch (35968 / 50000 train. data). Loss: 1.2791187763214111\n","Training log: 88 epoch (37248 / 50000 train. data). Loss: 1.286409854888916\n","Training log: 88 epoch (38528 / 50000 train. data). Loss: 1.4104472398757935\n","Training log: 88 epoch (39808 / 50000 train. data). Loss: 1.402112364768982\n","Training log: 88 epoch (41088 / 50000 train. data). Loss: 1.296978235244751\n","Training log: 88 epoch (42368 / 50000 train. data). Loss: 1.3176710605621338\n","Training log: 88 epoch (43648 / 50000 train. data). Loss: 1.5499403476715088\n","Training log: 88 epoch (44928 / 50000 train. data). Loss: 1.399640679359436\n","Training log: 88 epoch (46208 / 50000 train. data). Loss: 1.3150460720062256\n","Training log: 88 epoch (47488 / 50000 train. data). Loss: 1.2578791379928589\n","Training log: 88 epoch (48768 / 50000 train. data). Loss: 1.1710857152938843\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:12, 31.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 88 epoch (50048 / 50000 train. data). Loss: 1.4434527158737183\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.70it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.16it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.587400\n","Training log: 89 epoch (128 / 50000 train. data). Loss: 1.3170791864395142\n","Training log: 89 epoch (1408 / 50000 train. data). Loss: 1.4053784608840942\n","Training log: 89 epoch (2688 / 50000 train. data). Loss: 1.330451488494873\n","Training log: 89 epoch (3968 / 50000 train. data). Loss: 1.4220527410507202\n","Training log: 89 epoch (5248 / 50000 train. data). Loss: 1.3511310815811157\n","Training log: 89 epoch (6528 / 50000 train. data). Loss: 1.2290397882461548\n","Training log: 89 epoch (7808 / 50000 train. data). Loss: 1.2467180490493774\n","Training log: 89 epoch (9088 / 50000 train. data). Loss: 1.2596285343170166\n","Training log: 89 epoch (10368 / 50000 train. data). Loss: 1.3676131963729858\n","Training log: 89 epoch (11648 / 50000 train. data). Loss: 1.4597164392471313\n","Training log: 89 epoch (12928 / 50000 train. data). Loss: 1.463040828704834\n","Training log: 89 epoch (14208 / 50000 train. data). Loss: 1.4549875259399414\n","Training log: 89 epoch (15488 / 50000 train. data). Loss: 1.4867007732391357\n","Training log: 89 epoch (16768 / 50000 train. data). Loss: 1.3344310522079468\n","Training log: 89 epoch (18048 / 50000 train. data). Loss: 1.2397325038909912\n","Training log: 89 epoch (19328 / 50000 train. data). Loss: 1.3314998149871826\n","Training log: 89 epoch (20608 / 50000 train. data). Loss: 1.2513105869293213\n","Training log: 89 epoch (21888 / 50000 train. data). Loss: 1.4284573793411255\n","Training log: 89 epoch (23168 / 50000 train. data). Loss: 1.1917634010314941\n","Training log: 89 epoch (24448 / 50000 train. data). Loss: 1.1870787143707275\n","Training log: 89 epoch (25728 / 50000 train. data). Loss: 1.3564658164978027\n","Training log: 89 epoch (27008 / 50000 train. data). Loss: 1.2182741165161133\n","Training log: 89 epoch (28288 / 50000 train. data). Loss: 1.3628815412521362\n","Training log: 89 epoch (29568 / 50000 train. data). Loss: 1.573880910873413\n","Training log: 89 epoch (30848 / 50000 train. data). Loss: 1.3342715501785278\n","Training log: 89 epoch (32128 / 50000 train. data). Loss: 1.271836757659912\n","Training log: 89 epoch (33408 / 50000 train. data). Loss: 1.1677041053771973\n","Training log: 89 epoch (34688 / 50000 train. data). Loss: 1.2996537685394287\n","Training log: 89 epoch (35968 / 50000 train. data). Loss: 1.3591983318328857\n","Training log: 89 epoch (37248 / 50000 train. data). Loss: 1.3060169219970703\n","Training log: 89 epoch (38528 / 50000 train. data). Loss: 1.249382734298706\n","Training log: 89 epoch (39808 / 50000 train. data). Loss: 1.1828317642211914\n","Training log: 89 epoch (41088 / 50000 train. data). Loss: 1.225650668144226\n","Training log: 89 epoch (42368 / 50000 train. data). Loss: 1.282670497894287\n","Training log: 89 epoch (43648 / 50000 train. data). Loss: 1.3230645656585693\n","Training log: 89 epoch (44928 / 50000 train. data). Loss: 1.111700177192688\n","Training log: 89 epoch (46208 / 50000 train. data). Loss: 1.3423579931259155\n","Training log: 89 epoch (47488 / 50000 train. data). Loss: 1.32716965675354\n","Training log: 89 epoch (48768 / 50000 train. data). Loss: 1.3254657983779907\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.18it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 89 epoch (50048 / 50000 train. data). Loss: 1.1918987035751343\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.78it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.21it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.587400\n","Training log: 90 epoch (128 / 50000 train. data). Loss: 1.2362711429595947\n","Training log: 90 epoch (1408 / 50000 train. data). Loss: 1.3573476076126099\n","Training log: 90 epoch (2688 / 50000 train. data). Loss: 1.168280839920044\n","Training log: 90 epoch (3968 / 50000 train. data). Loss: 1.2872600555419922\n","Training log: 90 epoch (5248 / 50000 train. data). Loss: 1.309473991394043\n","Training log: 90 epoch (6528 / 50000 train. data). Loss: 1.4578540325164795\n","Training log: 90 epoch (7808 / 50000 train. data). Loss: 1.3547985553741455\n","Training log: 90 epoch (9088 / 50000 train. data). Loss: 1.3134851455688477\n","Training log: 90 epoch (10368 / 50000 train. data). Loss: 1.2641329765319824\n","Training log: 90 epoch (11648 / 50000 train. data). Loss: 1.1897379159927368\n","Training log: 90 epoch (12928 / 50000 train. data). Loss: 1.380357265472412\n","Training log: 90 epoch (14208 / 50000 train. data). Loss: 1.2804930210113525\n","Training log: 90 epoch (15488 / 50000 train. data). Loss: 1.3546892404556274\n","Training log: 90 epoch (16768 / 50000 train. data). Loss: 1.202980399131775\n","Training log: 90 epoch (18048 / 50000 train. data). Loss: 1.365460991859436\n","Training log: 90 epoch (19328 / 50000 train. data). Loss: 1.3543823957443237\n","Training log: 90 epoch (20608 / 50000 train. data). Loss: 1.3090871572494507\n","Training log: 90 epoch (21888 / 50000 train. data). Loss: 1.398726463317871\n","Training log: 90 epoch (23168 / 50000 train. data). Loss: 1.3601256608963013\n","Training log: 90 epoch (24448 / 50000 train. data). Loss: 1.2320111989974976\n","Training log: 90 epoch (25728 / 50000 train. data). Loss: 1.096831202507019\n","Training log: 90 epoch (27008 / 50000 train. data). Loss: 1.415237545967102\n","Training log: 90 epoch (28288 / 50000 train. data). Loss: 1.2598384618759155\n","Training log: 90 epoch (29568 / 50000 train. data). Loss: 1.30389404296875\n","Training log: 90 epoch (30848 / 50000 train. data). Loss: 1.367347002029419\n","Training log: 90 epoch (32128 / 50000 train. data). Loss: 1.4001352787017822\n","Training log: 90 epoch (33408 / 50000 train. data). Loss: 1.4744807481765747\n","Training log: 90 epoch (34688 / 50000 train. data). Loss: 1.321104884147644\n","Training log: 90 epoch (35968 / 50000 train. data). Loss: 1.2106456756591797\n","Training log: 90 epoch (37248 / 50000 train. data). Loss: 1.3508492708206177\n","Training log: 90 epoch (38528 / 50000 train. data). Loss: 1.2509843111038208\n","Training log: 90 epoch (39808 / 50000 train. data). Loss: 1.2392221689224243\n","Training log: 90 epoch (41088 / 50000 train. data). Loss: 1.2191210985183716\n","Training log: 90 epoch (42368 / 50000 train. data). Loss: 1.0590649843215942\n","Training log: 90 epoch (43648 / 50000 train. data). Loss: 1.2306568622589111\n","Training log: 90 epoch (44928 / 50000 train. data). Loss: 1.3571372032165527\n","Training log: 90 epoch (46208 / 50000 train. data). Loss: 1.3253511190414429\n","Training log: 90 epoch (47488 / 50000 train. data). Loss: 1.150071620941162\n","Training log: 90 epoch (48768 / 50000 train. data). Loss: 1.2766048908233643\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 90 epoch (50048 / 50000 train. data). Loss: 1.1924494504928589\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.36it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.88it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.596600\n","Training log: 91 epoch (128 / 50000 train. data). Loss: 1.1447467803955078\n","Training log: 91 epoch (1408 / 50000 train. data). Loss: 1.3031574487686157\n","Training log: 91 epoch (2688 / 50000 train. data). Loss: 1.2839484214782715\n","Training log: 91 epoch (3968 / 50000 train. data). Loss: 1.3202993869781494\n","Training log: 91 epoch (5248 / 50000 train. data). Loss: 1.1378945112228394\n","Training log: 91 epoch (6528 / 50000 train. data). Loss: 1.169583797454834\n","Training log: 91 epoch (7808 / 50000 train. data). Loss: 1.3925420045852661\n","Training log: 91 epoch (9088 / 50000 train. data). Loss: 1.2708483934402466\n","Training log: 91 epoch (10368 / 50000 train. data). Loss: 1.1825417280197144\n","Training log: 91 epoch (11648 / 50000 train. data). Loss: 1.1858956813812256\n","Training log: 91 epoch (12928 / 50000 train. data). Loss: 1.2790907621383667\n","Training log: 91 epoch (14208 / 50000 train. data). Loss: 1.2789690494537354\n","Training log: 91 epoch (15488 / 50000 train. data). Loss: 1.3284484148025513\n","Training log: 91 epoch (16768 / 50000 train. data). Loss: 1.463320016860962\n","Training log: 91 epoch (18048 / 50000 train. data). Loss: 1.1999229192733765\n","Training log: 91 epoch (19328 / 50000 train. data). Loss: 1.259286642074585\n","Training log: 91 epoch (20608 / 50000 train. data). Loss: 1.2949106693267822\n","Training log: 91 epoch (21888 / 50000 train. data). Loss: 1.414551019668579\n","Training log: 91 epoch (23168 / 50000 train. data). Loss: 1.3614519834518433\n","Training log: 91 epoch (24448 / 50000 train. data). Loss: 1.2754454612731934\n","Training log: 91 epoch (25728 / 50000 train. data). Loss: 1.3146203756332397\n","Training log: 91 epoch (27008 / 50000 train. data). Loss: 1.329404354095459\n","Training log: 91 epoch (28288 / 50000 train. data). Loss: 1.2999674081802368\n","Training log: 91 epoch (29568 / 50000 train. data). Loss: 1.1930062770843506\n","Training log: 91 epoch (30848 / 50000 train. data). Loss: 1.2533379793167114\n","Training log: 91 epoch (32128 / 50000 train. data). Loss: 1.2053710222244263\n","Training log: 91 epoch (33408 / 50000 train. data). Loss: 1.2918599843978882\n","Training log: 91 epoch (34688 / 50000 train. data). Loss: 1.3539507389068604\n","Training log: 91 epoch (35968 / 50000 train. data). Loss: 1.3323981761932373\n","Training log: 91 epoch (37248 / 50000 train. data). Loss: 1.3633348941802979\n","Training log: 91 epoch (38528 / 50000 train. data). Loss: 1.2190760374069214\n","Training log: 91 epoch (39808 / 50000 train. data). Loss: 1.2345409393310547\n","Training log: 91 epoch (41088 / 50000 train. data). Loss: 1.234406590461731\n","Training log: 91 epoch (42368 / 50000 train. data). Loss: 1.3313171863555908\n","Training log: 91 epoch (43648 / 50000 train. data). Loss: 1.3010133504867554\n","Training log: 91 epoch (44928 / 50000 train. data). Loss: 1.0918329954147339\n","Training log: 91 epoch (46208 / 50000 train. data). Loss: 1.2748652696609497\n","Training log: 91 epoch (47488 / 50000 train. data). Loss: 1.3815232515335083\n","Training log: 91 epoch (48768 / 50000 train. data). Loss: 1.1794660091400146\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 91 epoch (50048 / 50000 train. data). Loss: 1.156740427017212\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.56it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.34it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.594700\n","Training log: 92 epoch (128 / 50000 train. data). Loss: 1.1835565567016602\n","Training log: 92 epoch (1408 / 50000 train. data). Loss: 1.262433648109436\n","Training log: 92 epoch (2688 / 50000 train. data). Loss: 1.4599058628082275\n","Training log: 92 epoch (3968 / 50000 train. data). Loss: 1.2318726778030396\n","Training log: 92 epoch (5248 / 50000 train. data). Loss: 1.2468030452728271\n","Training log: 92 epoch (6528 / 50000 train. data). Loss: 1.1466749906539917\n","Training log: 92 epoch (7808 / 50000 train. data). Loss: 1.2826608419418335\n","Training log: 92 epoch (9088 / 50000 train. data). Loss: 1.2835988998413086\n","Training log: 92 epoch (10368 / 50000 train. data). Loss: 1.2366529703140259\n","Training log: 92 epoch (11648 / 50000 train. data). Loss: 1.494407057762146\n","Training log: 92 epoch (12928 / 50000 train. data). Loss: 1.2452452182769775\n","Training log: 92 epoch (14208 / 50000 train. data). Loss: 1.2543227672576904\n","Training log: 92 epoch (15488 / 50000 train. data). Loss: 1.2750300168991089\n","Training log: 92 epoch (16768 / 50000 train. data). Loss: 1.1473045349121094\n","Training log: 92 epoch (18048 / 50000 train. data). Loss: 1.3138360977172852\n","Training log: 92 epoch (19328 / 50000 train. data). Loss: 1.32170832157135\n","Training log: 92 epoch (20608 / 50000 train. data). Loss: 1.249050498008728\n","Training log: 92 epoch (21888 / 50000 train. data). Loss: 1.3429367542266846\n","Training log: 92 epoch (23168 / 50000 train. data). Loss: 1.3137518167495728\n","Training log: 92 epoch (24448 / 50000 train. data). Loss: 1.2830233573913574\n","Training log: 92 epoch (25728 / 50000 train. data). Loss: 1.3550573587417603\n","Training log: 92 epoch (27008 / 50000 train. data). Loss: 1.3832905292510986\n","Training log: 92 epoch (28288 / 50000 train. data). Loss: 1.2246572971343994\n","Training log: 92 epoch (29568 / 50000 train. data). Loss: 1.3909904956817627\n","Training log: 92 epoch (30848 / 50000 train. data). Loss: 1.165881872177124\n","Training log: 92 epoch (32128 / 50000 train. data). Loss: 1.1334974765777588\n","Training log: 92 epoch (33408 / 50000 train. data). Loss: 1.2196098566055298\n","Training log: 92 epoch (34688 / 50000 train. data). Loss: 1.2450302839279175\n","Training log: 92 epoch (35968 / 50000 train. data). Loss: 1.3952666521072388\n","Training log: 92 epoch (37248 / 50000 train. data). Loss: 1.381269931793213\n","Training log: 92 epoch (38528 / 50000 train. data). Loss: 1.5152159929275513\n","Training log: 92 epoch (39808 / 50000 train. data). Loss: 1.2118562459945679\n","Training log: 92 epoch (41088 / 50000 train. data). Loss: 1.31264066696167\n","Training log: 92 epoch (42368 / 50000 train. data). Loss: 1.268944501876831\n","Training log: 92 epoch (43648 / 50000 train. data). Loss: 1.2751774787902832\n","Training log: 92 epoch (44928 / 50000 train. data). Loss: 1.27682626247406\n","Training log: 92 epoch (46208 / 50000 train. data). Loss: 1.133984088897705\n","Training log: 92 epoch (47488 / 50000 train. data). Loss: 1.1197127103805542\n","Training log: 92 epoch (48768 / 50000 train. data). Loss: 1.1979707479476929\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:12, 31.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 92 epoch (50048 / 50000 train. data). Loss: 1.2614916563034058\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.32it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.590600\n","Training log: 93 epoch (128 / 50000 train. data). Loss: 1.444347620010376\n","Training log: 93 epoch (1408 / 50000 train. data). Loss: 1.270517349243164\n","Training log: 93 epoch (2688 / 50000 train. data). Loss: 1.0517504215240479\n","Training log: 93 epoch (3968 / 50000 train. data). Loss: 1.2029062509536743\n","Training log: 93 epoch (5248 / 50000 train. data). Loss: 1.2571141719818115\n","Training log: 93 epoch (6528 / 50000 train. data). Loss: 1.253241777420044\n","Training log: 93 epoch (7808 / 50000 train. data). Loss: 1.1980890035629272\n","Training log: 93 epoch (9088 / 50000 train. data). Loss: 1.3649383783340454\n","Training log: 93 epoch (10368 / 50000 train. data). Loss: 1.4029368162155151\n","Training log: 93 epoch (11648 / 50000 train. data). Loss: 1.1763112545013428\n","Training log: 93 epoch (12928 / 50000 train. data). Loss: 1.3660048246383667\n","Training log: 93 epoch (14208 / 50000 train. data). Loss: 1.394447684288025\n","Training log: 93 epoch (15488 / 50000 train. data). Loss: 1.3062548637390137\n","Training log: 93 epoch (16768 / 50000 train. data). Loss: 1.3081653118133545\n","Training log: 93 epoch (18048 / 50000 train. data). Loss: 1.0295734405517578\n","Training log: 93 epoch (19328 / 50000 train. data). Loss: 1.3375177383422852\n","Training log: 93 epoch (20608 / 50000 train. data). Loss: 1.2154141664505005\n","Training log: 93 epoch (21888 / 50000 train. data). Loss: 1.3421168327331543\n","Training log: 93 epoch (23168 / 50000 train. data). Loss: 1.3383065462112427\n","Training log: 93 epoch (24448 / 50000 train. data). Loss: 1.366684079170227\n","Training log: 93 epoch (25728 / 50000 train. data). Loss: 1.3915523290634155\n","Training log: 93 epoch (27008 / 50000 train. data). Loss: 1.2914420366287231\n","Training log: 93 epoch (28288 / 50000 train. data). Loss: 1.3101717233657837\n","Training log: 93 epoch (29568 / 50000 train. data). Loss: 1.3356248140335083\n","Training log: 93 epoch (30848 / 50000 train. data). Loss: 1.1465065479278564\n","Training log: 93 epoch (32128 / 50000 train. data). Loss: 1.32436203956604\n","Training log: 93 epoch (33408 / 50000 train. data). Loss: 1.2594512701034546\n","Training log: 93 epoch (34688 / 50000 train. data). Loss: 1.4012562036514282\n","Training log: 93 epoch (35968 / 50000 train. data). Loss: 1.2375261783599854\n","Training log: 93 epoch (37248 / 50000 train. data). Loss: 1.4089131355285645\n","Training log: 93 epoch (38528 / 50000 train. data). Loss: 1.299229621887207\n","Training log: 93 epoch (39808 / 50000 train. data). Loss: 1.1396502256393433\n","Training log: 93 epoch (41088 / 50000 train. data). Loss: 1.2335468530654907\n","Training log: 93 epoch (42368 / 50000 train. data). Loss: 1.3264312744140625\n","Training log: 93 epoch (43648 / 50000 train. data). Loss: 1.1051872968673706\n","Training log: 93 epoch (44928 / 50000 train. data). Loss: 1.180755376815796\n","Training log: 93 epoch (46208 / 50000 train. data). Loss: 1.2184736728668213\n","Training log: 93 epoch (47488 / 50000 train. data). Loss: 1.394960880279541\n","Training log: 93 epoch (48768 / 50000 train. data). Loss: 1.2122390270233154\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 93 epoch (50048 / 50000 train. data). Loss: 1.3622219562530518\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.87it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.35it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.598500\n","Training log: 94 epoch (128 / 50000 train. data). Loss: 1.3521710634231567\n","Training log: 94 epoch (1408 / 50000 train. data). Loss: 1.3182505369186401\n","Training log: 94 epoch (2688 / 50000 train. data). Loss: 1.2433189153671265\n","Training log: 94 epoch (3968 / 50000 train. data). Loss: 1.34857177734375\n","Training log: 94 epoch (5248 / 50000 train. data). Loss: 1.3760335445404053\n","Training log: 94 epoch (6528 / 50000 train. data). Loss: 1.2859729528427124\n","Training log: 94 epoch (7808 / 50000 train. data). Loss: 1.402606725692749\n","Training log: 94 epoch (9088 / 50000 train. data). Loss: 1.286935567855835\n","Training log: 94 epoch (10368 / 50000 train. data). Loss: 1.3044122457504272\n","Training log: 94 epoch (11648 / 50000 train. data). Loss: 1.4815418720245361\n","Training log: 94 epoch (12928 / 50000 train. data). Loss: 1.4643217325210571\n","Training log: 94 epoch (14208 / 50000 train. data). Loss: 1.1950663328170776\n","Training log: 94 epoch (15488 / 50000 train. data). Loss: 1.20901358127594\n","Training log: 94 epoch (16768 / 50000 train. data). Loss: 1.5770026445388794\n","Training log: 94 epoch (18048 / 50000 train. data). Loss: 1.4050166606903076\n","Training log: 94 epoch (19328 / 50000 train. data). Loss: 1.1489883661270142\n","Training log: 94 epoch (20608 / 50000 train. data). Loss: 1.3384730815887451\n","Training log: 94 epoch (21888 / 50000 train. data). Loss: 1.2861608266830444\n","Training log: 94 epoch (23168 / 50000 train. data). Loss: 1.4042266607284546\n","Training log: 94 epoch (24448 / 50000 train. data). Loss: 1.2364473342895508\n","Training log: 94 epoch (25728 / 50000 train. data). Loss: 1.3728694915771484\n","Training log: 94 epoch (27008 / 50000 train. data). Loss: 1.2377840280532837\n","Training log: 94 epoch (28288 / 50000 train. data). Loss: 1.301415205001831\n","Training log: 94 epoch (29568 / 50000 train. data). Loss: 1.3247815370559692\n","Training log: 94 epoch (30848 / 50000 train. data). Loss: 1.3727269172668457\n","Training log: 94 epoch (32128 / 50000 train. data). Loss: 1.3497346639633179\n","Training log: 94 epoch (33408 / 50000 train. data). Loss: 1.349129557609558\n","Training log: 94 epoch (34688 / 50000 train. data). Loss: 1.2674585580825806\n","Training log: 94 epoch (35968 / 50000 train. data). Loss: 1.2959644794464111\n","Training log: 94 epoch (37248 / 50000 train. data). Loss: 1.246903657913208\n","Training log: 94 epoch (38528 / 50000 train. data). Loss: 1.39659583568573\n","Training log: 94 epoch (39808 / 50000 train. data). Loss: 1.2613033056259155\n","Training log: 94 epoch (41088 / 50000 train. data). Loss: 1.3703616857528687\n","Training log: 94 epoch (42368 / 50000 train. data). Loss: 1.3862862586975098\n","Training log: 94 epoch (43648 / 50000 train. data). Loss: 1.222095251083374\n","Training log: 94 epoch (44928 / 50000 train. data). Loss: 1.2057498693466187\n","Training log: 94 epoch (46208 / 50000 train. data). Loss: 1.254469871520996\n","Training log: 94 epoch (47488 / 50000 train. data). Loss: 1.2272069454193115\n","Training log: 94 epoch (48768 / 50000 train. data). Loss: 1.4221779108047485\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 3/391 [00:00<00:13, 29.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 94 epoch (50048 / 50000 train. data). Loss: 1.3036208152770996\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.01it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.45it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.589500\n","Training log: 95 epoch (128 / 50000 train. data). Loss: 1.2421321868896484\n","Training log: 95 epoch (1408 / 50000 train. data). Loss: 1.215384602546692\n","Training log: 95 epoch (2688 / 50000 train. data). Loss: 1.2150547504425049\n","Training log: 95 epoch (3968 / 50000 train. data). Loss: 1.3168236017227173\n","Training log: 95 epoch (5248 / 50000 train. data). Loss: 1.2904404401779175\n","Training log: 95 epoch (6528 / 50000 train. data). Loss: 1.2177530527114868\n","Training log: 95 epoch (7808 / 50000 train. data). Loss: 1.2704557180404663\n","Training log: 95 epoch (9088 / 50000 train. data). Loss: 1.2702223062515259\n","Training log: 95 epoch (10368 / 50000 train. data). Loss: 1.2380623817443848\n","Training log: 95 epoch (11648 / 50000 train. data). Loss: 1.3324267864227295\n","Training log: 95 epoch (12928 / 50000 train. data). Loss: 1.3392447233200073\n","Training log: 95 epoch (14208 / 50000 train. data). Loss: 1.3384168148040771\n","Training log: 95 epoch (15488 / 50000 train. data). Loss: 1.247844934463501\n","Training log: 95 epoch (16768 / 50000 train. data). Loss: 1.2533705234527588\n","Training log: 95 epoch (18048 / 50000 train. data). Loss: 1.4266244173049927\n","Training log: 95 epoch (19328 / 50000 train. data). Loss: 1.3184558153152466\n","Training log: 95 epoch (20608 / 50000 train. data). Loss: 1.2843018770217896\n","Training log: 95 epoch (21888 / 50000 train. data). Loss: 1.3483833074569702\n","Training log: 95 epoch (23168 / 50000 train. data). Loss: 1.4231562614440918\n","Training log: 95 epoch (24448 / 50000 train. data). Loss: 1.3926597833633423\n","Training log: 95 epoch (25728 / 50000 train. data). Loss: 1.5061901807785034\n","Training log: 95 epoch (27008 / 50000 train. data). Loss: 1.258571743965149\n","Training log: 95 epoch (28288 / 50000 train. data). Loss: 1.120492696762085\n","Training log: 95 epoch (29568 / 50000 train. data). Loss: 1.2437940835952759\n","Training log: 95 epoch (30848 / 50000 train. data). Loss: 1.1662997007369995\n","Training log: 95 epoch (32128 / 50000 train. data). Loss: 1.4289731979370117\n","Training log: 95 epoch (33408 / 50000 train. data). Loss: 1.1922478675842285\n","Training log: 95 epoch (34688 / 50000 train. data). Loss: 1.2997932434082031\n","Training log: 95 epoch (35968 / 50000 train. data). Loss: 1.1611697673797607\n","Training log: 95 epoch (37248 / 50000 train. data). Loss: 1.0886000394821167\n","Training log: 95 epoch (38528 / 50000 train. data). Loss: 1.28429114818573\n","Training log: 95 epoch (39808 / 50000 train. data). Loss: 1.2586426734924316\n","Training log: 95 epoch (41088 / 50000 train. data). Loss: 1.1784415245056152\n","Training log: 95 epoch (42368 / 50000 train. data). Loss: 1.3655436038970947\n","Training log: 95 epoch (43648 / 50000 train. data). Loss: 1.4335263967514038\n","Training log: 95 epoch (44928 / 50000 train. data). Loss: 1.0928789377212524\n","Training log: 95 epoch (46208 / 50000 train. data). Loss: 1.3001643419265747\n","Training log: 95 epoch (47488 / 50000 train. data). Loss: 1.1783277988433838\n","Training log: 95 epoch (48768 / 50000 train. data). Loss: 1.231075406074524\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 95 epoch (50048 / 50000 train. data). Loss: 1.3554216623306274\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.85it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.17it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.596900\n","Training log: 96 epoch (128 / 50000 train. data). Loss: 1.3513728380203247\n","Training log: 96 epoch (1408 / 50000 train. data). Loss: 1.054574966430664\n","Training log: 96 epoch (2688 / 50000 train. data). Loss: 1.2037055492401123\n","Training log: 96 epoch (3968 / 50000 train. data). Loss: 1.3426616191864014\n","Training log: 96 epoch (5248 / 50000 train. data). Loss: 1.2044316530227661\n","Training log: 96 epoch (6528 / 50000 train. data). Loss: 1.2259255647659302\n","Training log: 96 epoch (7808 / 50000 train. data). Loss: 1.4227172136306763\n","Training log: 96 epoch (9088 / 50000 train. data). Loss: 1.4767149686813354\n","Training log: 96 epoch (10368 / 50000 train. data). Loss: 1.267963171005249\n","Training log: 96 epoch (11648 / 50000 train. data). Loss: 1.1483515501022339\n","Training log: 96 epoch (12928 / 50000 train. data). Loss: 1.2390209436416626\n","Training log: 96 epoch (14208 / 50000 train. data). Loss: 1.3404748439788818\n","Training log: 96 epoch (15488 / 50000 train. data). Loss: 1.2229678630828857\n","Training log: 96 epoch (16768 / 50000 train. data). Loss: 1.4118951559066772\n","Training log: 96 epoch (18048 / 50000 train. data). Loss: 1.3084158897399902\n","Training log: 96 epoch (19328 / 50000 train. data). Loss: 1.2058327198028564\n","Training log: 96 epoch (20608 / 50000 train. data). Loss: 1.3164259195327759\n","Training log: 96 epoch (21888 / 50000 train. data). Loss: 1.2393817901611328\n","Training log: 96 epoch (23168 / 50000 train. data). Loss: 1.4881385564804077\n","Training log: 96 epoch (24448 / 50000 train. data). Loss: 1.1990792751312256\n","Training log: 96 epoch (25728 / 50000 train. data). Loss: 1.2079635858535767\n","Training log: 96 epoch (27008 / 50000 train. data). Loss: 1.5294380187988281\n","Training log: 96 epoch (28288 / 50000 train. data). Loss: 1.2097488641738892\n","Training log: 96 epoch (29568 / 50000 train. data). Loss: 1.1539483070373535\n","Training log: 96 epoch (30848 / 50000 train. data). Loss: 1.2784979343414307\n","Training log: 96 epoch (32128 / 50000 train. data). Loss: 1.2570327520370483\n","Training log: 96 epoch (33408 / 50000 train. data). Loss: 1.1181104183197021\n","Training log: 96 epoch (34688 / 50000 train. data). Loss: 1.3051244020462036\n","Training log: 96 epoch (35968 / 50000 train. data). Loss: 1.3305519819259644\n","Training log: 96 epoch (37248 / 50000 train. data). Loss: 1.2889504432678223\n","Training log: 96 epoch (38528 / 50000 train. data). Loss: 1.3052787780761719\n","Training log: 96 epoch (39808 / 50000 train. data). Loss: 1.3048137426376343\n","Training log: 96 epoch (41088 / 50000 train. data). Loss: 1.2495839595794678\n","Training log: 96 epoch (42368 / 50000 train. data). Loss: 1.453568935394287\n","Training log: 96 epoch (43648 / 50000 train. data). Loss: 1.4051669836044312\n","Training log: 96 epoch (44928 / 50000 train. data). Loss: 1.3891998529434204\n","Training log: 96 epoch (46208 / 50000 train. data). Loss: 1.3264782428741455\n","Training log: 96 epoch (47488 / 50000 train. data). Loss: 1.1505500078201294\n","Training log: 96 epoch (48768 / 50000 train. data). Loss: 1.2132893800735474\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 96 epoch (50048 / 50000 train. data). Loss: 1.243096947669983\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.73it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.52it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.598000\n","Training log: 97 epoch (128 / 50000 train. data). Loss: 1.2887952327728271\n","Training log: 97 epoch (1408 / 50000 train. data). Loss: 1.3706833124160767\n","Training log: 97 epoch (2688 / 50000 train. data). Loss: 1.2328619956970215\n","Training log: 97 epoch (3968 / 50000 train. data). Loss: 1.3454151153564453\n","Training log: 97 epoch (5248 / 50000 train. data). Loss: 1.1942884922027588\n","Training log: 97 epoch (6528 / 50000 train. data). Loss: 1.334285855293274\n","Training log: 97 epoch (7808 / 50000 train. data). Loss: 1.3037219047546387\n","Training log: 97 epoch (9088 / 50000 train. data). Loss: 1.1670540571212769\n","Training log: 97 epoch (10368 / 50000 train. data). Loss: 1.3542841672897339\n","Training log: 97 epoch (11648 / 50000 train. data). Loss: 1.1293087005615234\n","Training log: 97 epoch (12928 / 50000 train. data). Loss: 1.2383443117141724\n","Training log: 97 epoch (14208 / 50000 train. data). Loss: 1.1883653402328491\n","Training log: 97 epoch (15488 / 50000 train. data). Loss: 1.3572351932525635\n","Training log: 97 epoch (16768 / 50000 train. data). Loss: 1.1898643970489502\n","Training log: 97 epoch (18048 / 50000 train. data). Loss: 1.3762352466583252\n","Training log: 97 epoch (19328 / 50000 train. data). Loss: 1.259614109992981\n","Training log: 97 epoch (20608 / 50000 train. data). Loss: 1.299226999282837\n","Training log: 97 epoch (21888 / 50000 train. data). Loss: 1.3563061952590942\n","Training log: 97 epoch (23168 / 50000 train. data). Loss: 1.2501648664474487\n","Training log: 97 epoch (24448 / 50000 train. data). Loss: 1.1631228923797607\n","Training log: 97 epoch (25728 / 50000 train. data). Loss: 1.4878628253936768\n","Training log: 97 epoch (27008 / 50000 train. data). Loss: 1.2809957265853882\n","Training log: 97 epoch (28288 / 50000 train. data). Loss: 1.393112063407898\n","Training log: 97 epoch (29568 / 50000 train. data). Loss: 1.2448668479919434\n","Training log: 97 epoch (30848 / 50000 train. data). Loss: 1.2848124504089355\n","Training log: 97 epoch (32128 / 50000 train. data). Loss: 1.3178508281707764\n","Training log: 97 epoch (33408 / 50000 train. data). Loss: 1.3648309707641602\n","Training log: 97 epoch (34688 / 50000 train. data). Loss: 1.3030481338500977\n","Training log: 97 epoch (35968 / 50000 train. data). Loss: 1.3014273643493652\n","Training log: 97 epoch (37248 / 50000 train. data). Loss: 1.2473626136779785\n","Training log: 97 epoch (38528 / 50000 train. data). Loss: 1.1908597946166992\n","Training log: 97 epoch (39808 / 50000 train. data). Loss: 1.3062418699264526\n","Training log: 97 epoch (41088 / 50000 train. data). Loss: 1.2228022813796997\n","Training log: 97 epoch (42368 / 50000 train. data). Loss: 1.3649907112121582\n","Training log: 97 epoch (43648 / 50000 train. data). Loss: 1.3426777124404907\n","Training log: 97 epoch (44928 / 50000 train. data). Loss: 1.3217170238494873\n","Training log: 97 epoch (46208 / 50000 train. data). Loss: 1.1960631608963013\n","Training log: 97 epoch (47488 / 50000 train. data). Loss: 1.2127348184585571\n","Training log: 97 epoch (48768 / 50000 train. data). Loss: 1.3869609832763672\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:12, 31.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 97 epoch (50048 / 50000 train. data). Loss: 1.5485588312149048\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.86it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.41it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.597600\n","Training log: 98 epoch (128 / 50000 train. data). Loss: 1.244742751121521\n","Training log: 98 epoch (1408 / 50000 train. data). Loss: 1.271564245223999\n","Training log: 98 epoch (2688 / 50000 train. data). Loss: 1.3825348615646362\n","Training log: 98 epoch (3968 / 50000 train. data). Loss: 1.3145945072174072\n","Training log: 98 epoch (5248 / 50000 train. data). Loss: 1.4881064891815186\n","Training log: 98 epoch (6528 / 50000 train. data). Loss: 1.3282504081726074\n","Training log: 98 epoch (7808 / 50000 train. data). Loss: 1.1807613372802734\n","Training log: 98 epoch (9088 / 50000 train. data). Loss: 1.3096970319747925\n","Training log: 98 epoch (10368 / 50000 train. data). Loss: 1.3902828693389893\n","Training log: 98 epoch (11648 / 50000 train. data). Loss: 1.1659106016159058\n","Training log: 98 epoch (12928 / 50000 train. data). Loss: 1.2487916946411133\n","Training log: 98 epoch (14208 / 50000 train. data). Loss: 1.309242606163025\n","Training log: 98 epoch (15488 / 50000 train. data). Loss: 1.3058093786239624\n","Training log: 98 epoch (16768 / 50000 train. data). Loss: 1.2571388483047485\n","Training log: 98 epoch (18048 / 50000 train. data). Loss: 1.30014967918396\n","Training log: 98 epoch (19328 / 50000 train. data). Loss: 1.5414009094238281\n","Training log: 98 epoch (20608 / 50000 train. data). Loss: 1.3611959218978882\n","Training log: 98 epoch (21888 / 50000 train. data). Loss: 1.220511794090271\n","Training log: 98 epoch (23168 / 50000 train. data). Loss: 1.3537899255752563\n","Training log: 98 epoch (24448 / 50000 train. data). Loss: 1.29264235496521\n","Training log: 98 epoch (25728 / 50000 train. data). Loss: 1.3052902221679688\n","Training log: 98 epoch (27008 / 50000 train. data). Loss: 1.3252308368682861\n","Training log: 98 epoch (28288 / 50000 train. data). Loss: 1.3130148649215698\n","Training log: 98 epoch (29568 / 50000 train. data). Loss: 1.3497929573059082\n","Training log: 98 epoch (30848 / 50000 train. data). Loss: 1.3840563297271729\n","Training log: 98 epoch (32128 / 50000 train. data). Loss: 1.3082653284072876\n","Training log: 98 epoch (33408 / 50000 train. data). Loss: 1.4355117082595825\n","Training log: 98 epoch (34688 / 50000 train. data). Loss: 1.5023750066757202\n","Training log: 98 epoch (35968 / 50000 train. data). Loss: 1.2617769241333008\n","Training log: 98 epoch (37248 / 50000 train. data). Loss: 1.288849115371704\n","Training log: 98 epoch (38528 / 50000 train. data). Loss: 1.3577840328216553\n","Training log: 98 epoch (39808 / 50000 train. data). Loss: 1.302721381187439\n","Training log: 98 epoch (41088 / 50000 train. data). Loss: 1.2751671075820923\n","Training log: 98 epoch (42368 / 50000 train. data). Loss: 1.2390457391738892\n","Training log: 98 epoch (43648 / 50000 train. data). Loss: 1.303375482559204\n","Training log: 98 epoch (44928 / 50000 train. data). Loss: 1.208106279373169\n","Training log: 98 epoch (46208 / 50000 train. data). Loss: 1.263122320175171\n","Training log: 98 epoch (47488 / 50000 train. data). Loss: 1.3181366920471191\n","Training log: 98 epoch (48768 / 50000 train. data). Loss: 1.3529552221298218\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 34.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 98 epoch (50048 / 50000 train. data). Loss: 1.1820659637451172\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.07it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.35it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.598100\n","Training log: 99 epoch (128 / 50000 train. data). Loss: 1.5091365575790405\n","Training log: 99 epoch (1408 / 50000 train. data). Loss: 1.4339817762374878\n","Training log: 99 epoch (2688 / 50000 train. data). Loss: 1.2381762266159058\n","Training log: 99 epoch (3968 / 50000 train. data). Loss: 1.2912431955337524\n","Training log: 99 epoch (5248 / 50000 train. data). Loss: 1.3339406251907349\n","Training log: 99 epoch (6528 / 50000 train. data). Loss: 1.1246861219406128\n","Training log: 99 epoch (7808 / 50000 train. data). Loss: 1.2335468530654907\n","Training log: 99 epoch (9088 / 50000 train. data). Loss: 1.324824333190918\n","Training log: 99 epoch (10368 / 50000 train. data). Loss: 1.3432137966156006\n","Training log: 99 epoch (11648 / 50000 train. data). Loss: 1.2333117723464966\n","Training log: 99 epoch (12928 / 50000 train. data). Loss: 1.367612600326538\n","Training log: 99 epoch (14208 / 50000 train. data). Loss: 1.1238309144973755\n","Training log: 99 epoch (15488 / 50000 train. data). Loss: 1.181939959526062\n","Training log: 99 epoch (16768 / 50000 train. data). Loss: 1.1459764242172241\n","Training log: 99 epoch (18048 / 50000 train. data). Loss: 1.210005283355713\n","Training log: 99 epoch (19328 / 50000 train. data). Loss: 1.3860923051834106\n","Training log: 99 epoch (20608 / 50000 train. data). Loss: 1.2203847169876099\n","Training log: 99 epoch (21888 / 50000 train. data). Loss: 1.1290372610092163\n","Training log: 99 epoch (23168 / 50000 train. data). Loss: 1.2221587896347046\n","Training log: 99 epoch (24448 / 50000 train. data). Loss: 1.247496485710144\n","Training log: 99 epoch (25728 / 50000 train. data). Loss: 1.156379222869873\n","Training log: 99 epoch (27008 / 50000 train. data). Loss: 1.37509024143219\n","Training log: 99 epoch (28288 / 50000 train. data). Loss: 1.3228079080581665\n","Training log: 99 epoch (29568 / 50000 train. data). Loss: 1.2370226383209229\n","Training log: 99 epoch (30848 / 50000 train. data). Loss: 1.4681975841522217\n","Training log: 99 epoch (32128 / 50000 train. data). Loss: 1.4248789548873901\n","Training log: 99 epoch (33408 / 50000 train. data). Loss: 1.0970104932785034\n","Training log: 99 epoch (34688 / 50000 train. data). Loss: 1.2679651975631714\n","Training log: 99 epoch (35968 / 50000 train. data). Loss: 1.2236411571502686\n","Training log: 99 epoch (37248 / 50000 train. data). Loss: 1.3104534149169922\n","Training log: 99 epoch (38528 / 50000 train. data). Loss: 1.2628031969070435\n","Training log: 99 epoch (39808 / 50000 train. data). Loss: 1.2643920183181763\n","Training log: 99 epoch (41088 / 50000 train. data). Loss: 1.3298592567443848\n","Training log: 99 epoch (42368 / 50000 train. data). Loss: 1.2850831747055054\n","Training log: 99 epoch (43648 / 50000 train. data). Loss: 1.3001943826675415\n","Training log: 99 epoch (44928 / 50000 train. data). Loss: 1.4524550437927246\n","Training log: 99 epoch (46208 / 50000 train. data). Loss: 1.4083843231201172\n","Training log: 99 epoch (47488 / 50000 train. data). Loss: 1.127380609512329\n","Training log: 99 epoch (48768 / 50000 train. data). Loss: 1.2914453744888306\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 99 epoch (50048 / 50000 train. data). Loss: 1.1807425022125244\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.96it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.70it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.597700\n","Training log: 100 epoch (128 / 50000 train. data). Loss: 1.2442387342453003\n","Training log: 100 epoch (1408 / 50000 train. data). Loss: 1.2710973024368286\n","Training log: 100 epoch (2688 / 50000 train. data). Loss: 1.3134448528289795\n","Training log: 100 epoch (3968 / 50000 train. data). Loss: 1.2551757097244263\n","Training log: 100 epoch (5248 / 50000 train. data). Loss: 1.1332648992538452\n","Training log: 100 epoch (6528 / 50000 train. data). Loss: 1.3204364776611328\n","Training log: 100 epoch (7808 / 50000 train. data). Loss: 1.3081194162368774\n","Training log: 100 epoch (9088 / 50000 train. data). Loss: 1.4427169561386108\n","Training log: 100 epoch (10368 / 50000 train. data). Loss: 1.2649387121200562\n","Training log: 100 epoch (11648 / 50000 train. data). Loss: 1.1897226572036743\n","Training log: 100 epoch (12928 / 50000 train. data). Loss: 1.3551008701324463\n","Training log: 100 epoch (14208 / 50000 train. data). Loss: 1.4880222082138062\n","Training log: 100 epoch (15488 / 50000 train. data). Loss: 1.3023052215576172\n","Training log: 100 epoch (16768 / 50000 train. data). Loss: 1.413837194442749\n","Training log: 100 epoch (18048 / 50000 train. data). Loss: 1.2089323997497559\n","Training log: 100 epoch (19328 / 50000 train. data). Loss: 1.245298147201538\n","Training log: 100 epoch (20608 / 50000 train. data). Loss: 1.2766401767730713\n","Training log: 100 epoch (21888 / 50000 train. data). Loss: 1.4242899417877197\n","Training log: 100 epoch (23168 / 50000 train. data). Loss: 1.288184642791748\n","Training log: 100 epoch (24448 / 50000 train. data). Loss: 1.2595617771148682\n","Training log: 100 epoch (25728 / 50000 train. data). Loss: 1.439531922340393\n","Training log: 100 epoch (27008 / 50000 train. data). Loss: 1.2573270797729492\n","Training log: 100 epoch (28288 / 50000 train. data). Loss: 1.2972363233566284\n","Training log: 100 epoch (29568 / 50000 train. data). Loss: 1.1171244382858276\n","Training log: 100 epoch (30848 / 50000 train. data). Loss: 1.2874372005462646\n","Training log: 100 epoch (32128 / 50000 train. data). Loss: 1.3270738124847412\n","Training log: 100 epoch (33408 / 50000 train. data). Loss: 1.232715129852295\n","Training log: 100 epoch (34688 / 50000 train. data). Loss: 1.1885292530059814\n","Training log: 100 epoch (35968 / 50000 train. data). Loss: 1.3762530088424683\n","Training log: 100 epoch (37248 / 50000 train. data). Loss: 1.3181394338607788\n","Training log: 100 epoch (38528 / 50000 train. data). Loss: 1.322351336479187\n","Training log: 100 epoch (39808 / 50000 train. data). Loss: 1.256744384765625\n","Training log: 100 epoch (41088 / 50000 train. data). Loss: 1.0704244375228882\n","Training log: 100 epoch (42368 / 50000 train. data). Loss: 1.2405407428741455\n","Training log: 100 epoch (43648 / 50000 train. data). Loss: 1.3746588230133057\n","Training log: 100 epoch (44928 / 50000 train. data). Loss: 1.2438148260116577\n","Training log: 100 epoch (46208 / 50000 train. data). Loss: 1.176486849784851\n","Training log: 100 epoch (47488 / 50000 train. data). Loss: 1.3787897825241089\n","Training log: 100 epoch (48768 / 50000 train. data). Loss: 1.2668293714523315\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 34.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 100 epoch (50048 / 50000 train. data). Loss: 1.159163475036621\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.09it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.81it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.602700\n","Training log: 101 epoch (128 / 50000 train. data). Loss: 1.257731556892395\n","Training log: 101 epoch (1408 / 50000 train. data). Loss: 1.3098257780075073\n","Training log: 101 epoch (2688 / 50000 train. data). Loss: 1.0941569805145264\n","Training log: 101 epoch (3968 / 50000 train. data). Loss: 1.1127175092697144\n","Training log: 101 epoch (5248 / 50000 train. data). Loss: 1.309199333190918\n","Training log: 101 epoch (6528 / 50000 train. data). Loss: 1.1714919805526733\n","Training log: 101 epoch (7808 / 50000 train. data). Loss: 1.3882156610488892\n","Training log: 101 epoch (9088 / 50000 train. data). Loss: 1.2499127388000488\n","Training log: 101 epoch (10368 / 50000 train. data). Loss: 1.2177913188934326\n","Training log: 101 epoch (11648 / 50000 train. data). Loss: 1.248088002204895\n","Training log: 101 epoch (12928 / 50000 train. data). Loss: 1.1271435022354126\n","Training log: 101 epoch (14208 / 50000 train. data). Loss: 1.3264187574386597\n","Training log: 101 epoch (15488 / 50000 train. data). Loss: 1.269387125968933\n","Training log: 101 epoch (16768 / 50000 train. data). Loss: 1.2650002241134644\n","Training log: 101 epoch (18048 / 50000 train. data). Loss: 1.3107644319534302\n","Training log: 101 epoch (19328 / 50000 train. data). Loss: 1.3204846382141113\n","Training log: 101 epoch (20608 / 50000 train. data). Loss: 1.2782632112503052\n","Training log: 101 epoch (21888 / 50000 train. data). Loss: 1.248252034187317\n","Training log: 101 epoch (23168 / 50000 train. data). Loss: 1.2170653343200684\n","Training log: 101 epoch (24448 / 50000 train. data). Loss: 1.4385348558425903\n","Training log: 101 epoch (25728 / 50000 train. data). Loss: 1.2678029537200928\n","Training log: 101 epoch (27008 / 50000 train. data). Loss: 1.4534552097320557\n","Training log: 101 epoch (28288 / 50000 train. data). Loss: 1.1928086280822754\n","Training log: 101 epoch (29568 / 50000 train. data). Loss: 1.2987312078475952\n","Training log: 101 epoch (30848 / 50000 train. data). Loss: 1.281579613685608\n","Training log: 101 epoch (32128 / 50000 train. data). Loss: 1.4089367389678955\n","Training log: 101 epoch (33408 / 50000 train. data). Loss: 1.1685216426849365\n","Training log: 101 epoch (34688 / 50000 train. data). Loss: 1.291874647140503\n","Training log: 101 epoch (35968 / 50000 train. data). Loss: 1.1354143619537354\n","Training log: 101 epoch (37248 / 50000 train. data). Loss: 1.3813832998275757\n","Training log: 101 epoch (38528 / 50000 train. data). Loss: 1.352662205696106\n","Training log: 101 epoch (39808 / 50000 train. data). Loss: 1.3353575468063354\n","Training log: 101 epoch (41088 / 50000 train. data). Loss: 1.3579668998718262\n","Training log: 101 epoch (42368 / 50000 train. data). Loss: 1.068591833114624\n","Training log: 101 epoch (43648 / 50000 train. data). Loss: 1.3264302015304565\n","Training log: 101 epoch (44928 / 50000 train. data). Loss: 1.2197434902191162\n","Training log: 101 epoch (46208 / 50000 train. data). Loss: 1.3986932039260864\n","Training log: 101 epoch (47488 / 50000 train. data). Loss: 1.1571303606033325\n","Training log: 101 epoch (48768 / 50000 train. data). Loss: 1.2660818099975586\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 101 epoch (50048 / 50000 train. data). Loss: 1.142648458480835\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.83it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.02it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.591500\n","Training log: 102 epoch (128 / 50000 train. data). Loss: 1.4469945430755615\n","Training log: 102 epoch (1408 / 50000 train. data). Loss: 1.2401769161224365\n","Training log: 102 epoch (2688 / 50000 train. data). Loss: 1.4744220972061157\n","Training log: 102 epoch (3968 / 50000 train. data). Loss: 1.3704206943511963\n","Training log: 102 epoch (5248 / 50000 train. data). Loss: 1.199445128440857\n","Training log: 102 epoch (6528 / 50000 train. data). Loss: 1.2170794010162354\n","Training log: 102 epoch (7808 / 50000 train. data). Loss: 1.2308156490325928\n","Training log: 102 epoch (9088 / 50000 train. data). Loss: 1.1867011785507202\n","Training log: 102 epoch (10368 / 50000 train. data). Loss: 1.2103271484375\n","Training log: 102 epoch (11648 / 50000 train. data). Loss: 1.3191819190979004\n","Training log: 102 epoch (12928 / 50000 train. data). Loss: 1.4383158683776855\n","Training log: 102 epoch (14208 / 50000 train. data). Loss: 1.310573935508728\n","Training log: 102 epoch (15488 / 50000 train. data). Loss: 1.2797527313232422\n","Training log: 102 epoch (16768 / 50000 train. data). Loss: 1.2480193376541138\n","Training log: 102 epoch (18048 / 50000 train. data). Loss: 1.3515193462371826\n","Training log: 102 epoch (19328 / 50000 train. data). Loss: 1.2215710878372192\n","Training log: 102 epoch (20608 / 50000 train. data). Loss: 1.2404617071151733\n","Training log: 102 epoch (21888 / 50000 train. data). Loss: 1.303025722503662\n","Training log: 102 epoch (23168 / 50000 train. data). Loss: 1.264708399772644\n","Training log: 102 epoch (24448 / 50000 train. data). Loss: 1.2845457792282104\n","Training log: 102 epoch (25728 / 50000 train. data). Loss: 1.198449730873108\n","Training log: 102 epoch (27008 / 50000 train. data). Loss: 1.286676287651062\n","Training log: 102 epoch (28288 / 50000 train. data). Loss: 1.1732022762298584\n","Training log: 102 epoch (29568 / 50000 train. data). Loss: 0.9732682704925537\n","Training log: 102 epoch (30848 / 50000 train. data). Loss: 1.184971570968628\n","Training log: 102 epoch (32128 / 50000 train. data). Loss: 1.2871782779693604\n","Training log: 102 epoch (33408 / 50000 train. data). Loss: 1.1761138439178467\n","Training log: 102 epoch (34688 / 50000 train. data). Loss: 1.174700379371643\n","Training log: 102 epoch (35968 / 50000 train. data). Loss: 1.378311276435852\n","Training log: 102 epoch (37248 / 50000 train. data). Loss: 1.280059576034546\n","Training log: 102 epoch (38528 / 50000 train. data). Loss: 1.147187352180481\n","Training log: 102 epoch (39808 / 50000 train. data). Loss: 1.2613425254821777\n","Training log: 102 epoch (41088 / 50000 train. data). Loss: 1.3158782720565796\n","Training log: 102 epoch (42368 / 50000 train. data). Loss: 1.4531375169754028\n","Training log: 102 epoch (43648 / 50000 train. data). Loss: 1.3477027416229248\n","Training log: 102 epoch (44928 / 50000 train. data). Loss: 1.3296489715576172\n","Training log: 102 epoch (46208 / 50000 train. data). Loss: 1.2977147102355957\n","Training log: 102 epoch (47488 / 50000 train. data). Loss: 1.512323021888733\n","Training log: 102 epoch (48768 / 50000 train. data). Loss: 1.2250962257385254\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 102 epoch (50048 / 50000 train. data). Loss: 1.1769707202911377\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.69it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.70it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.602200\n","Training log: 103 epoch (128 / 50000 train. data). Loss: 1.3101850748062134\n","Training log: 103 epoch (1408 / 50000 train. data). Loss: 1.394574761390686\n","Training log: 103 epoch (2688 / 50000 train. data). Loss: 1.309767484664917\n","Training log: 103 epoch (3968 / 50000 train. data). Loss: 1.3931260108947754\n","Training log: 103 epoch (5248 / 50000 train. data). Loss: 1.262129545211792\n","Training log: 103 epoch (6528 / 50000 train. data). Loss: 1.4807337522506714\n","Training log: 103 epoch (7808 / 50000 train. data). Loss: 1.2386599779129028\n","Training log: 103 epoch (9088 / 50000 train. data). Loss: 1.1856640577316284\n","Training log: 103 epoch (10368 / 50000 train. data). Loss: 1.391050100326538\n","Training log: 103 epoch (11648 / 50000 train. data). Loss: 1.275169014930725\n","Training log: 103 epoch (12928 / 50000 train. data). Loss: 1.382179617881775\n","Training log: 103 epoch (14208 / 50000 train. data). Loss: 1.1943891048431396\n","Training log: 103 epoch (15488 / 50000 train. data). Loss: 1.31098210811615\n","Training log: 103 epoch (16768 / 50000 train. data). Loss: 1.2789775133132935\n","Training log: 103 epoch (18048 / 50000 train. data). Loss: 1.1796597242355347\n","Training log: 103 epoch (19328 / 50000 train. data). Loss: 1.3923817873001099\n","Training log: 103 epoch (20608 / 50000 train. data). Loss: 1.2979319095611572\n","Training log: 103 epoch (21888 / 50000 train. data). Loss: 1.2768226861953735\n","Training log: 103 epoch (23168 / 50000 train. data). Loss: 1.2555047273635864\n","Training log: 103 epoch (24448 / 50000 train. data). Loss: 1.1943292617797852\n","Training log: 103 epoch (25728 / 50000 train. data). Loss: 1.1973782777786255\n","Training log: 103 epoch (27008 / 50000 train. data). Loss: 1.2608755826950073\n","Training log: 103 epoch (28288 / 50000 train. data). Loss: 1.175279974937439\n","Training log: 103 epoch (29568 / 50000 train. data). Loss: 1.1998599767684937\n","Training log: 103 epoch (30848 / 50000 train. data). Loss: 1.1151643991470337\n","Training log: 103 epoch (32128 / 50000 train. data). Loss: 1.0310136079788208\n","Training log: 103 epoch (33408 / 50000 train. data). Loss: 1.2070870399475098\n","Training log: 103 epoch (34688 / 50000 train. data). Loss: 1.219518780708313\n","Training log: 103 epoch (35968 / 50000 train. data). Loss: 1.3520227670669556\n","Training log: 103 epoch (37248 / 50000 train. data). Loss: 1.3569555282592773\n","Training log: 103 epoch (38528 / 50000 train. data). Loss: 1.3647007942199707\n","Training log: 103 epoch (39808 / 50000 train. data). Loss: 1.3523472547531128\n","Training log: 103 epoch (41088 / 50000 train. data). Loss: 1.2844574451446533\n","Training log: 103 epoch (42368 / 50000 train. data). Loss: 1.2651420831680298\n","Training log: 103 epoch (43648 / 50000 train. data). Loss: 1.1706444025039673\n","Training log: 103 epoch (44928 / 50000 train. data). Loss: 1.3217790126800537\n","Training log: 103 epoch (46208 / 50000 train. data). Loss: 1.2873669862747192\n","Training log: 103 epoch (47488 / 50000 train. data). Loss: 1.2615588903427124\n","Training log: 103 epoch (48768 / 50000 train. data). Loss: 1.310027837753296\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 103 epoch (50048 / 50000 train. data). Loss: 1.2713443040847778\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.94it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.27it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.602000\n","Training log: 104 epoch (128 / 50000 train. data). Loss: 1.3051716089248657\n","Training log: 104 epoch (1408 / 50000 train. data). Loss: 1.0441492795944214\n","Training log: 104 epoch (2688 / 50000 train. data). Loss: 1.2930114269256592\n","Training log: 104 epoch (3968 / 50000 train. data). Loss: 1.2462241649627686\n","Training log: 104 epoch (5248 / 50000 train. data). Loss: 1.4002578258514404\n","Training log: 104 epoch (6528 / 50000 train. data). Loss: 1.2627795934677124\n","Training log: 104 epoch (7808 / 50000 train. data). Loss: 1.3504936695098877\n","Training log: 104 epoch (9088 / 50000 train. data). Loss: 1.3003714084625244\n","Training log: 104 epoch (10368 / 50000 train. data). Loss: 1.2075822353363037\n","Training log: 104 epoch (11648 / 50000 train. data). Loss: 1.1947264671325684\n","Training log: 104 epoch (12928 / 50000 train. data). Loss: 1.3044688701629639\n","Training log: 104 epoch (14208 / 50000 train. data). Loss: 1.2080798149108887\n","Training log: 104 epoch (15488 / 50000 train. data). Loss: 1.120425820350647\n","Training log: 104 epoch (16768 / 50000 train. data). Loss: 1.527207374572754\n","Training log: 104 epoch (18048 / 50000 train. data). Loss: 1.342390775680542\n","Training log: 104 epoch (19328 / 50000 train. data). Loss: 1.2741605043411255\n","Training log: 104 epoch (20608 / 50000 train. data). Loss: 1.1020013093948364\n","Training log: 104 epoch (21888 / 50000 train. data). Loss: 1.2748870849609375\n","Training log: 104 epoch (23168 / 50000 train. data). Loss: 1.4107927083969116\n","Training log: 104 epoch (24448 / 50000 train. data). Loss: 1.303707242012024\n","Training log: 104 epoch (25728 / 50000 train. data). Loss: 1.1491397619247437\n","Training log: 104 epoch (27008 / 50000 train. data). Loss: 1.1572012901306152\n","Training log: 104 epoch (28288 / 50000 train. data). Loss: 1.1689510345458984\n","Training log: 104 epoch (29568 / 50000 train. data). Loss: 1.2669224739074707\n","Training log: 104 epoch (30848 / 50000 train. data). Loss: 1.4347901344299316\n","Training log: 104 epoch (32128 / 50000 train. data). Loss: 1.1995971202850342\n","Training log: 104 epoch (33408 / 50000 train. data). Loss: 1.295337438583374\n","Training log: 104 epoch (34688 / 50000 train. data). Loss: 1.2953811883926392\n","Training log: 104 epoch (35968 / 50000 train. data). Loss: 1.2441447973251343\n","Training log: 104 epoch (37248 / 50000 train. data). Loss: 1.320097804069519\n","Training log: 104 epoch (38528 / 50000 train. data). Loss: 1.1325976848602295\n","Training log: 104 epoch (39808 / 50000 train. data). Loss: 1.323915958404541\n","Training log: 104 epoch (41088 / 50000 train. data). Loss: 1.2809149026870728\n","Training log: 104 epoch (42368 / 50000 train. data). Loss: 1.3978286981582642\n","Training log: 104 epoch (43648 / 50000 train. data). Loss: 1.3937551975250244\n","Training log: 104 epoch (44928 / 50000 train. data). Loss: 1.3303611278533936\n","Training log: 104 epoch (46208 / 50000 train. data). Loss: 1.3143559694290161\n","Training log: 104 epoch (47488 / 50000 train. data). Loss: 1.228105068206787\n","Training log: 104 epoch (48768 / 50000 train. data). Loss: 1.286930799484253\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 34.02it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 104 epoch (50048 / 50000 train. data). Loss: 1.3329699039459229\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.90it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.01it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.605600\n","Training log: 105 epoch (128 / 50000 train. data). Loss: 1.301039218902588\n","Training log: 105 epoch (1408 / 50000 train. data). Loss: 1.3319382667541504\n","Training log: 105 epoch (2688 / 50000 train. data). Loss: 1.1922470331192017\n","Training log: 105 epoch (3968 / 50000 train. data). Loss: 1.3066483736038208\n","Training log: 105 epoch (5248 / 50000 train. data). Loss: 1.3744786977767944\n","Training log: 105 epoch (6528 / 50000 train. data). Loss: 1.2137013673782349\n","Training log: 105 epoch (7808 / 50000 train. data). Loss: 1.3133397102355957\n","Training log: 105 epoch (9088 / 50000 train. data). Loss: 1.3404948711395264\n","Training log: 105 epoch (10368 / 50000 train. data). Loss: 1.1900596618652344\n","Training log: 105 epoch (11648 / 50000 train. data). Loss: 1.2380541563034058\n","Training log: 105 epoch (12928 / 50000 train. data). Loss: 1.438403606414795\n","Training log: 105 epoch (14208 / 50000 train. data). Loss: 1.1696536540985107\n","Training log: 105 epoch (15488 / 50000 train. data). Loss: 1.3611286878585815\n","Training log: 105 epoch (16768 / 50000 train. data). Loss: 1.4942386150360107\n","Training log: 105 epoch (18048 / 50000 train. data). Loss: 1.2446991205215454\n","Training log: 105 epoch (19328 / 50000 train. data). Loss: 1.2215516567230225\n","Training log: 105 epoch (20608 / 50000 train. data). Loss: 1.2211681604385376\n","Training log: 105 epoch (21888 / 50000 train. data). Loss: 1.4184653759002686\n","Training log: 105 epoch (23168 / 50000 train. data). Loss: 1.2868939638137817\n","Training log: 105 epoch (24448 / 50000 train. data). Loss: 1.3806045055389404\n","Training log: 105 epoch (25728 / 50000 train. data). Loss: 1.4079160690307617\n","Training log: 105 epoch (27008 / 50000 train. data). Loss: 1.35999596118927\n","Training log: 105 epoch (28288 / 50000 train. data). Loss: 1.1728501319885254\n","Training log: 105 epoch (29568 / 50000 train. data). Loss: 1.3264472484588623\n","Training log: 105 epoch (30848 / 50000 train. data). Loss: 1.4038383960723877\n","Training log: 105 epoch (32128 / 50000 train. data). Loss: 1.309859275817871\n","Training log: 105 epoch (33408 / 50000 train. data). Loss: 1.2608747482299805\n","Training log: 105 epoch (34688 / 50000 train. data). Loss: 1.493168592453003\n","Training log: 105 epoch (35968 / 50000 train. data). Loss: 1.2723090648651123\n","Training log: 105 epoch (37248 / 50000 train. data). Loss: 1.32539701461792\n","Training log: 105 epoch (38528 / 50000 train. data). Loss: 1.114696979522705\n","Training log: 105 epoch (39808 / 50000 train. data). Loss: 1.3312275409698486\n","Training log: 105 epoch (41088 / 50000 train. data). Loss: 1.2958065271377563\n","Training log: 105 epoch (42368 / 50000 train. data). Loss: 1.2859346866607666\n","Training log: 105 epoch (43648 / 50000 train. data). Loss: 1.1651866436004639\n","Training log: 105 epoch (44928 / 50000 train. data). Loss: 1.3023900985717773\n","Training log: 105 epoch (46208 / 50000 train. data). Loss: 1.4145386219024658\n","Training log: 105 epoch (47488 / 50000 train. data). Loss: 1.2708660364151\n","Training log: 105 epoch (48768 / 50000 train. data). Loss: 1.3624788522720337\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 105 epoch (50048 / 50000 train. data). Loss: 1.2677505016326904\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.64it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.05it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.602500\n","Training log: 106 epoch (128 / 50000 train. data). Loss: 1.1949101686477661\n","Training log: 106 epoch (1408 / 50000 train. data). Loss: 1.4644486904144287\n","Training log: 106 epoch (2688 / 50000 train. data). Loss: 1.3619548082351685\n","Training log: 106 epoch (3968 / 50000 train. data). Loss: 1.13600492477417\n","Training log: 106 epoch (5248 / 50000 train. data). Loss: 1.3569464683532715\n","Training log: 106 epoch (6528 / 50000 train. data). Loss: 1.2258669137954712\n","Training log: 106 epoch (7808 / 50000 train. data). Loss: 1.3805932998657227\n","Training log: 106 epoch (9088 / 50000 train. data). Loss: 1.2240535020828247\n","Training log: 106 epoch (10368 / 50000 train. data). Loss: 1.395479440689087\n","Training log: 106 epoch (11648 / 50000 train. data). Loss: 1.2744145393371582\n","Training log: 106 epoch (12928 / 50000 train. data). Loss: 1.153258204460144\n","Training log: 106 epoch (14208 / 50000 train. data). Loss: 1.4339836835861206\n","Training log: 106 epoch (15488 / 50000 train. data). Loss: 1.3271178007125854\n","Training log: 106 epoch (16768 / 50000 train. data). Loss: 1.1441595554351807\n","Training log: 106 epoch (18048 / 50000 train. data). Loss: 1.4338829517364502\n","Training log: 106 epoch (19328 / 50000 train. data). Loss: 1.3512187004089355\n","Training log: 106 epoch (20608 / 50000 train. data). Loss: 1.2649534940719604\n","Training log: 106 epoch (21888 / 50000 train. data). Loss: 1.2603155374526978\n","Training log: 106 epoch (23168 / 50000 train. data). Loss: 1.1063709259033203\n","Training log: 106 epoch (24448 / 50000 train. data). Loss: 1.2470238208770752\n","Training log: 106 epoch (25728 / 50000 train. data). Loss: 1.0843708515167236\n","Training log: 106 epoch (27008 / 50000 train. data). Loss: 1.2879934310913086\n","Training log: 106 epoch (28288 / 50000 train. data). Loss: 1.3351410627365112\n","Training log: 106 epoch (29568 / 50000 train. data). Loss: 1.154977560043335\n","Training log: 106 epoch (30848 / 50000 train. data). Loss: 1.1390432119369507\n","Training log: 106 epoch (32128 / 50000 train. data). Loss: 1.389373779296875\n","Training log: 106 epoch (33408 / 50000 train. data). Loss: 1.3392127752304077\n","Training log: 106 epoch (34688 / 50000 train. data). Loss: 1.2742573022842407\n","Training log: 106 epoch (35968 / 50000 train. data). Loss: 1.1156145334243774\n","Training log: 106 epoch (37248 / 50000 train. data). Loss: 1.178176999092102\n","Training log: 106 epoch (38528 / 50000 train. data). Loss: 1.1971901655197144\n","Training log: 106 epoch (39808 / 50000 train. data). Loss: 1.2348617315292358\n","Training log: 106 epoch (41088 / 50000 train. data). Loss: 1.2609612941741943\n","Training log: 106 epoch (42368 / 50000 train. data). Loss: 1.2259132862091064\n","Training log: 106 epoch (43648 / 50000 train. data). Loss: 1.2167775630950928\n","Training log: 106 epoch (44928 / 50000 train. data). Loss: 1.3223233222961426\n","Training log: 106 epoch (46208 / 50000 train. data). Loss: 1.1546069383621216\n","Training log: 106 epoch (47488 / 50000 train. data). Loss: 1.3095219135284424\n","Training log: 106 epoch (48768 / 50000 train. data). Loss: 1.4586600065231323\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 33.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 106 epoch (50048 / 50000 train. data). Loss: 1.2119337320327759\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.95it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.22it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.599700\n","Training log: 107 epoch (128 / 50000 train. data). Loss: 1.1684253215789795\n","Training log: 107 epoch (1408 / 50000 train. data). Loss: 1.1758123636245728\n","Training log: 107 epoch (2688 / 50000 train. data). Loss: 1.1523518562316895\n","Training log: 107 epoch (3968 / 50000 train. data). Loss: 1.2644124031066895\n","Training log: 107 epoch (5248 / 50000 train. data). Loss: 1.1963928937911987\n","Training log: 107 epoch (6528 / 50000 train. data). Loss: 1.2086105346679688\n","Training log: 107 epoch (7808 / 50000 train. data). Loss: 1.1294969320297241\n","Training log: 107 epoch (9088 / 50000 train. data). Loss: 1.2905088663101196\n","Training log: 107 epoch (10368 / 50000 train. data). Loss: 1.274729609489441\n","Training log: 107 epoch (11648 / 50000 train. data). Loss: 1.3847274780273438\n","Training log: 107 epoch (12928 / 50000 train. data). Loss: 1.2521308660507202\n","Training log: 107 epoch (14208 / 50000 train. data). Loss: 1.2124357223510742\n","Training log: 107 epoch (15488 / 50000 train. data). Loss: 1.2985037565231323\n","Training log: 107 epoch (16768 / 50000 train. data). Loss: 1.2618223428726196\n","Training log: 107 epoch (18048 / 50000 train. data). Loss: 1.2187081575393677\n","Training log: 107 epoch (19328 / 50000 train. data). Loss: 1.233271837234497\n","Training log: 107 epoch (20608 / 50000 train. data). Loss: 1.3698360919952393\n","Training log: 107 epoch (21888 / 50000 train. data). Loss: 1.2136527299880981\n","Training log: 107 epoch (23168 / 50000 train. data). Loss: 1.2031160593032837\n","Training log: 107 epoch (24448 / 50000 train. data). Loss: 1.3031151294708252\n","Training log: 107 epoch (25728 / 50000 train. data). Loss: 1.3499423265457153\n","Training log: 107 epoch (27008 / 50000 train. data). Loss: 1.3306292295455933\n","Training log: 107 epoch (28288 / 50000 train. data). Loss: 1.2667604684829712\n","Training log: 107 epoch (29568 / 50000 train. data). Loss: 1.2013163566589355\n","Training log: 107 epoch (30848 / 50000 train. data). Loss: 1.337594747543335\n","Training log: 107 epoch (32128 / 50000 train. data). Loss: 1.237212061882019\n","Training log: 107 epoch (33408 / 50000 train. data). Loss: 1.2575623989105225\n","Training log: 107 epoch (34688 / 50000 train. data). Loss: 1.4708062410354614\n","Training log: 107 epoch (35968 / 50000 train. data). Loss: 1.4248130321502686\n","Training log: 107 epoch (37248 / 50000 train. data). Loss: 1.2297097444534302\n","Training log: 107 epoch (38528 / 50000 train. data). Loss: 1.1692692041397095\n","Training log: 107 epoch (39808 / 50000 train. data). Loss: 1.313156247138977\n","Training log: 107 epoch (41088 / 50000 train. data). Loss: 1.1205382347106934\n","Training log: 107 epoch (42368 / 50000 train. data). Loss: 1.3654183149337769\n","Training log: 107 epoch (43648 / 50000 train. data). Loss: 1.286414384841919\n","Training log: 107 epoch (44928 / 50000 train. data). Loss: 1.3219355344772339\n","Training log: 107 epoch (46208 / 50000 train. data). Loss: 1.2181222438812256\n","Training log: 107 epoch (47488 / 50000 train. data). Loss: 1.1673922538757324\n","Training log: 107 epoch (48768 / 50000 train. data). Loss: 1.3246102333068848\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 33.13it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 107 epoch (50048 / 50000 train. data). Loss: 1.1586085557937622\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.10it/s]\n","100%|██████████| 79/79 [00:02<00:00, 32.81it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.599800\n","Training log: 108 epoch (128 / 50000 train. data). Loss: 1.1528263092041016\n","Training log: 108 epoch (1408 / 50000 train. data). Loss: 1.217103123664856\n","Training log: 108 epoch (2688 / 50000 train. data). Loss: 1.1593726873397827\n","Training log: 108 epoch (3968 / 50000 train. data). Loss: 1.2276983261108398\n","Training log: 108 epoch (5248 / 50000 train. data). Loss: 1.224096417427063\n","Training log: 108 epoch (6528 / 50000 train. data). Loss: 1.2115592956542969\n","Training log: 108 epoch (7808 / 50000 train. data). Loss: 1.1441105604171753\n","Training log: 108 epoch (9088 / 50000 train. data). Loss: 1.226707100868225\n","Training log: 108 epoch (10368 / 50000 train. data). Loss: 1.3445333242416382\n","Training log: 108 epoch (11648 / 50000 train. data). Loss: 1.4143741130828857\n","Training log: 108 epoch (12928 / 50000 train. data). Loss: 1.208748459815979\n","Training log: 108 epoch (14208 / 50000 train. data). Loss: 1.4386956691741943\n","Training log: 108 epoch (15488 / 50000 train. data). Loss: 1.2505501508712769\n","Training log: 108 epoch (16768 / 50000 train. data). Loss: 1.299744963645935\n","Training log: 108 epoch (18048 / 50000 train. data). Loss: 1.3601926565170288\n","Training log: 108 epoch (19328 / 50000 train. data). Loss: 1.2046189308166504\n","Training log: 108 epoch (20608 / 50000 train. data). Loss: 1.2597637176513672\n","Training log: 108 epoch (21888 / 50000 train. data). Loss: 1.2320938110351562\n","Training log: 108 epoch (23168 / 50000 train. data). Loss: 1.4299812316894531\n","Training log: 108 epoch (24448 / 50000 train. data). Loss: 1.3563499450683594\n","Training log: 108 epoch (25728 / 50000 train. data). Loss: 1.2922922372817993\n","Training log: 108 epoch (27008 / 50000 train. data). Loss: 1.3809239864349365\n","Training log: 108 epoch (28288 / 50000 train. data). Loss: 1.1128361225128174\n","Training log: 108 epoch (29568 / 50000 train. data). Loss: 1.4753410816192627\n","Training log: 108 epoch (30848 / 50000 train. data). Loss: 1.2314515113830566\n","Training log: 108 epoch (32128 / 50000 train. data). Loss: 1.1588807106018066\n","Training log: 108 epoch (33408 / 50000 train. data). Loss: 1.2526506185531616\n","Training log: 108 epoch (34688 / 50000 train. data). Loss: 1.2596657276153564\n","Training log: 108 epoch (35968 / 50000 train. data). Loss: 1.262080192565918\n","Training log: 108 epoch (37248 / 50000 train. data). Loss: 1.2438838481903076\n","Training log: 108 epoch (38528 / 50000 train. data). Loss: 1.422315239906311\n","Training log: 108 epoch (39808 / 50000 train. data). Loss: 1.270013689994812\n","Training log: 108 epoch (41088 / 50000 train. data). Loss: 1.190790057182312\n","Training log: 108 epoch (42368 / 50000 train. data). Loss: 1.3057870864868164\n","Training log: 108 epoch (43648 / 50000 train. data). Loss: 1.1611868143081665\n","Training log: 108 epoch (44928 / 50000 train. data). Loss: 1.1273249387741089\n","Training log: 108 epoch (46208 / 50000 train. data). Loss: 1.4108809232711792\n","Training log: 108 epoch (47488 / 50000 train. data). Loss: 1.1715177297592163\n","Training log: 108 epoch (48768 / 50000 train. data). Loss: 1.295542597770691\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.07it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 108 epoch (50048 / 50000 train. data). Loss: 1.05038583278656\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.65it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.99it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.602700\n","Training log: 109 epoch (128 / 50000 train. data). Loss: 1.1742749214172363\n","Training log: 109 epoch (1408 / 50000 train. data). Loss: 1.1700881719589233\n","Training log: 109 epoch (2688 / 50000 train. data). Loss: 1.5126875638961792\n","Training log: 109 epoch (3968 / 50000 train. data). Loss: 1.2869739532470703\n","Training log: 109 epoch (5248 / 50000 train. data). Loss: 1.1460152864456177\n","Training log: 109 epoch (6528 / 50000 train. data). Loss: 1.3376562595367432\n","Training log: 109 epoch (7808 / 50000 train. data). Loss: 1.1757599115371704\n","Training log: 109 epoch (9088 / 50000 train. data). Loss: 1.27042818069458\n","Training log: 109 epoch (10368 / 50000 train. data). Loss: 1.239798665046692\n","Training log: 109 epoch (11648 / 50000 train. data). Loss: 1.2952919006347656\n","Training log: 109 epoch (12928 / 50000 train. data). Loss: 1.3773691654205322\n","Training log: 109 epoch (14208 / 50000 train. data). Loss: 1.2199147939682007\n","Training log: 109 epoch (15488 / 50000 train. data). Loss: 1.14162278175354\n","Training log: 109 epoch (16768 / 50000 train. data). Loss: 1.0703754425048828\n","Training log: 109 epoch (18048 / 50000 train. data). Loss: 1.1989293098449707\n","Training log: 109 epoch (19328 / 50000 train. data). Loss: 1.411367416381836\n","Training log: 109 epoch (20608 / 50000 train. data). Loss: 1.0816904306411743\n","Training log: 109 epoch (21888 / 50000 train. data). Loss: 1.4386917352676392\n","Training log: 109 epoch (23168 / 50000 train. data). Loss: 1.4493951797485352\n","Training log: 109 epoch (24448 / 50000 train. data). Loss: 1.1984899044036865\n","Training log: 109 epoch (25728 / 50000 train. data). Loss: 1.260128378868103\n","Training log: 109 epoch (27008 / 50000 train. data). Loss: 1.319601058959961\n","Training log: 109 epoch (28288 / 50000 train. data). Loss: 1.2730538845062256\n","Training log: 109 epoch (29568 / 50000 train. data). Loss: 1.3295204639434814\n","Training log: 109 epoch (30848 / 50000 train. data). Loss: 1.3403780460357666\n","Training log: 109 epoch (32128 / 50000 train. data). Loss: 1.211884617805481\n","Training log: 109 epoch (33408 / 50000 train. data). Loss: 1.3771302700042725\n","Training log: 109 epoch (34688 / 50000 train. data). Loss: 1.3158886432647705\n","Training log: 109 epoch (35968 / 50000 train. data). Loss: 1.359156847000122\n","Training log: 109 epoch (37248 / 50000 train. data). Loss: 1.1127949953079224\n","Training log: 109 epoch (38528 / 50000 train. data). Loss: 1.2795127630233765\n","Training log: 109 epoch (39808 / 50000 train. data). Loss: 1.2642935514450073\n","Training log: 109 epoch (41088 / 50000 train. data). Loss: 1.1269679069519043\n","Training log: 109 epoch (42368 / 50000 train. data). Loss: 1.3179895877838135\n","Training log: 109 epoch (43648 / 50000 train. data). Loss: 1.2479606866836548\n","Training log: 109 epoch (44928 / 50000 train. data). Loss: 1.3541697263717651\n","Training log: 109 epoch (46208 / 50000 train. data). Loss: 1.4164682626724243\n","Training log: 109 epoch (47488 / 50000 train. data). Loss: 1.4185911417007446\n","Training log: 109 epoch (48768 / 50000 train. data). Loss: 1.4226114749908447\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:12, 30.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 109 epoch (50048 / 50000 train. data). Loss: 1.2797890901565552\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.83it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.16it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.608000\n","Training log: 110 epoch (128 / 50000 train. data). Loss: 1.3353888988494873\n","Training log: 110 epoch (1408 / 50000 train. data). Loss: 1.1487467288970947\n","Training log: 110 epoch (2688 / 50000 train. data). Loss: 1.348952054977417\n","Training log: 110 epoch (3968 / 50000 train. data). Loss: 1.2388445138931274\n","Training log: 110 epoch (5248 / 50000 train. data). Loss: 1.2265899181365967\n","Training log: 110 epoch (6528 / 50000 train. data). Loss: 1.276902198791504\n","Training log: 110 epoch (7808 / 50000 train. data). Loss: 1.2525732517242432\n","Training log: 110 epoch (9088 / 50000 train. data). Loss: 1.1503275632858276\n","Training log: 110 epoch (10368 / 50000 train. data). Loss: 1.2992655038833618\n","Training log: 110 epoch (11648 / 50000 train. data). Loss: 1.34439218044281\n","Training log: 110 epoch (12928 / 50000 train. data). Loss: 1.2849842309951782\n","Training log: 110 epoch (14208 / 50000 train. data). Loss: 1.2919453382492065\n","Training log: 110 epoch (15488 / 50000 train. data). Loss: 1.2131608724594116\n","Training log: 110 epoch (16768 / 50000 train. data). Loss: 1.3682019710540771\n","Training log: 110 epoch (18048 / 50000 train. data). Loss: 1.3607771396636963\n","Training log: 110 epoch (19328 / 50000 train. data). Loss: 1.2567167282104492\n","Training log: 110 epoch (20608 / 50000 train. data). Loss: 1.3259413242340088\n","Training log: 110 epoch (21888 / 50000 train. data). Loss: 1.5076041221618652\n","Training log: 110 epoch (23168 / 50000 train. data). Loss: 1.205319881439209\n","Training log: 110 epoch (24448 / 50000 train. data). Loss: 1.3114937543869019\n","Training log: 110 epoch (25728 / 50000 train. data). Loss: 1.1875122785568237\n","Training log: 110 epoch (27008 / 50000 train. data). Loss: 1.196043610572815\n","Training log: 110 epoch (28288 / 50000 train. data). Loss: 1.1203250885009766\n","Training log: 110 epoch (29568 / 50000 train. data). Loss: 1.2113043069839478\n","Training log: 110 epoch (30848 / 50000 train. data). Loss: 1.220661997795105\n","Training log: 110 epoch (32128 / 50000 train. data). Loss: 1.2778500318527222\n","Training log: 110 epoch (33408 / 50000 train. data). Loss: 1.1471344232559204\n","Training log: 110 epoch (34688 / 50000 train. data). Loss: 1.3276236057281494\n","Training log: 110 epoch (35968 / 50000 train. data). Loss: 1.3284276723861694\n","Training log: 110 epoch (37248 / 50000 train. data). Loss: 1.3575845956802368\n","Training log: 110 epoch (38528 / 50000 train. data). Loss: 1.441762924194336\n","Training log: 110 epoch (39808 / 50000 train. data). Loss: 1.4273918867111206\n","Training log: 110 epoch (41088 / 50000 train. data). Loss: 1.133175253868103\n","Training log: 110 epoch (42368 / 50000 train. data). Loss: 1.313834309577942\n","Training log: 110 epoch (43648 / 50000 train. data). Loss: 1.3707977533340454\n","Training log: 110 epoch (44928 / 50000 train. data). Loss: 1.2026246786117554\n","Training log: 110 epoch (46208 / 50000 train. data). Loss: 1.375881552696228\n","Training log: 110 epoch (47488 / 50000 train. data). Loss: 1.208722472190857\n","Training log: 110 epoch (48768 / 50000 train. data). Loss: 1.3979878425598145\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 110 epoch (50048 / 50000 train. data). Loss: 1.0924052000045776\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.14it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.94it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.608500\n","Training log: 111 epoch (128 / 50000 train. data). Loss: 1.219763994216919\n","Training log: 111 epoch (1408 / 50000 train. data). Loss: 1.3191416263580322\n","Training log: 111 epoch (2688 / 50000 train. data). Loss: 1.2610349655151367\n","Training log: 111 epoch (3968 / 50000 train. data). Loss: 1.212683916091919\n","Training log: 111 epoch (5248 / 50000 train. data). Loss: 1.2193399667739868\n","Training log: 111 epoch (6528 / 50000 train. data). Loss: 1.4276970624923706\n","Training log: 111 epoch (7808 / 50000 train. data). Loss: 1.0626217126846313\n","Training log: 111 epoch (9088 / 50000 train. data). Loss: 1.4035183191299438\n","Training log: 111 epoch (10368 / 50000 train. data). Loss: 1.469064474105835\n","Training log: 111 epoch (11648 / 50000 train. data). Loss: 1.136927604675293\n","Training log: 111 epoch (12928 / 50000 train. data). Loss: 1.381323218345642\n","Training log: 111 epoch (14208 / 50000 train. data). Loss: 1.229594111442566\n","Training log: 111 epoch (15488 / 50000 train. data). Loss: 1.326177954673767\n","Training log: 111 epoch (16768 / 50000 train. data). Loss: 0.9970163702964783\n","Training log: 111 epoch (18048 / 50000 train. data). Loss: 1.1861934661865234\n","Training log: 111 epoch (19328 / 50000 train. data). Loss: 1.389011263847351\n","Training log: 111 epoch (20608 / 50000 train. data). Loss: 1.2699466943740845\n","Training log: 111 epoch (21888 / 50000 train. data). Loss: 1.3680450916290283\n","Training log: 111 epoch (23168 / 50000 train. data). Loss: 1.340536117553711\n","Training log: 111 epoch (24448 / 50000 train. data). Loss: 1.2291500568389893\n","Training log: 111 epoch (25728 / 50000 train. data). Loss: 1.1542861461639404\n","Training log: 111 epoch (27008 / 50000 train. data). Loss: 1.2729700803756714\n","Training log: 111 epoch (28288 / 50000 train. data). Loss: 1.0347797870635986\n","Training log: 111 epoch (29568 / 50000 train. data). Loss: 1.1487973928451538\n","Training log: 111 epoch (30848 / 50000 train. data). Loss: 1.161818265914917\n","Training log: 111 epoch (32128 / 50000 train. data). Loss: 1.2886637449264526\n","Training log: 111 epoch (33408 / 50000 train. data). Loss: 1.3933842182159424\n","Training log: 111 epoch (34688 / 50000 train. data). Loss: 1.2488377094268799\n","Training log: 111 epoch (35968 / 50000 train. data). Loss: 1.247737169265747\n","Training log: 111 epoch (37248 / 50000 train. data). Loss: 1.1145800352096558\n","Training log: 111 epoch (38528 / 50000 train. data). Loss: 1.1717199087142944\n","Training log: 111 epoch (39808 / 50000 train. data). Loss: 1.176826000213623\n","Training log: 111 epoch (41088 / 50000 train. data). Loss: 1.3566395044326782\n","Training log: 111 epoch (42368 / 50000 train. data). Loss: 1.2804518938064575\n","Training log: 111 epoch (43648 / 50000 train. data). Loss: 1.3134500980377197\n","Training log: 111 epoch (44928 / 50000 train. data). Loss: 1.268186092376709\n","Training log: 111 epoch (46208 / 50000 train. data). Loss: 1.4532434940338135\n","Training log: 111 epoch (47488 / 50000 train. data). Loss: 1.3852012157440186\n","Training log: 111 epoch (48768 / 50000 train. data). Loss: 1.5056350231170654\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.35it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 111 epoch (50048 / 50000 train. data). Loss: 1.4840701818466187\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.57it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.30it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.607700\n","Training log: 112 epoch (128 / 50000 train. data). Loss: 1.2522889375686646\n","Training log: 112 epoch (1408 / 50000 train. data). Loss: 1.3740596771240234\n","Training log: 112 epoch (2688 / 50000 train. data). Loss: 1.3260716199874878\n","Training log: 112 epoch (3968 / 50000 train. data). Loss: 1.0848803520202637\n","Training log: 112 epoch (5248 / 50000 train. data). Loss: 1.3322358131408691\n","Training log: 112 epoch (6528 / 50000 train. data). Loss: 1.238398790359497\n","Training log: 112 epoch (7808 / 50000 train. data). Loss: 1.295514702796936\n","Training log: 112 epoch (9088 / 50000 train. data). Loss: 1.1124728918075562\n","Training log: 112 epoch (10368 / 50000 train. data). Loss: 1.166581630706787\n","Training log: 112 epoch (11648 / 50000 train. data). Loss: 1.1468541622161865\n","Training log: 112 epoch (12928 / 50000 train. data). Loss: 1.1836334466934204\n","Training log: 112 epoch (14208 / 50000 train. data). Loss: 1.3237593173980713\n","Training log: 112 epoch (15488 / 50000 train. data). Loss: 1.169425129890442\n","Training log: 112 epoch (16768 / 50000 train. data). Loss: 1.2535994052886963\n","Training log: 112 epoch (18048 / 50000 train. data). Loss: 1.3095697164535522\n","Training log: 112 epoch (19328 / 50000 train. data). Loss: 1.3069226741790771\n","Training log: 112 epoch (20608 / 50000 train. data). Loss: 1.1289705038070679\n","Training log: 112 epoch (21888 / 50000 train. data). Loss: 1.2910643815994263\n","Training log: 112 epoch (23168 / 50000 train. data). Loss: 1.2468323707580566\n","Training log: 112 epoch (24448 / 50000 train. data). Loss: 1.232818603515625\n","Training log: 112 epoch (25728 / 50000 train. data). Loss: 1.338173270225525\n","Training log: 112 epoch (27008 / 50000 train. data). Loss: 1.1256780624389648\n","Training log: 112 epoch (28288 / 50000 train. data). Loss: 1.1524524688720703\n","Training log: 112 epoch (29568 / 50000 train. data). Loss: 1.2429183721542358\n","Training log: 112 epoch (30848 / 50000 train. data). Loss: 1.1650680303573608\n","Training log: 112 epoch (32128 / 50000 train. data). Loss: 1.259414792060852\n","Training log: 112 epoch (33408 / 50000 train. data). Loss: 1.2485829591751099\n","Training log: 112 epoch (34688 / 50000 train. data). Loss: 1.2493599653244019\n","Training log: 112 epoch (35968 / 50000 train. data). Loss: 1.4324086904525757\n","Training log: 112 epoch (37248 / 50000 train. data). Loss: 1.3785889148712158\n","Training log: 112 epoch (38528 / 50000 train. data). Loss: 1.4419342279434204\n","Training log: 112 epoch (39808 / 50000 train. data). Loss: 1.2684812545776367\n","Training log: 112 epoch (41088 / 50000 train. data). Loss: 1.2594482898712158\n","Training log: 112 epoch (42368 / 50000 train. data). Loss: 1.324128270149231\n","Training log: 112 epoch (43648 / 50000 train. data). Loss: 1.1515313386917114\n","Training log: 112 epoch (44928 / 50000 train. data). Loss: 1.3713288307189941\n","Training log: 112 epoch (46208 / 50000 train. data). Loss: 1.279823660850525\n","Training log: 112 epoch (47488 / 50000 train. data). Loss: 1.336449384689331\n","Training log: 112 epoch (48768 / 50000 train. data). Loss: 1.278853416442871\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 112 epoch (50048 / 50000 train. data). Loss: 1.2741541862487793\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 32.76it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.07it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.604800\n","Training log: 113 epoch (128 / 50000 train. data). Loss: 1.278194785118103\n","Training log: 113 epoch (1408 / 50000 train. data). Loss: 1.3069902658462524\n","Training log: 113 epoch (2688 / 50000 train. data). Loss: 1.1785798072814941\n","Training log: 113 epoch (3968 / 50000 train. data). Loss: 1.2336229085922241\n","Training log: 113 epoch (5248 / 50000 train. data). Loss: 1.2781474590301514\n","Training log: 113 epoch (6528 / 50000 train. data). Loss: 1.4171863794326782\n","Training log: 113 epoch (7808 / 50000 train. data). Loss: 1.2270547151565552\n","Training log: 113 epoch (9088 / 50000 train. data). Loss: 1.2367249727249146\n","Training log: 113 epoch (10368 / 50000 train. data). Loss: 1.4080302715301514\n","Training log: 113 epoch (11648 / 50000 train. data). Loss: 1.2110795974731445\n","Training log: 113 epoch (12928 / 50000 train. data). Loss: 1.4209257364273071\n","Training log: 113 epoch (14208 / 50000 train. data). Loss: 1.1636898517608643\n","Training log: 113 epoch (15488 / 50000 train. data). Loss: 1.395987868309021\n","Training log: 113 epoch (16768 / 50000 train. data). Loss: 1.1260478496551514\n","Training log: 113 epoch (18048 / 50000 train. data). Loss: 1.3615138530731201\n","Training log: 113 epoch (19328 / 50000 train. data). Loss: 1.3311166763305664\n","Training log: 113 epoch (20608 / 50000 train. data). Loss: 1.1898930072784424\n","Training log: 113 epoch (21888 / 50000 train. data). Loss: 1.3939658403396606\n","Training log: 113 epoch (23168 / 50000 train. data). Loss: 1.3276288509368896\n","Training log: 113 epoch (24448 / 50000 train. data). Loss: 1.2560491561889648\n","Training log: 113 epoch (25728 / 50000 train. data). Loss: 1.436338186264038\n","Training log: 113 epoch (27008 / 50000 train. data). Loss: 1.201640248298645\n","Training log: 113 epoch (28288 / 50000 train. data). Loss: 1.3105783462524414\n","Training log: 113 epoch (29568 / 50000 train. data). Loss: 1.2357946634292603\n","Training log: 113 epoch (30848 / 50000 train. data). Loss: 1.1170775890350342\n","Training log: 113 epoch (32128 / 50000 train. data). Loss: 1.5735774040222168\n","Training log: 113 epoch (33408 / 50000 train. data). Loss: 1.4001264572143555\n","Training log: 113 epoch (34688 / 50000 train. data). Loss: 1.2131563425064087\n","Training log: 113 epoch (35968 / 50000 train. data). Loss: 1.2873115539550781\n","Training log: 113 epoch (37248 / 50000 train. data). Loss: 1.3190683126449585\n","Training log: 113 epoch (38528 / 50000 train. data). Loss: 1.459886074066162\n","Training log: 113 epoch (39808 / 50000 train. data). Loss: 1.2471246719360352\n","Training log: 113 epoch (41088 / 50000 train. data). Loss: 1.3089308738708496\n","Training log: 113 epoch (42368 / 50000 train. data). Loss: 1.2259992361068726\n","Training log: 113 epoch (43648 / 50000 train. data). Loss: 1.1736582517623901\n","Training log: 113 epoch (44928 / 50000 train. data). Loss: 1.2524551153182983\n","Training log: 113 epoch (46208 / 50000 train. data). Loss: 1.3386820554733276\n","Training log: 113 epoch (47488 / 50000 train. data). Loss: 1.1127018928527832\n","Training log: 113 epoch (48768 / 50000 train. data). Loss: 1.4587801694869995\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.44it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 113 epoch (50048 / 50000 train. data). Loss: 1.3135473728179932\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.94it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.43it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.602600\n","Training log: 114 epoch (128 / 50000 train. data). Loss: 1.2409087419509888\n","Training log: 114 epoch (1408 / 50000 train. data). Loss: 1.3568580150604248\n","Training log: 114 epoch (2688 / 50000 train. data). Loss: 1.2330453395843506\n","Training log: 114 epoch (3968 / 50000 train. data). Loss: 1.3032721281051636\n","Training log: 114 epoch (5248 / 50000 train. data). Loss: 1.2466614246368408\n","Training log: 114 epoch (6528 / 50000 train. data). Loss: 1.3663954734802246\n","Training log: 114 epoch (7808 / 50000 train. data). Loss: 1.1239774227142334\n","Training log: 114 epoch (9088 / 50000 train. data). Loss: 1.3643062114715576\n","Training log: 114 epoch (10368 / 50000 train. data). Loss: 1.2777305841445923\n","Training log: 114 epoch (11648 / 50000 train. data). Loss: 1.1601969003677368\n","Training log: 114 epoch (12928 / 50000 train. data). Loss: 1.2843356132507324\n","Training log: 114 epoch (14208 / 50000 train. data). Loss: 1.1475285291671753\n","Training log: 114 epoch (15488 / 50000 train. data). Loss: 1.3684637546539307\n","Training log: 114 epoch (16768 / 50000 train. data). Loss: 1.1330935955047607\n","Training log: 114 epoch (18048 / 50000 train. data). Loss: 1.2543221712112427\n","Training log: 114 epoch (19328 / 50000 train. data). Loss: 1.2868313789367676\n","Training log: 114 epoch (20608 / 50000 train. data). Loss: 1.2334294319152832\n","Training log: 114 epoch (21888 / 50000 train. data). Loss: 1.2508654594421387\n","Training log: 114 epoch (23168 / 50000 train. data). Loss: 1.2909914255142212\n","Training log: 114 epoch (24448 / 50000 train. data). Loss: 1.3109593391418457\n","Training log: 114 epoch (25728 / 50000 train. data). Loss: 1.2531962394714355\n","Training log: 114 epoch (27008 / 50000 train. data). Loss: 1.285091757774353\n","Training log: 114 epoch (28288 / 50000 train. data). Loss: 1.3742727041244507\n","Training log: 114 epoch (29568 / 50000 train. data). Loss: 1.2150768041610718\n","Training log: 114 epoch (30848 / 50000 train. data). Loss: 1.302406668663025\n","Training log: 114 epoch (32128 / 50000 train. data). Loss: 1.3201249837875366\n","Training log: 114 epoch (33408 / 50000 train. data). Loss: 1.3584967851638794\n","Training log: 114 epoch (34688 / 50000 train. data). Loss: 1.234735131263733\n","Training log: 114 epoch (35968 / 50000 train. data). Loss: 1.1172808408737183\n","Training log: 114 epoch (37248 / 50000 train. data). Loss: 1.2120097875595093\n","Training log: 114 epoch (38528 / 50000 train. data). Loss: 1.1629241704940796\n","Training log: 114 epoch (39808 / 50000 train. data). Loss: 1.3717403411865234\n","Training log: 114 epoch (41088 / 50000 train. data). Loss: 1.2632274627685547\n","Training log: 114 epoch (42368 / 50000 train. data). Loss: 1.1641026735305786\n","Training log: 114 epoch (43648 / 50000 train. data). Loss: 1.1473721265792847\n","Training log: 114 epoch (44928 / 50000 train. data). Loss: 1.1674151420593262\n","Training log: 114 epoch (46208 / 50000 train. data). Loss: 1.2809665203094482\n","Training log: 114 epoch (47488 / 50000 train. data). Loss: 1.212876796722412\n","Training log: 114 epoch (48768 / 50000 train. data). Loss: 1.1970274448394775\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.72it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 114 epoch (50048 / 50000 train. data). Loss: 1.3587720394134521\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.87it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.33it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.605700\n","Training log: 115 epoch (128 / 50000 train. data). Loss: 1.297937273979187\n","Training log: 115 epoch (1408 / 50000 train. data). Loss: 1.27974271774292\n","Training log: 115 epoch (2688 / 50000 train. data). Loss: 1.199761152267456\n","Training log: 115 epoch (3968 / 50000 train. data). Loss: 1.3911864757537842\n","Training log: 115 epoch (5248 / 50000 train. data). Loss: 1.312537431716919\n","Training log: 115 epoch (6528 / 50000 train. data). Loss: 1.4030975103378296\n","Training log: 115 epoch (7808 / 50000 train. data). Loss: 1.0301319360733032\n","Training log: 115 epoch (9088 / 50000 train. data). Loss: 1.2310683727264404\n","Training log: 115 epoch (10368 / 50000 train. data). Loss: 1.2832298278808594\n","Training log: 115 epoch (11648 / 50000 train. data). Loss: 1.3795082569122314\n","Training log: 115 epoch (12928 / 50000 train. data). Loss: 1.2093628644943237\n","Training log: 115 epoch (14208 / 50000 train. data). Loss: 1.0567848682403564\n","Training log: 115 epoch (15488 / 50000 train. data). Loss: 1.1980998516082764\n","Training log: 115 epoch (16768 / 50000 train. data). Loss: 1.3662261962890625\n","Training log: 115 epoch (18048 / 50000 train. data). Loss: 1.1594212055206299\n","Training log: 115 epoch (19328 / 50000 train. data). Loss: 1.258784294128418\n","Training log: 115 epoch (20608 / 50000 train. data). Loss: 1.1891947984695435\n","Training log: 115 epoch (21888 / 50000 train. data). Loss: 1.3654091358184814\n","Training log: 115 epoch (23168 / 50000 train. data). Loss: 1.3878782987594604\n","Training log: 115 epoch (24448 / 50000 train. data). Loss: 1.454564094543457\n","Training log: 115 epoch (25728 / 50000 train. data). Loss: 1.2106863260269165\n","Training log: 115 epoch (27008 / 50000 train. data). Loss: 1.161263108253479\n","Training log: 115 epoch (28288 / 50000 train. data). Loss: 1.3164966106414795\n","Training log: 115 epoch (29568 / 50000 train. data). Loss: 1.1043132543563843\n","Training log: 115 epoch (30848 / 50000 train. data). Loss: 1.3008594512939453\n","Training log: 115 epoch (32128 / 50000 train. data). Loss: 1.2114474773406982\n","Training log: 115 epoch (33408 / 50000 train. data). Loss: 1.4578523635864258\n","Training log: 115 epoch (34688 / 50000 train. data). Loss: 1.3465226888656616\n","Training log: 115 epoch (35968 / 50000 train. data). Loss: 1.1986496448516846\n","Training log: 115 epoch (37248 / 50000 train. data). Loss: 1.3062487840652466\n","Training log: 115 epoch (38528 / 50000 train. data). Loss: 1.384621262550354\n","Training log: 115 epoch (39808 / 50000 train. data). Loss: 1.2087602615356445\n","Training log: 115 epoch (41088 / 50000 train. data). Loss: 1.1468112468719482\n","Training log: 115 epoch (42368 / 50000 train. data). Loss: 1.3484007120132446\n","Training log: 115 epoch (43648 / 50000 train. data). Loss: 1.3790360689163208\n","Training log: 115 epoch (44928 / 50000 train. data). Loss: 1.1156648397445679\n","Training log: 115 epoch (46208 / 50000 train. data). Loss: 1.3059684038162231\n","Training log: 115 epoch (47488 / 50000 train. data). Loss: 1.3364232778549194\n","Training log: 115 epoch (48768 / 50000 train. data). Loss: 1.1961945295333862\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 33.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 115 epoch (50048 / 50000 train. data). Loss: 1.4639180898666382\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.41it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.14it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.607400\n","Training log: 116 epoch (128 / 50000 train. data). Loss: 1.2754238843917847\n","Training log: 116 epoch (1408 / 50000 train. data). Loss: 1.1534615755081177\n","Training log: 116 epoch (2688 / 50000 train. data). Loss: 1.2364206314086914\n","Training log: 116 epoch (3968 / 50000 train. data). Loss: 1.4812734127044678\n","Training log: 116 epoch (5248 / 50000 train. data). Loss: 1.1561164855957031\n","Training log: 116 epoch (6528 / 50000 train. data). Loss: 1.3193824291229248\n","Training log: 116 epoch (7808 / 50000 train. data). Loss: 1.1526800394058228\n","Training log: 116 epoch (9088 / 50000 train. data). Loss: 1.3449511528015137\n","Training log: 116 epoch (10368 / 50000 train. data). Loss: 1.2229117155075073\n","Training log: 116 epoch (11648 / 50000 train. data). Loss: 1.4889016151428223\n","Training log: 116 epoch (12928 / 50000 train. data). Loss: 1.2156072854995728\n","Training log: 116 epoch (14208 / 50000 train. data). Loss: 1.2218196392059326\n","Training log: 116 epoch (15488 / 50000 train. data). Loss: 1.193483829498291\n","Training log: 116 epoch (16768 / 50000 train. data). Loss: 1.2132173776626587\n","Training log: 116 epoch (18048 / 50000 train. data). Loss: 1.3243353366851807\n","Training log: 116 epoch (19328 / 50000 train. data). Loss: 1.264483094215393\n","Training log: 116 epoch (20608 / 50000 train. data). Loss: 1.4278923273086548\n","Training log: 116 epoch (21888 / 50000 train. data). Loss: 1.234283208847046\n","Training log: 116 epoch (23168 / 50000 train. data). Loss: 1.2815238237380981\n","Training log: 116 epoch (24448 / 50000 train. data). Loss: 1.1721959114074707\n","Training log: 116 epoch (25728 / 50000 train. data). Loss: 1.3745548725128174\n","Training log: 116 epoch (27008 / 50000 train. data). Loss: 1.3953609466552734\n","Training log: 116 epoch (28288 / 50000 train. data). Loss: 1.300879955291748\n","Training log: 116 epoch (29568 / 50000 train. data). Loss: 1.1795843839645386\n","Training log: 116 epoch (30848 / 50000 train. data). Loss: 1.3634014129638672\n","Training log: 116 epoch (32128 / 50000 train. data). Loss: 1.399268388748169\n","Training log: 116 epoch (33408 / 50000 train. data). Loss: 1.060684323310852\n","Training log: 116 epoch (34688 / 50000 train. data). Loss: 1.142930269241333\n","Training log: 116 epoch (35968 / 50000 train. data). Loss: 1.2667381763458252\n","Training log: 116 epoch (37248 / 50000 train. data). Loss: 1.1729551553726196\n","Training log: 116 epoch (38528 / 50000 train. data). Loss: 1.3698002099990845\n","Training log: 116 epoch (39808 / 50000 train. data). Loss: 1.1617279052734375\n","Training log: 116 epoch (41088 / 50000 train. data). Loss: 1.1879243850708008\n","Training log: 116 epoch (42368 / 50000 train. data). Loss: 1.414986491203308\n","Training log: 116 epoch (43648 / 50000 train. data). Loss: 1.2515910863876343\n","Training log: 116 epoch (44928 / 50000 train. data). Loss: 1.330502986907959\n","Training log: 116 epoch (46208 / 50000 train. data). Loss: 1.2594938278198242\n","Training log: 116 epoch (47488 / 50000 train. data). Loss: 1.20416259765625\n","Training log: 116 epoch (48768 / 50000 train. data). Loss: 1.1261612176895142\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 116 epoch (50048 / 50000 train. data). Loss: 1.0947673320770264\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.91it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.46it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.602700\n","Training log: 117 epoch (128 / 50000 train. data). Loss: 1.2772104740142822\n","Training log: 117 epoch (1408 / 50000 train. data). Loss: 1.0429322719573975\n","Training log: 117 epoch (2688 / 50000 train. data). Loss: 1.3119616508483887\n","Training log: 117 epoch (3968 / 50000 train. data). Loss: 1.218110203742981\n","Training log: 117 epoch (5248 / 50000 train. data). Loss: 1.1893924474716187\n","Training log: 117 epoch (6528 / 50000 train. data). Loss: 1.3333443403244019\n","Training log: 117 epoch (7808 / 50000 train. data). Loss: 1.0751384496688843\n","Training log: 117 epoch (9088 / 50000 train. data). Loss: 1.329717755317688\n","Training log: 117 epoch (10368 / 50000 train. data). Loss: 1.3572742938995361\n","Training log: 117 epoch (11648 / 50000 train. data). Loss: 1.3312321901321411\n","Training log: 117 epoch (12928 / 50000 train. data). Loss: 1.2689553499221802\n","Training log: 117 epoch (14208 / 50000 train. data). Loss: 1.3573291301727295\n","Training log: 117 epoch (15488 / 50000 train. data). Loss: 1.296036720275879\n","Training log: 117 epoch (16768 / 50000 train. data). Loss: 1.1827175617218018\n","Training log: 117 epoch (18048 / 50000 train. data). Loss: 1.3076225519180298\n","Training log: 117 epoch (19328 / 50000 train. data). Loss: 1.3199408054351807\n","Training log: 117 epoch (20608 / 50000 train. data). Loss: 1.2811049222946167\n","Training log: 117 epoch (21888 / 50000 train. data). Loss: 1.206216812133789\n","Training log: 117 epoch (23168 / 50000 train. data). Loss: 1.143818736076355\n","Training log: 117 epoch (24448 / 50000 train. data). Loss: 1.2073218822479248\n","Training log: 117 epoch (25728 / 50000 train. data). Loss: 1.318860411643982\n","Training log: 117 epoch (27008 / 50000 train. data). Loss: 1.1363131999969482\n","Training log: 117 epoch (28288 / 50000 train. data). Loss: 1.408296823501587\n","Training log: 117 epoch (29568 / 50000 train. data). Loss: 1.3045579195022583\n","Training log: 117 epoch (30848 / 50000 train. data). Loss: 1.2752232551574707\n","Training log: 117 epoch (32128 / 50000 train. data). Loss: 1.1052824258804321\n","Training log: 117 epoch (33408 / 50000 train. data). Loss: 1.3855483531951904\n","Training log: 117 epoch (34688 / 50000 train. data). Loss: 1.3529903888702393\n","Training log: 117 epoch (35968 / 50000 train. data). Loss: 1.38612961769104\n","Training log: 117 epoch (37248 / 50000 train. data). Loss: 1.1967421770095825\n","Training log: 117 epoch (38528 / 50000 train. data). Loss: 1.1568107604980469\n","Training log: 117 epoch (39808 / 50000 train. data). Loss: 1.2819584608078003\n","Training log: 117 epoch (41088 / 50000 train. data). Loss: 1.3982014656066895\n","Training log: 117 epoch (42368 / 50000 train. data). Loss: 1.2167402505874634\n","Training log: 117 epoch (43648 / 50000 train. data). Loss: 1.3169879913330078\n","Training log: 117 epoch (44928 / 50000 train. data). Loss: 1.1431857347488403\n","Training log: 117 epoch (46208 / 50000 train. data). Loss: 1.4670031070709229\n","Training log: 117 epoch (47488 / 50000 train. data). Loss: 1.0894052982330322\n","Training log: 117 epoch (48768 / 50000 train. data). Loss: 1.2130444049835205\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.16it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 117 epoch (50048 / 50000 train. data). Loss: 1.1994882822036743\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.85it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.29it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.608300\n","Training log: 118 epoch (128 / 50000 train. data). Loss: 1.3021692037582397\n","Training log: 118 epoch (1408 / 50000 train. data). Loss: 1.4034143686294556\n","Training log: 118 epoch (2688 / 50000 train. data). Loss: 1.2272388935089111\n","Training log: 118 epoch (3968 / 50000 train. data). Loss: 1.1151548624038696\n","Training log: 118 epoch (5248 / 50000 train. data). Loss: 1.1931148767471313\n","Training log: 118 epoch (6528 / 50000 train. data). Loss: 1.2662005424499512\n","Training log: 118 epoch (7808 / 50000 train. data). Loss: 1.258100152015686\n","Training log: 118 epoch (9088 / 50000 train. data). Loss: 1.174165964126587\n","Training log: 118 epoch (10368 / 50000 train. data). Loss: 1.2427709102630615\n","Training log: 118 epoch (11648 / 50000 train. data). Loss: 1.292161464691162\n","Training log: 118 epoch (12928 / 50000 train. data). Loss: 1.3173819780349731\n","Training log: 118 epoch (14208 / 50000 train. data). Loss: 1.3340046405792236\n","Training log: 118 epoch (15488 / 50000 train. data). Loss: 1.35126531124115\n","Training log: 118 epoch (16768 / 50000 train. data). Loss: 1.3442744016647339\n","Training log: 118 epoch (18048 / 50000 train. data). Loss: 1.1174837350845337\n","Training log: 118 epoch (19328 / 50000 train. data). Loss: 1.3040294647216797\n","Training log: 118 epoch (20608 / 50000 train. data). Loss: 1.2442280054092407\n","Training log: 118 epoch (21888 / 50000 train. data). Loss: 1.3797357082366943\n","Training log: 118 epoch (23168 / 50000 train. data). Loss: 1.1509016752243042\n","Training log: 118 epoch (24448 / 50000 train. data). Loss: 1.1817858219146729\n","Training log: 118 epoch (25728 / 50000 train. data). Loss: 1.3525735139846802\n","Training log: 118 epoch (27008 / 50000 train. data). Loss: 1.2141602039337158\n","Training log: 118 epoch (28288 / 50000 train. data). Loss: 1.209064245223999\n","Training log: 118 epoch (29568 / 50000 train. data). Loss: 1.273100733757019\n","Training log: 118 epoch (30848 / 50000 train. data). Loss: 1.2953405380249023\n","Training log: 118 epoch (32128 / 50000 train. data). Loss: 1.2748873233795166\n","Training log: 118 epoch (33408 / 50000 train. data). Loss: 1.336426019668579\n","Training log: 118 epoch (34688 / 50000 train. data). Loss: 1.2508882284164429\n","Training log: 118 epoch (35968 / 50000 train. data). Loss: 1.0886985063552856\n","Training log: 118 epoch (37248 / 50000 train. data). Loss: 1.2278316020965576\n","Training log: 118 epoch (38528 / 50000 train. data). Loss: 1.292558193206787\n","Training log: 118 epoch (39808 / 50000 train. data). Loss: 1.4043349027633667\n","Training log: 118 epoch (41088 / 50000 train. data). Loss: 1.149396538734436\n","Training log: 118 epoch (42368 / 50000 train. data). Loss: 1.1974977254867554\n","Training log: 118 epoch (43648 / 50000 train. data). Loss: 1.2658944129943848\n","Training log: 118 epoch (44928 / 50000 train. data). Loss: 1.2883636951446533\n","Training log: 118 epoch (46208 / 50000 train. data). Loss: 1.3916358947753906\n","Training log: 118 epoch (47488 / 50000 train. data). Loss: 1.033056378364563\n","Training log: 118 epoch (48768 / 50000 train. data). Loss: 1.3133543729782104\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 118 epoch (50048 / 50000 train. data). Loss: 1.4046227931976318\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.66it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.54it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.610100\n","Training log: 119 epoch (128 / 50000 train. data). Loss: 1.2717761993408203\n","Training log: 119 epoch (1408 / 50000 train. data). Loss: 1.385054588317871\n","Training log: 119 epoch (2688 / 50000 train. data). Loss: 1.4738203287124634\n","Training log: 119 epoch (3968 / 50000 train. data). Loss: 1.3399763107299805\n","Training log: 119 epoch (5248 / 50000 train. data). Loss: 1.1528160572052002\n","Training log: 119 epoch (6528 / 50000 train. data). Loss: 1.2568538188934326\n","Training log: 119 epoch (7808 / 50000 train. data). Loss: 1.0978513956069946\n","Training log: 119 epoch (9088 / 50000 train. data). Loss: 1.2004531621932983\n","Training log: 119 epoch (10368 / 50000 train. data). Loss: 1.3623236417770386\n","Training log: 119 epoch (11648 / 50000 train. data). Loss: 1.3288809061050415\n","Training log: 119 epoch (12928 / 50000 train. data). Loss: 1.336289882659912\n","Training log: 119 epoch (14208 / 50000 train. data). Loss: 1.1515746116638184\n","Training log: 119 epoch (15488 / 50000 train. data). Loss: 1.2389976978302002\n","Training log: 119 epoch (16768 / 50000 train. data). Loss: 1.4279650449752808\n","Training log: 119 epoch (18048 / 50000 train. data). Loss: 1.2356984615325928\n","Training log: 119 epoch (19328 / 50000 train. data). Loss: 1.2128474712371826\n","Training log: 119 epoch (20608 / 50000 train. data). Loss: 1.2237489223480225\n","Training log: 119 epoch (21888 / 50000 train. data). Loss: 1.137825846672058\n","Training log: 119 epoch (23168 / 50000 train. data). Loss: 1.1909550428390503\n","Training log: 119 epoch (24448 / 50000 train. data). Loss: 1.421635389328003\n","Training log: 119 epoch (25728 / 50000 train. data). Loss: 1.2861517667770386\n","Training log: 119 epoch (27008 / 50000 train. data). Loss: 1.1352248191833496\n","Training log: 119 epoch (28288 / 50000 train. data). Loss: 1.4037667512893677\n","Training log: 119 epoch (29568 / 50000 train. data). Loss: 1.2493062019348145\n","Training log: 119 epoch (30848 / 50000 train. data). Loss: 1.217490553855896\n","Training log: 119 epoch (32128 / 50000 train. data). Loss: 1.2393391132354736\n","Training log: 119 epoch (33408 / 50000 train. data). Loss: 1.3437159061431885\n","Training log: 119 epoch (34688 / 50000 train. data). Loss: 1.3231101036071777\n","Training log: 119 epoch (35968 / 50000 train. data). Loss: 1.184509038925171\n","Training log: 119 epoch (37248 / 50000 train. data). Loss: 1.1390018463134766\n","Training log: 119 epoch (38528 / 50000 train. data). Loss: 1.366836667060852\n","Training log: 119 epoch (39808 / 50000 train. data). Loss: 1.2368884086608887\n","Training log: 119 epoch (41088 / 50000 train. data). Loss: 1.2160699367523193\n","Training log: 119 epoch (42368 / 50000 train. data). Loss: 1.2044670581817627\n","Training log: 119 epoch (43648 / 50000 train. data). Loss: 1.2789384126663208\n","Training log: 119 epoch (44928 / 50000 train. data). Loss: 1.2284854650497437\n","Training log: 119 epoch (46208 / 50000 train. data). Loss: 1.3775120973587036\n","Training log: 119 epoch (47488 / 50000 train. data). Loss: 1.3019599914550781\n","Training log: 119 epoch (48768 / 50000 train. data). Loss: 1.1487118005752563\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 119 epoch (50048 / 50000 train. data). Loss: 1.4691133499145508\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.46it/s]\n","100%|██████████| 79/79 [00:02<00:00, 31.29it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.603700\n","Training log: 120 epoch (128 / 50000 train. data). Loss: 1.2669832706451416\n","Training log: 120 epoch (1408 / 50000 train. data). Loss: 1.2409298419952393\n","Training log: 120 epoch (2688 / 50000 train. data). Loss: 1.1888606548309326\n","Training log: 120 epoch (3968 / 50000 train. data). Loss: 1.2835979461669922\n","Training log: 120 epoch (5248 / 50000 train. data). Loss: 1.3460503816604614\n","Training log: 120 epoch (6528 / 50000 train. data). Loss: 1.275664210319519\n","Training log: 120 epoch (7808 / 50000 train. data). Loss: 1.3019213676452637\n","Training log: 120 epoch (9088 / 50000 train. data). Loss: 1.2709776163101196\n","Training log: 120 epoch (10368 / 50000 train. data). Loss: 1.2873280048370361\n","Training log: 120 epoch (11648 / 50000 train. data). Loss: 1.234861969947815\n","Training log: 120 epoch (12928 / 50000 train. data). Loss: 1.3261659145355225\n","Training log: 120 epoch (14208 / 50000 train. data). Loss: 1.3478496074676514\n","Training log: 120 epoch (15488 / 50000 train. data). Loss: 1.2746903896331787\n","Training log: 120 epoch (16768 / 50000 train. data). Loss: 1.3889259099960327\n","Training log: 120 epoch (18048 / 50000 train. data). Loss: 1.2008020877838135\n","Training log: 120 epoch (19328 / 50000 train. data). Loss: 1.273856520652771\n","Training log: 120 epoch (20608 / 50000 train. data). Loss: 1.1540002822875977\n","Training log: 120 epoch (21888 / 50000 train. data). Loss: 1.1054283380508423\n","Training log: 120 epoch (23168 / 50000 train. data). Loss: 1.2111244201660156\n","Training log: 120 epoch (24448 / 50000 train. data). Loss: 1.4790782928466797\n","Training log: 120 epoch (25728 / 50000 train. data). Loss: 1.1741186380386353\n","Training log: 120 epoch (27008 / 50000 train. data). Loss: 1.187177062034607\n","Training log: 120 epoch (28288 / 50000 train. data). Loss: 1.2088637351989746\n","Training log: 120 epoch (29568 / 50000 train. data). Loss: 1.2277978658676147\n","Training log: 120 epoch (30848 / 50000 train. data). Loss: 1.134437918663025\n","Training log: 120 epoch (32128 / 50000 train. data). Loss: 1.1577610969543457\n","Training log: 120 epoch (33408 / 50000 train. data). Loss: 1.262575387954712\n","Training log: 120 epoch (34688 / 50000 train. data). Loss: 1.252763032913208\n","Training log: 120 epoch (35968 / 50000 train. data). Loss: 1.1068795919418335\n","Training log: 120 epoch (37248 / 50000 train. data). Loss: 1.2576956748962402\n","Training log: 120 epoch (38528 / 50000 train. data). Loss: 1.4041608572006226\n","Training log: 120 epoch (39808 / 50000 train. data). Loss: 0.9599708318710327\n","Training log: 120 epoch (41088 / 50000 train. data). Loss: 1.1160426139831543\n","Training log: 120 epoch (42368 / 50000 train. data). Loss: 1.321021556854248\n","Training log: 120 epoch (43648 / 50000 train. data). Loss: 1.2626005411148071\n","Training log: 120 epoch (44928 / 50000 train. data). Loss: 1.227760910987854\n","Training log: 120 epoch (46208 / 50000 train. data). Loss: 1.3819173574447632\n","Training log: 120 epoch (47488 / 50000 train. data). Loss: 1.437462568283081\n","Training log: 120 epoch (48768 / 50000 train. data). Loss: 1.3859825134277344\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:12, 31.34it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 120 epoch (50048 / 50000 train. data). Loss: 1.3380800485610962\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.01it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.45it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.612400\n","Training log: 121 epoch (128 / 50000 train. data). Loss: 1.244215965270996\n","Training log: 121 epoch (1408 / 50000 train. data). Loss: 1.0497339963912964\n","Training log: 121 epoch (2688 / 50000 train. data). Loss: 1.1256574392318726\n","Training log: 121 epoch (3968 / 50000 train. data). Loss: 1.2260721921920776\n","Training log: 121 epoch (5248 / 50000 train. data). Loss: 1.2833268642425537\n","Training log: 121 epoch (6528 / 50000 train. data). Loss: 1.2217055559158325\n","Training log: 121 epoch (7808 / 50000 train. data). Loss: 1.1477954387664795\n","Training log: 121 epoch (9088 / 50000 train. data). Loss: 1.2706462144851685\n","Training log: 121 epoch (10368 / 50000 train. data). Loss: 1.3547852039337158\n","Training log: 121 epoch (11648 / 50000 train. data). Loss: 1.1668843030929565\n","Training log: 121 epoch (12928 / 50000 train. data). Loss: 1.2752997875213623\n","Training log: 121 epoch (14208 / 50000 train. data). Loss: 1.374873399734497\n","Training log: 121 epoch (15488 / 50000 train. data). Loss: 1.2975122928619385\n","Training log: 121 epoch (16768 / 50000 train. data). Loss: 1.3209093809127808\n","Training log: 121 epoch (18048 / 50000 train. data). Loss: 1.2422314882278442\n","Training log: 121 epoch (19328 / 50000 train. data). Loss: 1.3609038591384888\n","Training log: 121 epoch (20608 / 50000 train. data). Loss: 1.3628429174423218\n","Training log: 121 epoch (21888 / 50000 train. data). Loss: 1.1161249876022339\n","Training log: 121 epoch (23168 / 50000 train. data). Loss: 1.162710189819336\n","Training log: 121 epoch (24448 / 50000 train. data). Loss: 1.2094781398773193\n","Training log: 121 epoch (25728 / 50000 train. data). Loss: 1.1413556337356567\n","Training log: 121 epoch (27008 / 50000 train. data). Loss: 1.1958799362182617\n","Training log: 121 epoch (28288 / 50000 train. data). Loss: 1.4212779998779297\n","Training log: 121 epoch (29568 / 50000 train. data). Loss: 1.1499552726745605\n","Training log: 121 epoch (30848 / 50000 train. data). Loss: 1.3331807851791382\n","Training log: 121 epoch (32128 / 50000 train. data). Loss: 1.2322622537612915\n","Training log: 121 epoch (33408 / 50000 train. data). Loss: 1.2589396238327026\n","Training log: 121 epoch (34688 / 50000 train. data). Loss: 1.1493810415267944\n","Training log: 121 epoch (35968 / 50000 train. data). Loss: 1.1695616245269775\n","Training log: 121 epoch (37248 / 50000 train. data). Loss: 1.1708379983901978\n","Training log: 121 epoch (38528 / 50000 train. data). Loss: 1.360369086265564\n","Training log: 121 epoch (39808 / 50000 train. data). Loss: 1.2913471460342407\n","Training log: 121 epoch (41088 / 50000 train. data). Loss: 1.25017249584198\n","Training log: 121 epoch (42368 / 50000 train. data). Loss: 1.2344837188720703\n","Training log: 121 epoch (43648 / 50000 train. data). Loss: 1.2335326671600342\n","Training log: 121 epoch (44928 / 50000 train. data). Loss: 1.2848349809646606\n","Training log: 121 epoch (46208 / 50000 train. data). Loss: 1.2612107992172241\n","Training log: 121 epoch (47488 / 50000 train. data). Loss: 1.2043966054916382\n","Training log: 121 epoch (48768 / 50000 train. data). Loss: 1.2573882341384888\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 3/391 [00:00<00:13, 29.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 121 epoch (50048 / 50000 train. data). Loss: 1.1816216707229614\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.20it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.28it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.608300\n","Training log: 122 epoch (128 / 50000 train. data). Loss: 1.1292226314544678\n","Training log: 122 epoch (1408 / 50000 train. data). Loss: 1.3859930038452148\n","Training log: 122 epoch (2688 / 50000 train. data). Loss: 1.1765397787094116\n","Training log: 122 epoch (3968 / 50000 train. data). Loss: 1.2859750986099243\n","Training log: 122 epoch (5248 / 50000 train. data). Loss: 1.3291109800338745\n","Training log: 122 epoch (6528 / 50000 train. data). Loss: 1.2293829917907715\n","Training log: 122 epoch (7808 / 50000 train. data). Loss: 1.2030048370361328\n","Training log: 122 epoch (9088 / 50000 train. data). Loss: 1.2905410528182983\n","Training log: 122 epoch (10368 / 50000 train. data). Loss: 1.236501932144165\n","Training log: 122 epoch (11648 / 50000 train. data). Loss: 1.251412034034729\n","Training log: 122 epoch (12928 / 50000 train. data). Loss: 1.3504970073699951\n","Training log: 122 epoch (14208 / 50000 train. data). Loss: 1.2792348861694336\n","Training log: 122 epoch (15488 / 50000 train. data). Loss: 1.26800537109375\n","Training log: 122 epoch (16768 / 50000 train. data). Loss: 1.2880618572235107\n","Training log: 122 epoch (18048 / 50000 train. data). Loss: 1.1560237407684326\n","Training log: 122 epoch (19328 / 50000 train. data). Loss: 1.2174993753433228\n","Training log: 122 epoch (20608 / 50000 train. data). Loss: 1.0731483697891235\n","Training log: 122 epoch (21888 / 50000 train. data). Loss: 1.2435075044631958\n","Training log: 122 epoch (23168 / 50000 train. data). Loss: 1.2787585258483887\n","Training log: 122 epoch (24448 / 50000 train. data). Loss: 1.267460584640503\n","Training log: 122 epoch (25728 / 50000 train. data). Loss: 1.2553268671035767\n","Training log: 122 epoch (27008 / 50000 train. data). Loss: 1.2047001123428345\n","Training log: 122 epoch (28288 / 50000 train. data). Loss: 1.1495344638824463\n","Training log: 122 epoch (29568 / 50000 train. data). Loss: 1.2626034021377563\n","Training log: 122 epoch (30848 / 50000 train. data). Loss: 1.266708493232727\n","Training log: 122 epoch (32128 / 50000 train. data). Loss: 1.193268060684204\n","Training log: 122 epoch (33408 / 50000 train. data). Loss: 1.2611807584762573\n","Training log: 122 epoch (34688 / 50000 train. data). Loss: 1.2649562358856201\n","Training log: 122 epoch (35968 / 50000 train. data). Loss: 1.345744013786316\n","Training log: 122 epoch (37248 / 50000 train. data). Loss: 1.089228630065918\n","Training log: 122 epoch (38528 / 50000 train. data). Loss: 1.2034703493118286\n","Training log: 122 epoch (39808 / 50000 train. data). Loss: 1.1270731687545776\n","Training log: 122 epoch (41088 / 50000 train. data). Loss: 1.1666029691696167\n","Training log: 122 epoch (42368 / 50000 train. data). Loss: 1.1208839416503906\n","Training log: 122 epoch (43648 / 50000 train. data). Loss: 1.174638032913208\n","Training log: 122 epoch (44928 / 50000 train. data). Loss: 1.2924824953079224\n","Training log: 122 epoch (46208 / 50000 train. data). Loss: 1.0883046388626099\n","Training log: 122 epoch (47488 / 50000 train. data). Loss: 1.1995114088058472\n","Training log: 122 epoch (48768 / 50000 train. data). Loss: 1.183808445930481\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.20it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 122 epoch (50048 / 50000 train. data). Loss: 1.171250581741333\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.07it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.41it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.609500\n","Training log: 123 epoch (128 / 50000 train. data). Loss: 1.2673640251159668\n","Training log: 123 epoch (1408 / 50000 train. data). Loss: 1.3081210851669312\n","Training log: 123 epoch (2688 / 50000 train. data). Loss: 1.2888309955596924\n","Training log: 123 epoch (3968 / 50000 train. data). Loss: 1.0248627662658691\n","Training log: 123 epoch (5248 / 50000 train. data). Loss: 1.116093397140503\n","Training log: 123 epoch (6528 / 50000 train. data). Loss: 1.1198985576629639\n","Training log: 123 epoch (7808 / 50000 train. data). Loss: 1.3997896909713745\n","Training log: 123 epoch (9088 / 50000 train. data). Loss: 1.3245725631713867\n","Training log: 123 epoch (10368 / 50000 train. data). Loss: 1.037588357925415\n","Training log: 123 epoch (11648 / 50000 train. data). Loss: 1.3577321767807007\n","Training log: 123 epoch (12928 / 50000 train. data). Loss: 1.1469272375106812\n","Training log: 123 epoch (14208 / 50000 train. data). Loss: 1.299469232559204\n","Training log: 123 epoch (15488 / 50000 train. data). Loss: 1.0046250820159912\n","Training log: 123 epoch (16768 / 50000 train. data). Loss: 1.2823694944381714\n","Training log: 123 epoch (18048 / 50000 train. data). Loss: 1.249070405960083\n","Training log: 123 epoch (19328 / 50000 train. data). Loss: 1.255863070487976\n","Training log: 123 epoch (20608 / 50000 train. data). Loss: 1.3644696474075317\n","Training log: 123 epoch (21888 / 50000 train. data). Loss: 1.3083181381225586\n","Training log: 123 epoch (23168 / 50000 train. data). Loss: 1.2480528354644775\n","Training log: 123 epoch (24448 / 50000 train. data). Loss: 1.2608901262283325\n","Training log: 123 epoch (25728 / 50000 train. data). Loss: 1.2114598751068115\n","Training log: 123 epoch (27008 / 50000 train. data). Loss: 1.3257758617401123\n","Training log: 123 epoch (28288 / 50000 train. data). Loss: 1.2661288976669312\n","Training log: 123 epoch (29568 / 50000 train. data). Loss: 1.2599482536315918\n","Training log: 123 epoch (30848 / 50000 train. data). Loss: 1.2891428470611572\n","Training log: 123 epoch (32128 / 50000 train. data). Loss: 1.1452077627182007\n","Training log: 123 epoch (33408 / 50000 train. data). Loss: 1.3900970220565796\n","Training log: 123 epoch (34688 / 50000 train. data). Loss: 1.2575950622558594\n","Training log: 123 epoch (35968 / 50000 train. data). Loss: 1.3596171140670776\n","Training log: 123 epoch (37248 / 50000 train. data). Loss: 1.220630168914795\n","Training log: 123 epoch (38528 / 50000 train. data). Loss: 1.110714077949524\n","Training log: 123 epoch (39808 / 50000 train. data). Loss: 1.303912878036499\n","Training log: 123 epoch (41088 / 50000 train. data). Loss: 1.2729936838150024\n","Training log: 123 epoch (42368 / 50000 train. data). Loss: 1.15647554397583\n","Training log: 123 epoch (43648 / 50000 train. data). Loss: 1.2713885307312012\n","Training log: 123 epoch (44928 / 50000 train. data). Loss: 1.1630266904830933\n","Training log: 123 epoch (46208 / 50000 train. data). Loss: 1.1552064418792725\n","Training log: 123 epoch (47488 / 50000 train. data). Loss: 1.2567659616470337\n","Training log: 123 epoch (48768 / 50000 train. data). Loss: 1.4996247291564941\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 123 epoch (50048 / 50000 train. data). Loss: 1.3693279027938843\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.31it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.71it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.607200\n","Training log: 124 epoch (128 / 50000 train. data). Loss: 1.2429786920547485\n","Training log: 124 epoch (1408 / 50000 train. data). Loss: 1.2035984992980957\n","Training log: 124 epoch (2688 / 50000 train. data). Loss: 1.265803575515747\n","Training log: 124 epoch (3968 / 50000 train. data). Loss: 1.0614780187606812\n","Training log: 124 epoch (5248 / 50000 train. data). Loss: 1.3465057611465454\n","Training log: 124 epoch (6528 / 50000 train. data). Loss: 1.1600568294525146\n","Training log: 124 epoch (7808 / 50000 train. data). Loss: 1.2320863008499146\n","Training log: 124 epoch (9088 / 50000 train. data). Loss: 1.3020776510238647\n","Training log: 124 epoch (10368 / 50000 train. data). Loss: 1.2765605449676514\n","Training log: 124 epoch (11648 / 50000 train. data). Loss: 1.284923791885376\n","Training log: 124 epoch (12928 / 50000 train. data). Loss: 1.2988182306289673\n","Training log: 124 epoch (14208 / 50000 train. data). Loss: 1.1852543354034424\n","Training log: 124 epoch (15488 / 50000 train. data). Loss: 1.3761903047561646\n","Training log: 124 epoch (16768 / 50000 train. data). Loss: 1.25034761428833\n","Training log: 124 epoch (18048 / 50000 train. data). Loss: 1.3895289897918701\n","Training log: 124 epoch (19328 / 50000 train. data). Loss: 1.29793381690979\n","Training log: 124 epoch (20608 / 50000 train. data). Loss: 1.3043543100357056\n","Training log: 124 epoch (21888 / 50000 train. data). Loss: 1.2602136135101318\n","Training log: 124 epoch (23168 / 50000 train. data). Loss: 1.287062406539917\n","Training log: 124 epoch (24448 / 50000 train. data). Loss: 1.1897175312042236\n","Training log: 124 epoch (25728 / 50000 train. data). Loss: 1.2690895795822144\n","Training log: 124 epoch (27008 / 50000 train. data). Loss: 1.1538220643997192\n","Training log: 124 epoch (28288 / 50000 train. data). Loss: 1.374729037284851\n","Training log: 124 epoch (29568 / 50000 train. data). Loss: 1.1794233322143555\n","Training log: 124 epoch (30848 / 50000 train. data). Loss: 1.0382771492004395\n","Training log: 124 epoch (32128 / 50000 train. data). Loss: 1.1268421411514282\n","Training log: 124 epoch (33408 / 50000 train. data). Loss: 1.253093957901001\n","Training log: 124 epoch (34688 / 50000 train. data). Loss: 1.2517043352127075\n","Training log: 124 epoch (35968 / 50000 train. data). Loss: 1.1707895994186401\n","Training log: 124 epoch (37248 / 50000 train. data). Loss: 1.2621666193008423\n","Training log: 124 epoch (38528 / 50000 train. data). Loss: 1.2255690097808838\n","Training log: 124 epoch (39808 / 50000 train. data). Loss: 1.1628484725952148\n","Training log: 124 epoch (41088 / 50000 train. data). Loss: 1.3570774793624878\n","Training log: 124 epoch (42368 / 50000 train. data). Loss: 1.1980884075164795\n","Training log: 124 epoch (43648 / 50000 train. data). Loss: 1.2325389385223389\n","Training log: 124 epoch (44928 / 50000 train. data). Loss: 1.3580821752548218\n","Training log: 124 epoch (46208 / 50000 train. data). Loss: 1.4338576793670654\n","Training log: 124 epoch (47488 / 50000 train. data). Loss: 1.2296247482299805\n","Training log: 124 epoch (48768 / 50000 train. data). Loss: 1.242567539215088\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.73it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 124 epoch (50048 / 50000 train. data). Loss: 0.9396427273750305\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.91it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.99it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.610400\n","Training log: 125 epoch (128 / 50000 train. data). Loss: 1.2755672931671143\n","Training log: 125 epoch (1408 / 50000 train. data). Loss: 1.3588764667510986\n","Training log: 125 epoch (2688 / 50000 train. data). Loss: 1.189908742904663\n","Training log: 125 epoch (3968 / 50000 train. data). Loss: 1.1943371295928955\n","Training log: 125 epoch (5248 / 50000 train. data). Loss: 1.1438031196594238\n","Training log: 125 epoch (6528 / 50000 train. data). Loss: 1.303399682044983\n","Training log: 125 epoch (7808 / 50000 train. data). Loss: 1.323381781578064\n","Training log: 125 epoch (9088 / 50000 train. data). Loss: 1.2771836519241333\n","Training log: 125 epoch (10368 / 50000 train. data). Loss: 1.2044109106063843\n","Training log: 125 epoch (11648 / 50000 train. data). Loss: 1.2308459281921387\n","Training log: 125 epoch (12928 / 50000 train. data). Loss: 1.1979937553405762\n","Training log: 125 epoch (14208 / 50000 train. data). Loss: 1.2725708484649658\n","Training log: 125 epoch (15488 / 50000 train. data). Loss: 1.2273008823394775\n","Training log: 125 epoch (16768 / 50000 train. data). Loss: 1.4163857698440552\n","Training log: 125 epoch (18048 / 50000 train. data). Loss: 1.28184974193573\n","Training log: 125 epoch (19328 / 50000 train. data). Loss: 1.4251071214675903\n","Training log: 125 epoch (20608 / 50000 train. data). Loss: 1.3083486557006836\n","Training log: 125 epoch (21888 / 50000 train. data). Loss: 1.1472300291061401\n","Training log: 125 epoch (23168 / 50000 train. data). Loss: 1.4631718397140503\n","Training log: 125 epoch (24448 / 50000 train. data). Loss: 1.2813531160354614\n","Training log: 125 epoch (25728 / 50000 train. data). Loss: 1.2480170726776123\n","Training log: 125 epoch (27008 / 50000 train. data). Loss: 1.252914547920227\n","Training log: 125 epoch (28288 / 50000 train. data). Loss: 1.4093925952911377\n","Training log: 125 epoch (29568 / 50000 train. data). Loss: 1.334808111190796\n","Training log: 125 epoch (30848 / 50000 train. data). Loss: 1.1418960094451904\n","Training log: 125 epoch (32128 / 50000 train. data). Loss: 1.2999235391616821\n","Training log: 125 epoch (33408 / 50000 train. data). Loss: 1.331365942955017\n","Training log: 125 epoch (34688 / 50000 train. data). Loss: 1.3009166717529297\n","Training log: 125 epoch (35968 / 50000 train. data). Loss: 1.2776540517807007\n","Training log: 125 epoch (37248 / 50000 train. data). Loss: 1.297194480895996\n","Training log: 125 epoch (38528 / 50000 train. data). Loss: 1.1588828563690186\n","Training log: 125 epoch (39808 / 50000 train. data). Loss: 1.1640756130218506\n","Training log: 125 epoch (41088 / 50000 train. data). Loss: 1.2337658405303955\n","Training log: 125 epoch (42368 / 50000 train. data). Loss: 1.4172269105911255\n","Training log: 125 epoch (43648 / 50000 train. data). Loss: 1.284770131111145\n","Training log: 125 epoch (44928 / 50000 train. data). Loss: 1.3017418384552002\n","Training log: 125 epoch (46208 / 50000 train. data). Loss: 1.2710691690444946\n","Training log: 125 epoch (47488 / 50000 train. data). Loss: 1.1781526803970337\n","Training log: 125 epoch (48768 / 50000 train. data). Loss: 1.188841700553894\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 3/391 [00:00<00:13, 29.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 125 epoch (50048 / 50000 train. data). Loss: 1.2577412128448486\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.45it/s]\n","100%|██████████| 79/79 [00:02<00:00, 37.11it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.614200\n","Training log: 126 epoch (128 / 50000 train. data). Loss: 1.2285332679748535\n","Training log: 126 epoch (1408 / 50000 train. data). Loss: 1.0221548080444336\n","Training log: 126 epoch (2688 / 50000 train. data). Loss: 1.2448331117630005\n","Training log: 126 epoch (3968 / 50000 train. data). Loss: 1.175865650177002\n","Training log: 126 epoch (5248 / 50000 train. data). Loss: 1.2304232120513916\n","Training log: 126 epoch (6528 / 50000 train. data). Loss: 1.2517988681793213\n","Training log: 126 epoch (7808 / 50000 train. data). Loss: 1.2053693532943726\n","Training log: 126 epoch (9088 / 50000 train. data). Loss: 1.2359880208969116\n","Training log: 126 epoch (10368 / 50000 train. data). Loss: 1.330496907234192\n","Training log: 126 epoch (11648 / 50000 train. data). Loss: 1.2237790822982788\n","Training log: 126 epoch (12928 / 50000 train. data). Loss: 1.1594483852386475\n","Training log: 126 epoch (14208 / 50000 train. data). Loss: 1.291886568069458\n","Training log: 126 epoch (15488 / 50000 train. data). Loss: 1.3519384860992432\n","Training log: 126 epoch (16768 / 50000 train. data). Loss: 1.2384002208709717\n","Training log: 126 epoch (18048 / 50000 train. data). Loss: 1.2467126846313477\n","Training log: 126 epoch (19328 / 50000 train. data). Loss: 1.3323407173156738\n","Training log: 126 epoch (20608 / 50000 train. data). Loss: 1.3529932498931885\n","Training log: 126 epoch (21888 / 50000 train. data). Loss: 1.1943072080612183\n","Training log: 126 epoch (23168 / 50000 train. data). Loss: 1.3533587455749512\n","Training log: 126 epoch (24448 / 50000 train. data). Loss: 1.2351436614990234\n","Training log: 126 epoch (25728 / 50000 train. data). Loss: 1.2913955450057983\n","Training log: 126 epoch (27008 / 50000 train. data). Loss: 1.2487049102783203\n","Training log: 126 epoch (28288 / 50000 train. data). Loss: 1.383297085762024\n","Training log: 126 epoch (29568 / 50000 train. data). Loss: 1.2687655687332153\n","Training log: 126 epoch (30848 / 50000 train. data). Loss: 1.2474278211593628\n","Training log: 126 epoch (32128 / 50000 train. data). Loss: 1.197696566581726\n","Training log: 126 epoch (33408 / 50000 train. data). Loss: 1.2209604978561401\n","Training log: 126 epoch (34688 / 50000 train. data). Loss: 1.2425202131271362\n","Training log: 126 epoch (35968 / 50000 train. data). Loss: 1.1785056591033936\n","Training log: 126 epoch (37248 / 50000 train. data). Loss: 1.10382878780365\n","Training log: 126 epoch (38528 / 50000 train. data). Loss: 1.3554513454437256\n","Training log: 126 epoch (39808 / 50000 train. data). Loss: 1.2297989130020142\n","Training log: 126 epoch (41088 / 50000 train. data). Loss: 1.2243238687515259\n","Training log: 126 epoch (42368 / 50000 train. data). Loss: 1.3268917798995972\n","Training log: 126 epoch (43648 / 50000 train. data). Loss: 1.1656910181045532\n","Training log: 126 epoch (44928 / 50000 train. data). Loss: 1.2680422067642212\n","Training log: 126 epoch (46208 / 50000 train. data). Loss: 1.2582528591156006\n","Training log: 126 epoch (47488 / 50000 train. data). Loss: 1.144254446029663\n","Training log: 126 epoch (48768 / 50000 train. data). Loss: 1.2167311906814575\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 126 epoch (50048 / 50000 train. data). Loss: 1.3288856744766235\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.27it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.64it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.612900\n","Training log: 127 epoch (128 / 50000 train. data). Loss: 1.2272400856018066\n","Training log: 127 epoch (1408 / 50000 train. data). Loss: 1.4457565546035767\n","Training log: 127 epoch (2688 / 50000 train. data). Loss: 1.0878961086273193\n","Training log: 127 epoch (3968 / 50000 train. data). Loss: 1.1906410455703735\n","Training log: 127 epoch (5248 / 50000 train. data). Loss: 1.2843196392059326\n","Training log: 127 epoch (6528 / 50000 train. data). Loss: 1.2780430316925049\n","Training log: 127 epoch (7808 / 50000 train. data). Loss: 1.381015419960022\n","Training log: 127 epoch (9088 / 50000 train. data). Loss: 1.3175592422485352\n","Training log: 127 epoch (10368 / 50000 train. data). Loss: 1.2326170206069946\n","Training log: 127 epoch (11648 / 50000 train. data). Loss: 1.2963343858718872\n","Training log: 127 epoch (12928 / 50000 train. data). Loss: 1.3646615743637085\n","Training log: 127 epoch (14208 / 50000 train. data). Loss: 1.2714062929153442\n","Training log: 127 epoch (15488 / 50000 train. data). Loss: 1.137482762336731\n","Training log: 127 epoch (16768 / 50000 train. data). Loss: 1.067635416984558\n","Training log: 127 epoch (18048 / 50000 train. data). Loss: 1.3093791007995605\n","Training log: 127 epoch (19328 / 50000 train. data). Loss: 1.4305737018585205\n","Training log: 127 epoch (20608 / 50000 train. data). Loss: 1.3942897319793701\n","Training log: 127 epoch (21888 / 50000 train. data). Loss: 1.1726423501968384\n","Training log: 127 epoch (23168 / 50000 train. data). Loss: 1.313310980796814\n","Training log: 127 epoch (24448 / 50000 train. data). Loss: 1.1696908473968506\n","Training log: 127 epoch (25728 / 50000 train. data). Loss: 1.2813664674758911\n","Training log: 127 epoch (27008 / 50000 train. data). Loss: 1.3540362119674683\n","Training log: 127 epoch (28288 / 50000 train. data). Loss: 1.2123887538909912\n","Training log: 127 epoch (29568 / 50000 train. data). Loss: 1.249892234802246\n","Training log: 127 epoch (30848 / 50000 train. data). Loss: 1.2442322969436646\n","Training log: 127 epoch (32128 / 50000 train. data). Loss: 1.1187489032745361\n","Training log: 127 epoch (33408 / 50000 train. data). Loss: 1.2158912420272827\n","Training log: 127 epoch (34688 / 50000 train. data). Loss: 1.165714979171753\n","Training log: 127 epoch (35968 / 50000 train. data). Loss: 1.2014405727386475\n","Training log: 127 epoch (37248 / 50000 train. data). Loss: 1.241618037223816\n","Training log: 127 epoch (38528 / 50000 train. data). Loss: 1.4184404611587524\n","Training log: 127 epoch (39808 / 50000 train. data). Loss: 1.2329057455062866\n","Training log: 127 epoch (41088 / 50000 train. data). Loss: 1.3364733457565308\n","Training log: 127 epoch (42368 / 50000 train. data). Loss: 1.3296012878417969\n","Training log: 127 epoch (43648 / 50000 train. data). Loss: 1.5029972791671753\n","Training log: 127 epoch (44928 / 50000 train. data). Loss: 1.119362235069275\n","Training log: 127 epoch (46208 / 50000 train. data). Loss: 1.2679431438446045\n","Training log: 127 epoch (47488 / 50000 train. data). Loss: 1.194899082183838\n","Training log: 127 epoch (48768 / 50000 train. data). Loss: 1.2276091575622559\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:12, 29.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 127 epoch (50048 / 50000 train. data). Loss: 1.3461427688598633\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.92it/s]\n","100%|██████████| 79/79 [00:02<00:00, 31.57it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.613600\n","Training log: 128 epoch (128 / 50000 train. data). Loss: 1.1213045120239258\n","Training log: 128 epoch (1408 / 50000 train. data). Loss: 1.3433712720870972\n","Training log: 128 epoch (2688 / 50000 train. data). Loss: 1.2595860958099365\n","Training log: 128 epoch (3968 / 50000 train. data). Loss: 1.2501816749572754\n","Training log: 128 epoch (5248 / 50000 train. data). Loss: 1.353899598121643\n","Training log: 128 epoch (6528 / 50000 train. data). Loss: 1.255113959312439\n","Training log: 128 epoch (7808 / 50000 train. data). Loss: 1.3160842657089233\n","Training log: 128 epoch (9088 / 50000 train. data). Loss: 1.149195671081543\n","Training log: 128 epoch (10368 / 50000 train. data). Loss: 1.2587330341339111\n","Training log: 128 epoch (11648 / 50000 train. data). Loss: 1.2332710027694702\n","Training log: 128 epoch (12928 / 50000 train. data). Loss: 1.2132501602172852\n","Training log: 128 epoch (14208 / 50000 train. data). Loss: 1.1031922101974487\n","Training log: 128 epoch (15488 / 50000 train. data). Loss: 1.1911101341247559\n","Training log: 128 epoch (16768 / 50000 train. data). Loss: 1.1131013631820679\n","Training log: 128 epoch (18048 / 50000 train. data). Loss: 1.4636735916137695\n","Training log: 128 epoch (19328 / 50000 train. data). Loss: 1.2624036073684692\n","Training log: 128 epoch (20608 / 50000 train. data). Loss: 1.30430006980896\n","Training log: 128 epoch (21888 / 50000 train. data). Loss: 1.138688564300537\n","Training log: 128 epoch (23168 / 50000 train. data). Loss: 1.222357153892517\n","Training log: 128 epoch (24448 / 50000 train. data). Loss: 1.3395119905471802\n","Training log: 128 epoch (25728 / 50000 train. data). Loss: 1.0495206117630005\n","Training log: 128 epoch (27008 / 50000 train. data). Loss: 1.1270649433135986\n","Training log: 128 epoch (28288 / 50000 train. data). Loss: 1.172600269317627\n","Training log: 128 epoch (29568 / 50000 train. data). Loss: 1.3345699310302734\n","Training log: 128 epoch (30848 / 50000 train. data). Loss: 1.388412356376648\n","Training log: 128 epoch (32128 / 50000 train. data). Loss: 1.3925106525421143\n","Training log: 128 epoch (33408 / 50000 train. data). Loss: 1.1921374797821045\n","Training log: 128 epoch (34688 / 50000 train. data). Loss: 1.2094485759735107\n","Training log: 128 epoch (35968 / 50000 train. data). Loss: 1.3164753913879395\n","Training log: 128 epoch (37248 / 50000 train. data). Loss: 1.12392258644104\n","Training log: 128 epoch (38528 / 50000 train. data). Loss: 1.2426090240478516\n","Training log: 128 epoch (39808 / 50000 train. data). Loss: 1.3776459693908691\n","Training log: 128 epoch (41088 / 50000 train. data). Loss: 1.2371008396148682\n","Training log: 128 epoch (42368 / 50000 train. data). Loss: 1.3195607662200928\n","Training log: 128 epoch (43648 / 50000 train. data). Loss: 1.076428771018982\n","Training log: 128 epoch (44928 / 50000 train. data). Loss: 1.0510718822479248\n","Training log: 128 epoch (46208 / 50000 train. data). Loss: 1.101649284362793\n","Training log: 128 epoch (47488 / 50000 train. data). Loss: 1.298534631729126\n","Training log: 128 epoch (48768 / 50000 train. data). Loss: 1.1977180242538452\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 38.38it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 128 epoch (50048 / 50000 train. data). Loss: 1.1776478290557861\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.24it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.19it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.610800\n","Training log: 129 epoch (128 / 50000 train. data). Loss: 1.176221489906311\n","Training log: 129 epoch (1408 / 50000 train. data). Loss: 1.200952410697937\n","Training log: 129 epoch (2688 / 50000 train. data). Loss: 1.1982150077819824\n","Training log: 129 epoch (3968 / 50000 train. data). Loss: 1.1484649181365967\n","Training log: 129 epoch (5248 / 50000 train. data). Loss: 1.3724511861801147\n","Training log: 129 epoch (6528 / 50000 train. data). Loss: 1.1115700006484985\n","Training log: 129 epoch (7808 / 50000 train. data). Loss: 1.1783311367034912\n","Training log: 129 epoch (9088 / 50000 train. data). Loss: 1.2112548351287842\n","Training log: 129 epoch (10368 / 50000 train. data). Loss: 1.2294256687164307\n","Training log: 129 epoch (11648 / 50000 train. data). Loss: 1.3113961219787598\n","Training log: 129 epoch (12928 / 50000 train. data). Loss: 1.3592479228973389\n","Training log: 129 epoch (14208 / 50000 train. data). Loss: 1.229876160621643\n","Training log: 129 epoch (15488 / 50000 train. data). Loss: 1.252539038658142\n","Training log: 129 epoch (16768 / 50000 train. data). Loss: 1.2251851558685303\n","Training log: 129 epoch (18048 / 50000 train. data). Loss: 1.1508489847183228\n","Training log: 129 epoch (19328 / 50000 train. data). Loss: 1.268603801727295\n","Training log: 129 epoch (20608 / 50000 train. data). Loss: 1.2596474885940552\n","Training log: 129 epoch (21888 / 50000 train. data). Loss: 1.0795012712478638\n","Training log: 129 epoch (23168 / 50000 train. data). Loss: 1.2798494100570679\n","Training log: 129 epoch (24448 / 50000 train. data). Loss: 1.454832673072815\n","Training log: 129 epoch (25728 / 50000 train. data). Loss: 1.168660044670105\n","Training log: 129 epoch (27008 / 50000 train. data). Loss: 1.2867504358291626\n","Training log: 129 epoch (28288 / 50000 train. data). Loss: 1.1819735765457153\n","Training log: 129 epoch (29568 / 50000 train. data). Loss: 1.1613178253173828\n","Training log: 129 epoch (30848 / 50000 train. data). Loss: 1.134914517402649\n","Training log: 129 epoch (32128 / 50000 train. data). Loss: 1.453137993812561\n","Training log: 129 epoch (33408 / 50000 train. data). Loss: 1.214938998222351\n","Training log: 129 epoch (34688 / 50000 train. data). Loss: 1.1641159057617188\n","Training log: 129 epoch (35968 / 50000 train. data). Loss: 1.0866551399230957\n","Training log: 129 epoch (37248 / 50000 train. data). Loss: 1.3661669492721558\n","Training log: 129 epoch (38528 / 50000 train. data). Loss: 1.245853304862976\n","Training log: 129 epoch (39808 / 50000 train. data). Loss: 1.441004991531372\n","Training log: 129 epoch (41088 / 50000 train. data). Loss: 1.0944398641586304\n","Training log: 129 epoch (42368 / 50000 train. data). Loss: 1.465922236442566\n","Training log: 129 epoch (43648 / 50000 train. data). Loss: 1.3290133476257324\n","Training log: 129 epoch (44928 / 50000 train. data). Loss: 1.1911091804504395\n","Training log: 129 epoch (46208 / 50000 train. data). Loss: 1.2884113788604736\n","Training log: 129 epoch (47488 / 50000 train. data). Loss: 1.1390300989151\n","Training log: 129 epoch (48768 / 50000 train. data). Loss: 1.0042039155960083\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 33.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 129 epoch (50048 / 50000 train. data). Loss: 1.3265857696533203\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.22it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.02it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.619600\n","Training log: 130 epoch (128 / 50000 train. data). Loss: 1.2817689180374146\n","Training log: 130 epoch (1408 / 50000 train. data). Loss: 1.2134946584701538\n","Training log: 130 epoch (2688 / 50000 train. data). Loss: 1.3395047187805176\n","Training log: 130 epoch (3968 / 50000 train. data). Loss: 1.1486823558807373\n","Training log: 130 epoch (5248 / 50000 train. data). Loss: 1.3896141052246094\n","Training log: 130 epoch (6528 / 50000 train. data). Loss: 1.2746381759643555\n","Training log: 130 epoch (7808 / 50000 train. data). Loss: 1.478380799293518\n","Training log: 130 epoch (9088 / 50000 train. data). Loss: 1.2617825269699097\n","Training log: 130 epoch (10368 / 50000 train. data). Loss: 1.3403005599975586\n","Training log: 130 epoch (11648 / 50000 train. data). Loss: 1.2454761266708374\n","Training log: 130 epoch (12928 / 50000 train. data). Loss: 1.1691181659698486\n","Training log: 130 epoch (14208 / 50000 train. data). Loss: 1.1847364902496338\n","Training log: 130 epoch (15488 / 50000 train. data). Loss: 1.4442358016967773\n","Training log: 130 epoch (16768 / 50000 train. data). Loss: 1.3929091691970825\n","Training log: 130 epoch (18048 / 50000 train. data). Loss: 1.2138164043426514\n","Training log: 130 epoch (19328 / 50000 train. data). Loss: 1.1768584251403809\n","Training log: 130 epoch (20608 / 50000 train. data). Loss: 1.2308727502822876\n","Training log: 130 epoch (21888 / 50000 train. data). Loss: 1.225389838218689\n","Training log: 130 epoch (23168 / 50000 train. data). Loss: 1.195084810256958\n","Training log: 130 epoch (24448 / 50000 train. data). Loss: 1.3237379789352417\n","Training log: 130 epoch (25728 / 50000 train. data). Loss: 1.295223593711853\n","Training log: 130 epoch (27008 / 50000 train. data). Loss: 1.3786215782165527\n","Training log: 130 epoch (28288 / 50000 train. data). Loss: 1.1813453435897827\n","Training log: 130 epoch (29568 / 50000 train. data). Loss: 1.0554900169372559\n","Training log: 130 epoch (30848 / 50000 train. data). Loss: 1.1552256345748901\n","Training log: 130 epoch (32128 / 50000 train. data). Loss: 1.3650383949279785\n","Training log: 130 epoch (33408 / 50000 train. data). Loss: 1.2759571075439453\n","Training log: 130 epoch (34688 / 50000 train. data). Loss: 1.2057299613952637\n","Training log: 130 epoch (35968 / 50000 train. data). Loss: 1.1540521383285522\n","Training log: 130 epoch (37248 / 50000 train. data). Loss: 1.3236408233642578\n","Training log: 130 epoch (38528 / 50000 train. data). Loss: 1.1391804218292236\n","Training log: 130 epoch (39808 / 50000 train. data). Loss: 1.4178946018218994\n","Training log: 130 epoch (41088 / 50000 train. data). Loss: 1.1679035425186157\n","Training log: 130 epoch (42368 / 50000 train. data). Loss: 1.328089952468872\n","Training log: 130 epoch (43648 / 50000 train. data). Loss: 1.342043161392212\n","Training log: 130 epoch (44928 / 50000 train. data). Loss: 1.235342264175415\n","Training log: 130 epoch (46208 / 50000 train. data). Loss: 1.1500824689865112\n","Training log: 130 epoch (47488 / 50000 train. data). Loss: 1.2144533395767212\n","Training log: 130 epoch (48768 / 50000 train. data). Loss: 1.1247259378433228\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.23it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 130 epoch (50048 / 50000 train. data). Loss: 1.3505923748016357\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.01it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.42it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.614300\n","Training log: 131 epoch (128 / 50000 train. data). Loss: 1.1520189046859741\n","Training log: 131 epoch (1408 / 50000 train. data). Loss: 1.2202187776565552\n","Training log: 131 epoch (2688 / 50000 train. data). Loss: 1.19467031955719\n","Training log: 131 epoch (3968 / 50000 train. data). Loss: 1.2572680711746216\n","Training log: 131 epoch (5248 / 50000 train. data). Loss: 1.4893572330474854\n","Training log: 131 epoch (6528 / 50000 train. data). Loss: 1.4081814289093018\n","Training log: 131 epoch (7808 / 50000 train. data). Loss: 1.31135094165802\n","Training log: 131 epoch (9088 / 50000 train. data). Loss: 1.2765229940414429\n","Training log: 131 epoch (10368 / 50000 train. data). Loss: 1.3011823892593384\n","Training log: 131 epoch (11648 / 50000 train. data). Loss: 1.158621072769165\n","Training log: 131 epoch (12928 / 50000 train. data). Loss: 1.1629105806350708\n","Training log: 131 epoch (14208 / 50000 train. data). Loss: 1.230950117111206\n","Training log: 131 epoch (15488 / 50000 train. data). Loss: 1.2385587692260742\n","Training log: 131 epoch (16768 / 50000 train. data). Loss: 1.213189959526062\n","Training log: 131 epoch (18048 / 50000 train. data). Loss: 1.4358036518096924\n","Training log: 131 epoch (19328 / 50000 train. data). Loss: 1.293005347251892\n","Training log: 131 epoch (20608 / 50000 train. data). Loss: 1.2755457162857056\n","Training log: 131 epoch (21888 / 50000 train. data). Loss: 1.2007317543029785\n","Training log: 131 epoch (23168 / 50000 train. data). Loss: 1.0047268867492676\n","Training log: 131 epoch (24448 / 50000 train. data). Loss: 1.284226894378662\n","Training log: 131 epoch (25728 / 50000 train. data). Loss: 1.2525148391723633\n","Training log: 131 epoch (27008 / 50000 train. data). Loss: 1.3386750221252441\n","Training log: 131 epoch (28288 / 50000 train. data). Loss: 1.2220150232315063\n","Training log: 131 epoch (29568 / 50000 train. data). Loss: 1.1860747337341309\n","Training log: 131 epoch (30848 / 50000 train. data). Loss: 1.326367974281311\n","Training log: 131 epoch (32128 / 50000 train. data). Loss: 1.1444844007492065\n","Training log: 131 epoch (33408 / 50000 train. data). Loss: 1.164721965789795\n","Training log: 131 epoch (34688 / 50000 train. data). Loss: 1.1329553127288818\n","Training log: 131 epoch (35968 / 50000 train. data). Loss: 1.2930238246917725\n","Training log: 131 epoch (37248 / 50000 train. data). Loss: 1.2035658359527588\n","Training log: 131 epoch (38528 / 50000 train. data). Loss: 1.1257743835449219\n","Training log: 131 epoch (39808 / 50000 train. data). Loss: 1.2675719261169434\n","Training log: 131 epoch (41088 / 50000 train. data). Loss: 1.2544206380844116\n","Training log: 131 epoch (42368 / 50000 train. data). Loss: 1.4012365341186523\n","Training log: 131 epoch (43648 / 50000 train. data). Loss: 1.3003082275390625\n","Training log: 131 epoch (44928 / 50000 train. data). Loss: 1.2373342514038086\n","Training log: 131 epoch (46208 / 50000 train. data). Loss: 1.201143741607666\n","Training log: 131 epoch (47488 / 50000 train. data). Loss: 1.10343599319458\n","Training log: 131 epoch (48768 / 50000 train. data). Loss: 1.1765272617340088\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 3/391 [00:00<00:13, 28.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 131 epoch (50048 / 50000 train. data). Loss: 1.0799696445465088\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.10it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.61it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.618900\n","Training log: 132 epoch (128 / 50000 train. data). Loss: 1.1601011753082275\n","Training log: 132 epoch (1408 / 50000 train. data). Loss: 1.2790542840957642\n","Training log: 132 epoch (2688 / 50000 train. data). Loss: 1.2723183631896973\n","Training log: 132 epoch (3968 / 50000 train. data). Loss: 1.2858737707138062\n","Training log: 132 epoch (5248 / 50000 train. data). Loss: 1.2411885261535645\n","Training log: 132 epoch (6528 / 50000 train. data). Loss: 1.3259960412979126\n","Training log: 132 epoch (7808 / 50000 train. data). Loss: 1.4192757606506348\n","Training log: 132 epoch (9088 / 50000 train. data). Loss: 1.162068247795105\n","Training log: 132 epoch (10368 / 50000 train. data). Loss: 1.1160166263580322\n","Training log: 132 epoch (11648 / 50000 train. data). Loss: 1.266295075416565\n","Training log: 132 epoch (12928 / 50000 train. data). Loss: 1.2898836135864258\n","Training log: 132 epoch (14208 / 50000 train. data). Loss: 1.3651987314224243\n","Training log: 132 epoch (15488 / 50000 train. data). Loss: 1.02777898311615\n","Training log: 132 epoch (16768 / 50000 train. data). Loss: 1.309182047843933\n","Training log: 132 epoch (18048 / 50000 train. data). Loss: 1.1538748741149902\n","Training log: 132 epoch (19328 / 50000 train. data). Loss: 1.1851075887680054\n","Training log: 132 epoch (20608 / 50000 train. data). Loss: 1.297210454940796\n","Training log: 132 epoch (21888 / 50000 train. data). Loss: 1.2238433361053467\n","Training log: 132 epoch (23168 / 50000 train. data). Loss: 1.2418699264526367\n","Training log: 132 epoch (24448 / 50000 train. data). Loss: 1.2126049995422363\n","Training log: 132 epoch (25728 / 50000 train. data). Loss: 1.2100449800491333\n","Training log: 132 epoch (27008 / 50000 train. data). Loss: 1.3413910865783691\n","Training log: 132 epoch (28288 / 50000 train. data). Loss: 1.2491323947906494\n","Training log: 132 epoch (29568 / 50000 train. data). Loss: 1.279166579246521\n","Training log: 132 epoch (30848 / 50000 train. data). Loss: 1.0645604133605957\n","Training log: 132 epoch (32128 / 50000 train. data). Loss: 1.2908391952514648\n","Training log: 132 epoch (33408 / 50000 train. data). Loss: 1.0704243183135986\n","Training log: 132 epoch (34688 / 50000 train. data). Loss: 1.158980369567871\n","Training log: 132 epoch (35968 / 50000 train. data). Loss: 1.0970332622528076\n","Training log: 132 epoch (37248 / 50000 train. data). Loss: 1.2725780010223389\n","Training log: 132 epoch (38528 / 50000 train. data). Loss: 1.2468467950820923\n","Training log: 132 epoch (39808 / 50000 train. data). Loss: 1.2193204164505005\n","Training log: 132 epoch (41088 / 50000 train. data). Loss: 1.2653114795684814\n","Training log: 132 epoch (42368 / 50000 train. data). Loss: 1.1989105939865112\n","Training log: 132 epoch (43648 / 50000 train. data). Loss: 1.125545859336853\n","Training log: 132 epoch (44928 / 50000 train. data). Loss: 1.3493596315383911\n","Training log: 132 epoch (46208 / 50000 train. data). Loss: 1.2509865760803223\n","Training log: 132 epoch (47488 / 50000 train. data). Loss: 1.268824577331543\n","Training log: 132 epoch (48768 / 50000 train. data). Loss: 1.189351201057434\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 32.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 132 epoch (50048 / 50000 train. data). Loss: 1.3020182847976685\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.53it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.77it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.606800\n","Training log: 133 epoch (128 / 50000 train. data). Loss: 1.2334569692611694\n","Training log: 133 epoch (1408 / 50000 train. data). Loss: 1.3444323539733887\n","Training log: 133 epoch (2688 / 50000 train. data). Loss: 1.331102967262268\n","Training log: 133 epoch (3968 / 50000 train. data). Loss: 1.2238143682479858\n","Training log: 133 epoch (5248 / 50000 train. data). Loss: 1.146528959274292\n","Training log: 133 epoch (6528 / 50000 train. data). Loss: 1.194619059562683\n","Training log: 133 epoch (7808 / 50000 train. data). Loss: 1.3890842199325562\n","Training log: 133 epoch (9088 / 50000 train. data). Loss: 1.2516865730285645\n","Training log: 133 epoch (10368 / 50000 train. data). Loss: 1.265006184577942\n","Training log: 133 epoch (11648 / 50000 train. data). Loss: 1.236371397972107\n","Training log: 133 epoch (12928 / 50000 train. data). Loss: 1.1597859859466553\n","Training log: 133 epoch (14208 / 50000 train. data). Loss: 1.2604066133499146\n","Training log: 133 epoch (15488 / 50000 train. data). Loss: 1.3611648082733154\n","Training log: 133 epoch (16768 / 50000 train. data). Loss: 1.349602460861206\n","Training log: 133 epoch (18048 / 50000 train. data). Loss: 1.318953275680542\n","Training log: 133 epoch (19328 / 50000 train. data). Loss: 1.118216872215271\n","Training log: 133 epoch (20608 / 50000 train. data). Loss: 1.197824239730835\n","Training log: 133 epoch (21888 / 50000 train. data). Loss: 1.147052526473999\n","Training log: 133 epoch (23168 / 50000 train. data). Loss: 1.1532654762268066\n","Training log: 133 epoch (24448 / 50000 train. data). Loss: 1.2850587368011475\n","Training log: 133 epoch (25728 / 50000 train. data). Loss: 1.2273849248886108\n","Training log: 133 epoch (27008 / 50000 train. data). Loss: 1.2479361295700073\n","Training log: 133 epoch (28288 / 50000 train. data). Loss: 1.280571699142456\n","Training log: 133 epoch (29568 / 50000 train. data). Loss: 1.1757570505142212\n","Training log: 133 epoch (30848 / 50000 train. data). Loss: 1.3700861930847168\n","Training log: 133 epoch (32128 / 50000 train. data). Loss: 1.2479461431503296\n","Training log: 133 epoch (33408 / 50000 train. data). Loss: 1.0872410535812378\n","Training log: 133 epoch (34688 / 50000 train. data). Loss: 1.1522035598754883\n","Training log: 133 epoch (35968 / 50000 train. data). Loss: 1.335329532623291\n","Training log: 133 epoch (37248 / 50000 train. data). Loss: 1.339266061782837\n","Training log: 133 epoch (38528 / 50000 train. data). Loss: 1.2618802785873413\n","Training log: 133 epoch (39808 / 50000 train. data). Loss: 1.3683408498764038\n","Training log: 133 epoch (41088 / 50000 train. data). Loss: 1.385403037071228\n","Training log: 133 epoch (42368 / 50000 train. data). Loss: 1.2450164556503296\n","Training log: 133 epoch (43648 / 50000 train. data). Loss: 1.1541064977645874\n","Training log: 133 epoch (44928 / 50000 train. data). Loss: 1.235001564025879\n","Training log: 133 epoch (46208 / 50000 train. data). Loss: 1.192563772201538\n","Training log: 133 epoch (47488 / 50000 train. data). Loss: 1.1335020065307617\n","Training log: 133 epoch (48768 / 50000 train. data). Loss: 1.2901123762130737\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 133 epoch (50048 / 50000 train. data). Loss: 1.032294511795044\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.08it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.11it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.613800\n","Training log: 134 epoch (128 / 50000 train. data). Loss: 1.563643455505371\n","Training log: 134 epoch (1408 / 50000 train. data). Loss: 1.20409095287323\n","Training log: 134 epoch (2688 / 50000 train. data). Loss: 1.1205992698669434\n","Training log: 134 epoch (3968 / 50000 train. data). Loss: 1.104315996170044\n","Training log: 134 epoch (5248 / 50000 train. data). Loss: 1.3223448991775513\n","Training log: 134 epoch (6528 / 50000 train. data). Loss: 1.2557270526885986\n","Training log: 134 epoch (7808 / 50000 train. data). Loss: 1.2697889804840088\n","Training log: 134 epoch (9088 / 50000 train. data). Loss: 1.3015098571777344\n","Training log: 134 epoch (10368 / 50000 train. data). Loss: 1.2375680208206177\n","Training log: 134 epoch (11648 / 50000 train. data). Loss: 1.267074465751648\n","Training log: 134 epoch (12928 / 50000 train. data). Loss: 1.1852010488510132\n","Training log: 134 epoch (14208 / 50000 train. data). Loss: 1.1114201545715332\n","Training log: 134 epoch (15488 / 50000 train. data). Loss: 1.3705953359603882\n","Training log: 134 epoch (16768 / 50000 train. data). Loss: 1.036412000656128\n","Training log: 134 epoch (18048 / 50000 train. data). Loss: 1.0574309825897217\n","Training log: 134 epoch (19328 / 50000 train. data). Loss: 1.1129670143127441\n","Training log: 134 epoch (20608 / 50000 train. data). Loss: 1.2737677097320557\n","Training log: 134 epoch (21888 / 50000 train. data). Loss: 1.235517978668213\n","Training log: 134 epoch (23168 / 50000 train. data). Loss: 1.4412052631378174\n","Training log: 134 epoch (24448 / 50000 train. data). Loss: 1.2124104499816895\n","Training log: 134 epoch (25728 / 50000 train. data). Loss: 1.27986478805542\n","Training log: 134 epoch (27008 / 50000 train. data). Loss: 1.1994349956512451\n","Training log: 134 epoch (28288 / 50000 train. data). Loss: 1.2317534685134888\n","Training log: 134 epoch (29568 / 50000 train. data). Loss: 1.0870482921600342\n","Training log: 134 epoch (30848 / 50000 train. data). Loss: 1.1635993719100952\n","Training log: 134 epoch (32128 / 50000 train. data). Loss: 1.2252928018569946\n","Training log: 134 epoch (33408 / 50000 train. data). Loss: 1.15422785282135\n","Training log: 134 epoch (34688 / 50000 train. data). Loss: 1.1650428771972656\n","Training log: 134 epoch (35968 / 50000 train. data). Loss: 1.2217786312103271\n","Training log: 134 epoch (37248 / 50000 train. data). Loss: 1.2730076313018799\n","Training log: 134 epoch (38528 / 50000 train. data). Loss: 1.0748409032821655\n","Training log: 134 epoch (39808 / 50000 train. data). Loss: 1.25031316280365\n","Training log: 134 epoch (41088 / 50000 train. data). Loss: 1.3040785789489746\n","Training log: 134 epoch (42368 / 50000 train. data). Loss: 1.3393802642822266\n","Training log: 134 epoch (43648 / 50000 train. data). Loss: 1.2443736791610718\n","Training log: 134 epoch (44928 / 50000 train. data). Loss: 1.3598519563674927\n","Training log: 134 epoch (46208 / 50000 train. data). Loss: 1.2756702899932861\n","Training log: 134 epoch (47488 / 50000 train. data). Loss: 1.1125438213348389\n","Training log: 134 epoch (48768 / 50000 train. data). Loss: 1.1513594388961792\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 34.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 134 epoch (50048 / 50000 train. data). Loss: 1.3040494918823242\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.98it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.618700\n","Training log: 135 epoch (128 / 50000 train. data). Loss: 1.0388129949569702\n","Training log: 135 epoch (1408 / 50000 train. data). Loss: 1.1925685405731201\n","Training log: 135 epoch (2688 / 50000 train. data). Loss: 1.1668920516967773\n","Training log: 135 epoch (3968 / 50000 train. data). Loss: 1.2855010032653809\n","Training log: 135 epoch (5248 / 50000 train. data). Loss: 1.2461943626403809\n","Training log: 135 epoch (6528 / 50000 train. data). Loss: 1.3619954586029053\n","Training log: 135 epoch (7808 / 50000 train. data). Loss: 1.3451708555221558\n","Training log: 135 epoch (9088 / 50000 train. data). Loss: 1.3948750495910645\n","Training log: 135 epoch (10368 / 50000 train. data). Loss: 1.1961568593978882\n","Training log: 135 epoch (11648 / 50000 train. data). Loss: 1.303065538406372\n","Training log: 135 epoch (12928 / 50000 train. data). Loss: 1.1160554885864258\n","Training log: 135 epoch (14208 / 50000 train. data). Loss: 1.1202155351638794\n","Training log: 135 epoch (15488 / 50000 train. data). Loss: 1.217758297920227\n","Training log: 135 epoch (16768 / 50000 train. data). Loss: 1.1749038696289062\n","Training log: 135 epoch (18048 / 50000 train. data). Loss: 1.2401771545410156\n","Training log: 135 epoch (19328 / 50000 train. data). Loss: 1.1718473434448242\n","Training log: 135 epoch (20608 / 50000 train. data). Loss: 1.5162776708602905\n","Training log: 135 epoch (21888 / 50000 train. data). Loss: 1.254891037940979\n","Training log: 135 epoch (23168 / 50000 train. data). Loss: 1.0597509145736694\n","Training log: 135 epoch (24448 / 50000 train. data). Loss: 1.224169135093689\n","Training log: 135 epoch (25728 / 50000 train. data). Loss: 1.3476622104644775\n","Training log: 135 epoch (27008 / 50000 train. data). Loss: 1.2077434062957764\n","Training log: 135 epoch (28288 / 50000 train. data). Loss: 1.2571535110473633\n","Training log: 135 epoch (29568 / 50000 train. data). Loss: 1.296686053276062\n","Training log: 135 epoch (30848 / 50000 train. data). Loss: 1.3665754795074463\n","Training log: 135 epoch (32128 / 50000 train. data). Loss: 1.3161319494247437\n","Training log: 135 epoch (33408 / 50000 train. data). Loss: 1.3446134328842163\n","Training log: 135 epoch (34688 / 50000 train. data). Loss: 1.1515835523605347\n","Training log: 135 epoch (35968 / 50000 train. data). Loss: 1.132280945777893\n","Training log: 135 epoch (37248 / 50000 train. data). Loss: 1.2518349885940552\n","Training log: 135 epoch (38528 / 50000 train. data). Loss: 1.2626878023147583\n","Training log: 135 epoch (39808 / 50000 train. data). Loss: 1.287495493888855\n","Training log: 135 epoch (41088 / 50000 train. data). Loss: 1.5046473741531372\n","Training log: 135 epoch (42368 / 50000 train. data). Loss: 1.3036037683486938\n","Training log: 135 epoch (43648 / 50000 train. data). Loss: 1.1339409351348877\n","Training log: 135 epoch (44928 / 50000 train. data). Loss: 1.2567007541656494\n","Training log: 135 epoch (46208 / 50000 train. data). Loss: 1.182645559310913\n","Training log: 135 epoch (47488 / 50000 train. data). Loss: 1.143363356590271\n","Training log: 135 epoch (48768 / 50000 train. data). Loss: 1.3263541460037231\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.34it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 135 epoch (50048 / 50000 train. data). Loss: 1.4161021709442139\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.88it/s]\n","100%|██████████| 79/79 [00:02<00:00, 31.58it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.621100\n","Training log: 136 epoch (128 / 50000 train. data). Loss: 1.3230310678482056\n","Training log: 136 epoch (1408 / 50000 train. data). Loss: 1.2872651815414429\n","Training log: 136 epoch (2688 / 50000 train. data). Loss: 1.2308969497680664\n","Training log: 136 epoch (3968 / 50000 train. data). Loss: 1.275475263595581\n","Training log: 136 epoch (5248 / 50000 train. data). Loss: 1.1735130548477173\n","Training log: 136 epoch (6528 / 50000 train. data). Loss: 1.2860796451568604\n","Training log: 136 epoch (7808 / 50000 train. data). Loss: 1.3202471733093262\n","Training log: 136 epoch (9088 / 50000 train. data). Loss: 1.2689656019210815\n","Training log: 136 epoch (10368 / 50000 train. data). Loss: 1.1501003503799438\n","Training log: 136 epoch (11648 / 50000 train. data). Loss: 1.1865376234054565\n","Training log: 136 epoch (12928 / 50000 train. data). Loss: 1.1814296245574951\n","Training log: 136 epoch (14208 / 50000 train. data). Loss: 1.2396564483642578\n","Training log: 136 epoch (15488 / 50000 train. data). Loss: 1.3625428676605225\n","Training log: 136 epoch (16768 / 50000 train. data). Loss: 1.194583535194397\n","Training log: 136 epoch (18048 / 50000 train. data). Loss: 1.219127893447876\n","Training log: 136 epoch (19328 / 50000 train. data). Loss: 1.2542058229446411\n","Training log: 136 epoch (20608 / 50000 train. data). Loss: 1.2456252574920654\n","Training log: 136 epoch (21888 / 50000 train. data). Loss: 1.2075814008712769\n","Training log: 136 epoch (23168 / 50000 train. data). Loss: 1.364128828048706\n","Training log: 136 epoch (24448 / 50000 train. data). Loss: 1.2304396629333496\n","Training log: 136 epoch (25728 / 50000 train. data). Loss: 1.1314961910247803\n","Training log: 136 epoch (27008 / 50000 train. data). Loss: 1.2970569133758545\n","Training log: 136 epoch (28288 / 50000 train. data). Loss: 1.2224953174591064\n","Training log: 136 epoch (29568 / 50000 train. data). Loss: 1.1108224391937256\n","Training log: 136 epoch (30848 / 50000 train. data). Loss: 1.232379674911499\n","Training log: 136 epoch (32128 / 50000 train. data). Loss: 1.0889406204223633\n","Training log: 136 epoch (33408 / 50000 train. data). Loss: 1.1016696691513062\n","Training log: 136 epoch (34688 / 50000 train. data). Loss: 1.2336840629577637\n","Training log: 136 epoch (35968 / 50000 train. data). Loss: 1.092576026916504\n","Training log: 136 epoch (37248 / 50000 train. data). Loss: 1.1101207733154297\n","Training log: 136 epoch (38528 / 50000 train. data). Loss: 1.157114028930664\n","Training log: 136 epoch (39808 / 50000 train. data). Loss: 1.3743597269058228\n","Training log: 136 epoch (41088 / 50000 train. data). Loss: 1.3883521556854248\n","Training log: 136 epoch (42368 / 50000 train. data). Loss: 1.2131835222244263\n","Training log: 136 epoch (43648 / 50000 train. data). Loss: 1.0706794261932373\n","Training log: 136 epoch (44928 / 50000 train. data). Loss: 1.2058453559875488\n","Training log: 136 epoch (46208 / 50000 train. data). Loss: 1.4011991024017334\n","Training log: 136 epoch (47488 / 50000 train. data). Loss: 1.2507798671722412\n","Training log: 136 epoch (48768 / 50000 train. data). Loss: 1.2368608713150024\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 136 epoch (50048 / 50000 train. data). Loss: 1.3533321619033813\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.05it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.75it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.616800\n","Training log: 137 epoch (128 / 50000 train. data). Loss: 1.1837295293807983\n","Training log: 137 epoch (1408 / 50000 train. data). Loss: 1.2120083570480347\n","Training log: 137 epoch (2688 / 50000 train. data). Loss: 1.0890536308288574\n","Training log: 137 epoch (3968 / 50000 train. data). Loss: 1.3120137453079224\n","Training log: 137 epoch (5248 / 50000 train. data). Loss: 1.3514360189437866\n","Training log: 137 epoch (6528 / 50000 train. data). Loss: 1.1725304126739502\n","Training log: 137 epoch (7808 / 50000 train. data). Loss: 1.1777985095977783\n","Training log: 137 epoch (9088 / 50000 train. data). Loss: 1.3303718566894531\n","Training log: 137 epoch (10368 / 50000 train. data). Loss: 1.1941343545913696\n","Training log: 137 epoch (11648 / 50000 train. data). Loss: 1.2929871082305908\n","Training log: 137 epoch (12928 / 50000 train. data). Loss: 1.2457960844039917\n","Training log: 137 epoch (14208 / 50000 train. data). Loss: 1.2502243518829346\n","Training log: 137 epoch (15488 / 50000 train. data). Loss: 1.365196704864502\n","Training log: 137 epoch (16768 / 50000 train. data). Loss: 1.2082453966140747\n","Training log: 137 epoch (18048 / 50000 train. data). Loss: 1.1607574224472046\n","Training log: 137 epoch (19328 / 50000 train. data). Loss: 1.1600050926208496\n","Training log: 137 epoch (20608 / 50000 train. data). Loss: 1.2771998643875122\n","Training log: 137 epoch (21888 / 50000 train. data). Loss: 1.1383183002471924\n","Training log: 137 epoch (23168 / 50000 train. data). Loss: 1.0444679260253906\n","Training log: 137 epoch (24448 / 50000 train. data). Loss: 1.2040395736694336\n","Training log: 137 epoch (25728 / 50000 train. data). Loss: 1.2556405067443848\n","Training log: 137 epoch (27008 / 50000 train. data). Loss: 1.1265217065811157\n","Training log: 137 epoch (28288 / 50000 train. data). Loss: 1.149553894996643\n","Training log: 137 epoch (29568 / 50000 train. data). Loss: 1.3787919282913208\n","Training log: 137 epoch (30848 / 50000 train. data). Loss: 1.194413423538208\n","Training log: 137 epoch (32128 / 50000 train. data). Loss: 1.2565569877624512\n","Training log: 137 epoch (33408 / 50000 train. data). Loss: 1.143319845199585\n","Training log: 137 epoch (34688 / 50000 train. data). Loss: 1.2395750284194946\n","Training log: 137 epoch (35968 / 50000 train. data). Loss: 1.2686777114868164\n","Training log: 137 epoch (37248 / 50000 train. data). Loss: 1.3246731758117676\n","Training log: 137 epoch (38528 / 50000 train. data). Loss: 1.295689344406128\n","Training log: 137 epoch (39808 / 50000 train. data). Loss: 1.0861248970031738\n","Training log: 137 epoch (41088 / 50000 train. data). Loss: 1.2271379232406616\n","Training log: 137 epoch (42368 / 50000 train. data). Loss: 1.3263776302337646\n","Training log: 137 epoch (43648 / 50000 train. data). Loss: 1.4756821393966675\n","Training log: 137 epoch (44928 / 50000 train. data). Loss: 1.3738605976104736\n","Training log: 137 epoch (46208 / 50000 train. data). Loss: 1.0468286275863647\n","Training log: 137 epoch (47488 / 50000 train. data). Loss: 1.3747611045837402\n","Training log: 137 epoch (48768 / 50000 train. data). Loss: 1.2316581010818481\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 34.92it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 137 epoch (50048 / 50000 train. data). Loss: 1.1543707847595215\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.54it/s]\n","100%|██████████| 79/79 [00:02<00:00, 31.64it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.609700\n","Training log: 138 epoch (128 / 50000 train. data). Loss: 1.2449315786361694\n","Training log: 138 epoch (1408 / 50000 train. data). Loss: 1.0566860437393188\n","Training log: 138 epoch (2688 / 50000 train. data). Loss: 1.2058038711547852\n","Training log: 138 epoch (3968 / 50000 train. data). Loss: 1.2026368379592896\n","Training log: 138 epoch (5248 / 50000 train. data). Loss: 1.08701491355896\n","Training log: 138 epoch (6528 / 50000 train. data). Loss: 1.331777811050415\n","Training log: 138 epoch (7808 / 50000 train. data). Loss: 1.4706225395202637\n","Training log: 138 epoch (9088 / 50000 train. data). Loss: 1.1573659181594849\n","Training log: 138 epoch (10368 / 50000 train. data). Loss: 1.1051291227340698\n","Training log: 138 epoch (11648 / 50000 train. data). Loss: 1.1709394454956055\n","Training log: 138 epoch (12928 / 50000 train. data). Loss: 1.2732642889022827\n","Training log: 138 epoch (14208 / 50000 train. data). Loss: 1.2704769372940063\n","Training log: 138 epoch (15488 / 50000 train. data). Loss: 1.0668535232543945\n","Training log: 138 epoch (16768 / 50000 train. data). Loss: 1.2721961736679077\n","Training log: 138 epoch (18048 / 50000 train. data). Loss: 1.1917355060577393\n","Training log: 138 epoch (19328 / 50000 train. data). Loss: 1.2155503034591675\n","Training log: 138 epoch (20608 / 50000 train. data). Loss: 1.2577545642852783\n","Training log: 138 epoch (21888 / 50000 train. data). Loss: 1.3066656589508057\n","Training log: 138 epoch (23168 / 50000 train. data). Loss: 1.2414829730987549\n","Training log: 138 epoch (24448 / 50000 train. data). Loss: 1.3875163793563843\n","Training log: 138 epoch (25728 / 50000 train. data). Loss: 1.1197233200073242\n","Training log: 138 epoch (27008 / 50000 train. data). Loss: 1.255013108253479\n","Training log: 138 epoch (28288 / 50000 train. data). Loss: 1.1979588270187378\n","Training log: 138 epoch (29568 / 50000 train. data). Loss: 1.2583543062210083\n","Training log: 138 epoch (30848 / 50000 train. data). Loss: 1.2207436561584473\n","Training log: 138 epoch (32128 / 50000 train. data). Loss: 1.1820741891860962\n","Training log: 138 epoch (33408 / 50000 train. data). Loss: 1.1968553066253662\n","Training log: 138 epoch (34688 / 50000 train. data). Loss: 1.1470507383346558\n","Training log: 138 epoch (35968 / 50000 train. data). Loss: 1.3656768798828125\n","Training log: 138 epoch (37248 / 50000 train. data). Loss: 1.233686089515686\n","Training log: 138 epoch (38528 / 50000 train. data). Loss: 1.2979100942611694\n","Training log: 138 epoch (39808 / 50000 train. data). Loss: 1.2963484525680542\n","Training log: 138 epoch (41088 / 50000 train. data). Loss: 1.3701601028442383\n","Training log: 138 epoch (42368 / 50000 train. data). Loss: 1.3553217649459839\n","Training log: 138 epoch (43648 / 50000 train. data). Loss: 1.2290990352630615\n","Training log: 138 epoch (44928 / 50000 train. data). Loss: 1.2506611347198486\n","Training log: 138 epoch (46208 / 50000 train. data). Loss: 1.264617919921875\n","Training log: 138 epoch (47488 / 50000 train. data). Loss: 1.0396701097488403\n","Training log: 138 epoch (48768 / 50000 train. data). Loss: 1.185814380645752\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 35.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 138 epoch (50048 / 50000 train. data). Loss: 1.1145679950714111\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.47it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.618500\n","Training log: 139 epoch (128 / 50000 train. data). Loss: 1.215715765953064\n","Training log: 139 epoch (1408 / 50000 train. data). Loss: 1.150524377822876\n","Training log: 139 epoch (2688 / 50000 train. data). Loss: 1.1933249235153198\n","Training log: 139 epoch (3968 / 50000 train. data). Loss: 1.08330500125885\n","Training log: 139 epoch (5248 / 50000 train. data). Loss: 1.1683392524719238\n","Training log: 139 epoch (6528 / 50000 train. data). Loss: 1.18631112575531\n","Training log: 139 epoch (7808 / 50000 train. data). Loss: 1.144079566001892\n","Training log: 139 epoch (9088 / 50000 train. data). Loss: 1.1032793521881104\n","Training log: 139 epoch (10368 / 50000 train. data). Loss: 1.2318881750106812\n","Training log: 139 epoch (11648 / 50000 train. data). Loss: 1.2197643518447876\n","Training log: 139 epoch (12928 / 50000 train. data). Loss: 1.3391984701156616\n","Training log: 139 epoch (14208 / 50000 train. data). Loss: 1.1527589559555054\n","Training log: 139 epoch (15488 / 50000 train. data). Loss: 1.1850346326828003\n","Training log: 139 epoch (16768 / 50000 train. data). Loss: 1.178322672843933\n","Training log: 139 epoch (18048 / 50000 train. data). Loss: 1.2103768587112427\n","Training log: 139 epoch (19328 / 50000 train. data). Loss: 1.2425140142440796\n","Training log: 139 epoch (20608 / 50000 train. data). Loss: 1.3871676921844482\n","Training log: 139 epoch (21888 / 50000 train. data). Loss: 1.2564067840576172\n","Training log: 139 epoch (23168 / 50000 train. data). Loss: 1.1044132709503174\n","Training log: 139 epoch (24448 / 50000 train. data). Loss: 1.2860599756240845\n","Training log: 139 epoch (25728 / 50000 train. data). Loss: 1.1223305463790894\n","Training log: 139 epoch (27008 / 50000 train. data). Loss: 1.3364137411117554\n","Training log: 139 epoch (28288 / 50000 train. data). Loss: 1.176790714263916\n","Training log: 139 epoch (29568 / 50000 train. data). Loss: 1.3617066144943237\n","Training log: 139 epoch (30848 / 50000 train. data). Loss: 1.2764713764190674\n","Training log: 139 epoch (32128 / 50000 train. data). Loss: 1.2242764234542847\n","Training log: 139 epoch (33408 / 50000 train. data). Loss: 1.1383154392242432\n","Training log: 139 epoch (34688 / 50000 train. data). Loss: 1.4492484331130981\n","Training log: 139 epoch (35968 / 50000 train. data). Loss: 1.3754684925079346\n","Training log: 139 epoch (37248 / 50000 train. data). Loss: 1.3070054054260254\n","Training log: 139 epoch (38528 / 50000 train. data). Loss: 1.0663514137268066\n","Training log: 139 epoch (39808 / 50000 train. data). Loss: 1.0939480066299438\n","Training log: 139 epoch (41088 / 50000 train. data). Loss: 1.2958526611328125\n","Training log: 139 epoch (42368 / 50000 train. data). Loss: 1.232811450958252\n","Training log: 139 epoch (43648 / 50000 train. data). Loss: 1.1944234371185303\n","Training log: 139 epoch (44928 / 50000 train. data). Loss: 1.262086272239685\n","Training log: 139 epoch (46208 / 50000 train. data). Loss: 1.2112776041030884\n","Training log: 139 epoch (47488 / 50000 train. data). Loss: 1.1691381931304932\n","Training log: 139 epoch (48768 / 50000 train. data). Loss: 1.0586484670639038\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.73it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 139 epoch (50048 / 50000 train. data). Loss: 1.2949833869934082\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 33.33it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.84it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.609500\n","Training log: 140 epoch (128 / 50000 train. data). Loss: 1.4214153289794922\n","Training log: 140 epoch (1408 / 50000 train. data). Loss: 1.4031577110290527\n","Training log: 140 epoch (2688 / 50000 train. data). Loss: 1.34028160572052\n","Training log: 140 epoch (3968 / 50000 train. data). Loss: 1.2676345109939575\n","Training log: 140 epoch (5248 / 50000 train. data). Loss: 1.172033667564392\n","Training log: 140 epoch (6528 / 50000 train. data). Loss: 1.2872146368026733\n","Training log: 140 epoch (7808 / 50000 train. data). Loss: 1.3310984373092651\n","Training log: 140 epoch (9088 / 50000 train. data). Loss: 1.3075973987579346\n","Training log: 140 epoch (10368 / 50000 train. data). Loss: 1.347269892692566\n","Training log: 140 epoch (11648 / 50000 train. data). Loss: 1.291313886642456\n","Training log: 140 epoch (12928 / 50000 train. data). Loss: 1.1444374322891235\n","Training log: 140 epoch (14208 / 50000 train. data). Loss: 1.2592803239822388\n","Training log: 140 epoch (15488 / 50000 train. data). Loss: 1.258451223373413\n","Training log: 140 epoch (16768 / 50000 train. data). Loss: 1.0271456241607666\n","Training log: 140 epoch (18048 / 50000 train. data). Loss: 1.2473714351654053\n","Training log: 140 epoch (19328 / 50000 train. data). Loss: 1.4680736064910889\n","Training log: 140 epoch (20608 / 50000 train. data). Loss: 1.1553490161895752\n","Training log: 140 epoch (21888 / 50000 train. data). Loss: 1.1755759716033936\n","Training log: 140 epoch (23168 / 50000 train. data). Loss: 1.2455987930297852\n","Training log: 140 epoch (24448 / 50000 train. data). Loss: 1.1246650218963623\n","Training log: 140 epoch (25728 / 50000 train. data). Loss: 1.1948823928833008\n","Training log: 140 epoch (27008 / 50000 train. data). Loss: 1.1994439363479614\n","Training log: 140 epoch (28288 / 50000 train. data). Loss: 1.2631070613861084\n","Training log: 140 epoch (29568 / 50000 train. data). Loss: 1.3768736124038696\n","Training log: 140 epoch (30848 / 50000 train. data). Loss: 1.355818748474121\n","Training log: 140 epoch (32128 / 50000 train. data). Loss: 1.2615476846694946\n","Training log: 140 epoch (33408 / 50000 train. data). Loss: 1.2596006393432617\n","Training log: 140 epoch (34688 / 50000 train. data). Loss: 1.1514694690704346\n","Training log: 140 epoch (35968 / 50000 train. data). Loss: 1.1960530281066895\n","Training log: 140 epoch (37248 / 50000 train. data). Loss: 1.3391739130020142\n","Training log: 140 epoch (38528 / 50000 train. data). Loss: 1.2362960577011108\n","Training log: 140 epoch (39808 / 50000 train. data). Loss: 1.216012716293335\n","Training log: 140 epoch (41088 / 50000 train. data). Loss: 1.2294907569885254\n","Training log: 140 epoch (42368 / 50000 train. data). Loss: 1.2745802402496338\n","Training log: 140 epoch (43648 / 50000 train. data). Loss: 1.362349510192871\n","Training log: 140 epoch (44928 / 50000 train. data). Loss: 1.1621469259262085\n","Training log: 140 epoch (46208 / 50000 train. data). Loss: 1.4316236972808838\n","Training log: 140 epoch (47488 / 50000 train. data). Loss: 1.4350401163101196\n","Training log: 140 epoch (48768 / 50000 train. data). Loss: 1.216676950454712\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 32.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 140 epoch (50048 / 50000 train. data). Loss: 1.2341700792312622\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 34.56it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.49it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.615500\n","Training log: 141 epoch (128 / 50000 train. data). Loss: 1.1890828609466553\n","Training log: 141 epoch (1408 / 50000 train. data). Loss: 1.2165169715881348\n","Training log: 141 epoch (2688 / 50000 train. data). Loss: 1.180465817451477\n","Training log: 141 epoch (3968 / 50000 train. data). Loss: 1.2152159214019775\n","Training log: 141 epoch (5248 / 50000 train. data). Loss: 1.2925931215286255\n","Training log: 141 epoch (6528 / 50000 train. data). Loss: 1.1000192165374756\n","Training log: 141 epoch (7808 / 50000 train. data). Loss: 1.2861342430114746\n","Training log: 141 epoch (9088 / 50000 train. data). Loss: 1.1231498718261719\n","Training log: 141 epoch (10368 / 50000 train. data). Loss: 1.2494158744812012\n","Training log: 141 epoch (11648 / 50000 train. data). Loss: 1.2746816873550415\n","Training log: 141 epoch (12928 / 50000 train. data). Loss: 1.2481883764266968\n","Training log: 141 epoch (14208 / 50000 train. data). Loss: 1.1392451524734497\n","Training log: 141 epoch (15488 / 50000 train. data). Loss: 1.1948120594024658\n","Training log: 141 epoch (16768 / 50000 train. data). Loss: 1.2498081922531128\n","Training log: 141 epoch (18048 / 50000 train. data). Loss: 1.2278187274932861\n","Training log: 141 epoch (19328 / 50000 train. data). Loss: 1.2701448202133179\n","Training log: 141 epoch (20608 / 50000 train. data). Loss: 1.2336704730987549\n","Training log: 141 epoch (21888 / 50000 train. data). Loss: 1.3039482831954956\n","Training log: 141 epoch (23168 / 50000 train. data). Loss: 1.2816293239593506\n","Training log: 141 epoch (24448 / 50000 train. data). Loss: 1.1847752332687378\n","Training log: 141 epoch (25728 / 50000 train. data). Loss: 1.294994592666626\n","Training log: 141 epoch (27008 / 50000 train. data). Loss: 1.300050973892212\n","Training log: 141 epoch (28288 / 50000 train. data). Loss: 1.2532305717468262\n","Training log: 141 epoch (29568 / 50000 train. data). Loss: 1.1776515245437622\n","Training log: 141 epoch (30848 / 50000 train. data). Loss: 1.0545591115951538\n","Training log: 141 epoch (32128 / 50000 train. data). Loss: 1.170606255531311\n","Training log: 141 epoch (33408 / 50000 train. data). Loss: 1.2163368463516235\n","Training log: 141 epoch (34688 / 50000 train. data). Loss: 1.3673572540283203\n","Training log: 141 epoch (35968 / 50000 train. data). Loss: 1.2786282300949097\n","Training log: 141 epoch (37248 / 50000 train. data). Loss: 1.2613091468811035\n","Training log: 141 epoch (38528 / 50000 train. data). Loss: 1.220231056213379\n","Training log: 141 epoch (39808 / 50000 train. data). Loss: 1.430907964706421\n","Training log: 141 epoch (41088 / 50000 train. data). Loss: 1.379463791847229\n","Training log: 141 epoch (42368 / 50000 train. data). Loss: 1.1330283880233765\n","Training log: 141 epoch (43648 / 50000 train. data). Loss: 1.1084108352661133\n","Training log: 141 epoch (44928 / 50000 train. data). Loss: 1.131258487701416\n","Training log: 141 epoch (46208 / 50000 train. data). Loss: 1.2323501110076904\n","Training log: 141 epoch (47488 / 50000 train. data). Loss: 1.284903883934021\n","Training log: 141 epoch (48768 / 50000 train. data). Loss: 1.2207087278366089\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 37.19it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 141 epoch (50048 / 50000 train. data). Loss: 1.2539732456207275\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.84it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.616000\n","Training log: 142 epoch (128 / 50000 train. data). Loss: 1.1247791051864624\n","Training log: 142 epoch (1408 / 50000 train. data). Loss: 1.1194283962249756\n","Training log: 142 epoch (2688 / 50000 train. data). Loss: 1.087862253189087\n","Training log: 142 epoch (3968 / 50000 train. data). Loss: 1.0543971061706543\n","Training log: 142 epoch (5248 / 50000 train. data). Loss: 1.2029341459274292\n","Training log: 142 epoch (6528 / 50000 train. data). Loss: 1.1228491067886353\n","Training log: 142 epoch (7808 / 50000 train. data). Loss: 1.22829008102417\n","Training log: 142 epoch (9088 / 50000 train. data). Loss: 1.2983522415161133\n","Training log: 142 epoch (10368 / 50000 train. data). Loss: 1.1922836303710938\n","Training log: 142 epoch (11648 / 50000 train. data). Loss: 1.1111420392990112\n","Training log: 142 epoch (12928 / 50000 train. data). Loss: 1.2897469997406006\n","Training log: 142 epoch (14208 / 50000 train. data). Loss: 1.154166340827942\n","Training log: 142 epoch (15488 / 50000 train. data). Loss: 1.1524181365966797\n","Training log: 142 epoch (16768 / 50000 train. data). Loss: 1.337203860282898\n","Training log: 142 epoch (18048 / 50000 train. data). Loss: 1.1262667179107666\n","Training log: 142 epoch (19328 / 50000 train. data). Loss: 1.2708349227905273\n","Training log: 142 epoch (20608 / 50000 train. data). Loss: 1.2973802089691162\n","Training log: 142 epoch (21888 / 50000 train. data). Loss: 1.34828782081604\n","Training log: 142 epoch (23168 / 50000 train. data). Loss: 1.1012247800827026\n","Training log: 142 epoch (24448 / 50000 train. data). Loss: 1.1526048183441162\n","Training log: 142 epoch (25728 / 50000 train. data). Loss: 1.3141167163848877\n","Training log: 142 epoch (27008 / 50000 train. data). Loss: 1.1833925247192383\n","Training log: 142 epoch (28288 / 50000 train. data). Loss: 1.296949028968811\n","Training log: 142 epoch (29568 / 50000 train. data). Loss: 1.2242205142974854\n","Training log: 142 epoch (30848 / 50000 train. data). Loss: 1.1646043062210083\n","Training log: 142 epoch (32128 / 50000 train. data). Loss: 1.1706349849700928\n","Training log: 142 epoch (33408 / 50000 train. data). Loss: 1.2899805307388306\n","Training log: 142 epoch (34688 / 50000 train. data). Loss: 1.1847894191741943\n","Training log: 142 epoch (35968 / 50000 train. data). Loss: 1.1175386905670166\n","Training log: 142 epoch (37248 / 50000 train. data). Loss: 1.2171450853347778\n","Training log: 142 epoch (38528 / 50000 train. data). Loss: 1.0823825597763062\n","Training log: 142 epoch (39808 / 50000 train. data). Loss: 1.1932673454284668\n","Training log: 142 epoch (41088 / 50000 train. data). Loss: 1.2988406419754028\n","Training log: 142 epoch (42368 / 50000 train. data). Loss: 1.1492887735366821\n","Training log: 142 epoch (43648 / 50000 train. data). Loss: 1.213112473487854\n","Training log: 142 epoch (44928 / 50000 train. data). Loss: 1.242409586906433\n","Training log: 142 epoch (46208 / 50000 train. data). Loss: 1.3545745611190796\n","Training log: 142 epoch (47488 / 50000 train. data). Loss: 1.1712610721588135\n","Training log: 142 epoch (48768 / 50000 train. data). Loss: 1.2935266494750977\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:10, 36.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 142 epoch (50048 / 50000 train. data). Loss: 1.3425838947296143\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.56it/s]\n","100%|██████████| 79/79 [00:02<00:00, 35.27it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.619500\n","Training log: 143 epoch (128 / 50000 train. data). Loss: 1.224331259727478\n","Training log: 143 epoch (1408 / 50000 train. data). Loss: 1.1465892791748047\n","Training log: 143 epoch (2688 / 50000 train. data). Loss: 1.1914921998977661\n","Training log: 143 epoch (3968 / 50000 train. data). Loss: 1.322809100151062\n","Training log: 143 epoch (5248 / 50000 train. data). Loss: 1.1504061222076416\n","Training log: 143 epoch (6528 / 50000 train. data). Loss: 1.1343059539794922\n","Training log: 143 epoch (7808 / 50000 train. data). Loss: 1.2951428890228271\n","Training log: 143 epoch (9088 / 50000 train. data). Loss: 1.3056997060775757\n","Training log: 143 epoch (10368 / 50000 train. data). Loss: 1.1938234567642212\n","Training log: 143 epoch (11648 / 50000 train. data). Loss: 1.194028377532959\n","Training log: 143 epoch (12928 / 50000 train. data). Loss: 1.1941406726837158\n","Training log: 143 epoch (14208 / 50000 train. data). Loss: 1.31459641456604\n","Training log: 143 epoch (15488 / 50000 train. data). Loss: 1.1501507759094238\n","Training log: 143 epoch (16768 / 50000 train. data). Loss: 1.170761227607727\n","Training log: 143 epoch (18048 / 50000 train. data). Loss: 1.3350138664245605\n","Training log: 143 epoch (19328 / 50000 train. data). Loss: 1.1925655603408813\n","Training log: 143 epoch (20608 / 50000 train. data). Loss: 1.0267950296401978\n","Training log: 143 epoch (21888 / 50000 train. data). Loss: 1.2187221050262451\n","Training log: 143 epoch (23168 / 50000 train. data). Loss: 1.1840730905532837\n","Training log: 143 epoch (24448 / 50000 train. data). Loss: 1.1025112867355347\n","Training log: 143 epoch (25728 / 50000 train. data). Loss: 1.172148585319519\n","Training log: 143 epoch (27008 / 50000 train. data). Loss: 1.1147440671920776\n","Training log: 143 epoch (28288 / 50000 train. data). Loss: 1.074771523475647\n","Training log: 143 epoch (29568 / 50000 train. data). Loss: 1.2091190814971924\n","Training log: 143 epoch (30848 / 50000 train. data). Loss: 1.2453640699386597\n","Training log: 143 epoch (32128 / 50000 train. data). Loss: 1.179322361946106\n","Training log: 143 epoch (33408 / 50000 train. data). Loss: 1.240957260131836\n","Training log: 143 epoch (34688 / 50000 train. data). Loss: 1.3539494276046753\n","Training log: 143 epoch (35968 / 50000 train. data). Loss: 0.9761175513267517\n","Training log: 143 epoch (37248 / 50000 train. data). Loss: 1.1414765119552612\n","Training log: 143 epoch (38528 / 50000 train. data). Loss: 1.3851351737976074\n","Training log: 143 epoch (39808 / 50000 train. data). Loss: 1.168176293373108\n","Training log: 143 epoch (41088 / 50000 train. data). Loss: 1.0703896284103394\n","Training log: 143 epoch (42368 / 50000 train. data). Loss: 1.2146655321121216\n","Training log: 143 epoch (43648 / 50000 train. data). Loss: 1.266522765159607\n","Training log: 143 epoch (44928 / 50000 train. data). Loss: 1.3201196193695068\n","Training log: 143 epoch (46208 / 50000 train. data). Loss: 1.3745449781417847\n","Training log: 143 epoch (47488 / 50000 train. data). Loss: 1.0705416202545166\n","Training log: 143 epoch (48768 / 50000 train. data). Loss: 1.1054432392120361\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:12, 31.21it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 143 epoch (50048 / 50000 train. data). Loss: 1.2274221181869507\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.18it/s]\n","100%|██████████| 79/79 [00:02<00:00, 36.18it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.607800\n","Training log: 144 epoch (128 / 50000 train. data). Loss: 1.0383347272872925\n","Training log: 144 epoch (1408 / 50000 train. data). Loss: 1.1403839588165283\n","Training log: 144 epoch (2688 / 50000 train. data). Loss: 1.1768969297409058\n","Training log: 144 epoch (3968 / 50000 train. data). Loss: 1.40341317653656\n","Training log: 144 epoch (5248 / 50000 train. data). Loss: 1.2596420049667358\n","Training log: 144 epoch (6528 / 50000 train. data). Loss: 1.1964691877365112\n","Training log: 144 epoch (7808 / 50000 train. data). Loss: 1.1372599601745605\n","Training log: 144 epoch (9088 / 50000 train. data). Loss: 1.1808065176010132\n","Training log: 144 epoch (10368 / 50000 train. data). Loss: 1.1637879610061646\n","Training log: 144 epoch (11648 / 50000 train. data). Loss: 1.1616759300231934\n","Training log: 144 epoch (12928 / 50000 train. data). Loss: 1.0469149351119995\n","Training log: 144 epoch (14208 / 50000 train. data). Loss: 1.2010291814804077\n","Training log: 144 epoch (15488 / 50000 train. data). Loss: 1.0546003580093384\n","Training log: 144 epoch (16768 / 50000 train. data). Loss: 1.2716633081436157\n","Training log: 144 epoch (18048 / 50000 train. data). Loss: 1.2056858539581299\n","Training log: 144 epoch (19328 / 50000 train. data). Loss: 1.209735631942749\n","Training log: 144 epoch (20608 / 50000 train. data). Loss: 1.3304439783096313\n","Training log: 144 epoch (21888 / 50000 train. data). Loss: 1.279509425163269\n","Training log: 144 epoch (23168 / 50000 train. data). Loss: 1.2879341840744019\n","Training log: 144 epoch (24448 / 50000 train. data). Loss: 1.0592286586761475\n","Training log: 144 epoch (25728 / 50000 train. data). Loss: 1.2940130233764648\n","Training log: 144 epoch (27008 / 50000 train. data). Loss: 1.1835095882415771\n","Training log: 144 epoch (28288 / 50000 train. data). Loss: 1.3466745615005493\n","Training log: 144 epoch (29568 / 50000 train. data). Loss: 1.2240984439849854\n","Training log: 144 epoch (30848 / 50000 train. data). Loss: 1.241654396057129\n","Training log: 144 epoch (32128 / 50000 train. data). Loss: 1.3222514390945435\n","Training log: 144 epoch (33408 / 50000 train. data). Loss: 1.2857728004455566\n","Training log: 144 epoch (34688 / 50000 train. data). Loss: 1.2774152755737305\n","Training log: 144 epoch (35968 / 50000 train. data). Loss: 1.2945759296417236\n","Training log: 144 epoch (37248 / 50000 train. data). Loss: 1.1373785734176636\n","Training log: 144 epoch (38528 / 50000 train. data). Loss: 1.1654155254364014\n","Training log: 144 epoch (39808 / 50000 train. data). Loss: 1.1623144149780273\n","Training log: 144 epoch (41088 / 50000 train. data). Loss: 1.3672130107879639\n","Training log: 144 epoch (42368 / 50000 train. data). Loss: 1.1868942975997925\n","Training log: 144 epoch (43648 / 50000 train. data). Loss: 1.3087267875671387\n","Training log: 144 epoch (44928 / 50000 train. data). Loss: 1.3444195985794067\n","Training log: 144 epoch (46208 / 50000 train. data). Loss: 1.3388988971710205\n","Training log: 144 epoch (47488 / 50000 train. data). Loss: 1.310302734375\n","Training log: 144 epoch (48768 / 50000 train. data). Loss: 1.2938611507415771\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:12, 30.36it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 144 epoch (50048 / 50000 train. data). Loss: 1.1145503520965576\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.26it/s]\n","100%|██████████| 79/79 [00:02<00:00, 33.57it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.616100\n","Training log: 145 epoch (128 / 50000 train. data). Loss: 1.2714473009109497\n","Training log: 145 epoch (1408 / 50000 train. data). Loss: 1.1933022737503052\n","Training log: 145 epoch (2688 / 50000 train. data). Loss: 1.2046101093292236\n","Training log: 145 epoch (3968 / 50000 train. data). Loss: 1.037720799446106\n","Training log: 145 epoch (5248 / 50000 train. data). Loss: 1.2138723134994507\n","Training log: 145 epoch (6528 / 50000 train. data). Loss: 1.318698763847351\n","Training log: 145 epoch (7808 / 50000 train. data). Loss: 1.1905276775360107\n","Training log: 145 epoch (9088 / 50000 train. data). Loss: 1.289987564086914\n","Training log: 145 epoch (10368 / 50000 train. data). Loss: 1.172258734703064\n","Training log: 145 epoch (11648 / 50000 train. data). Loss: 1.1688402891159058\n","Training log: 145 epoch (12928 / 50000 train. data). Loss: 1.2602918148040771\n","Training log: 145 epoch (14208 / 50000 train. data). Loss: 1.2445193529129028\n","Training log: 145 epoch (15488 / 50000 train. data). Loss: 1.3641389608383179\n","Training log: 145 epoch (16768 / 50000 train. data). Loss: 1.4491722583770752\n","Training log: 145 epoch (18048 / 50000 train. data). Loss: 1.233388066291809\n","Training log: 145 epoch (19328 / 50000 train. data). Loss: 1.2140835523605347\n","Training log: 145 epoch (20608 / 50000 train. data). Loss: 1.2151129245758057\n","Training log: 145 epoch (21888 / 50000 train. data). Loss: 1.344734787940979\n","Training log: 145 epoch (23168 / 50000 train. data). Loss: 1.1596661806106567\n","Training log: 145 epoch (24448 / 50000 train. data). Loss: 1.0714492797851562\n","Training log: 145 epoch (25728 / 50000 train. data). Loss: 1.1641230583190918\n","Training log: 145 epoch (27008 / 50000 train. data). Loss: 1.2815253734588623\n","Training log: 145 epoch (28288 / 50000 train. data). Loss: 1.249890923500061\n","Training log: 145 epoch (29568 / 50000 train. data). Loss: 1.1259156465530396\n","Training log: 145 epoch (30848 / 50000 train. data). Loss: 1.2517133951187134\n","Training log: 145 epoch (32128 / 50000 train. data). Loss: 1.2271580696105957\n","Training log: 145 epoch (33408 / 50000 train. data). Loss: 1.2200590372085571\n","Training log: 145 epoch (34688 / 50000 train. data). Loss: 1.2113202810287476\n","Training log: 145 epoch (35968 / 50000 train. data). Loss: 1.2602603435516357\n","Training log: 145 epoch (37248 / 50000 train. data). Loss: 1.0527492761611938\n","Training log: 145 epoch (38528 / 50000 train. data). Loss: 1.2435646057128906\n","Training log: 145 epoch (39808 / 50000 train. data). Loss: 1.2064989805221558\n","Training log: 145 epoch (41088 / 50000 train. data). Loss: 1.1641974449157715\n","Training log: 145 epoch (42368 / 50000 train. data). Loss: 1.1031275987625122\n","Training log: 145 epoch (43648 / 50000 train. data). Loss: 1.2136377096176147\n","Training log: 145 epoch (44928 / 50000 train. data). Loss: 1.187768578529358\n","Training log: 145 epoch (46208 / 50000 train. data). Loss: 1.2217684984207153\n","Training log: 145 epoch (47488 / 50000 train. data). Loss: 1.2751479148864746\n","Training log: 145 epoch (48768 / 50000 train. data). Loss: 1.1064209938049316\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:09, 39.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 145 epoch (50048 / 50000 train. data). Loss: 1.0130339860916138\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:10<00:00, 35.84it/s]\n","100%|██████████| 79/79 [00:02<00:00, 34.28it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.618200\n","Training log: 146 epoch (128 / 50000 train. data). Loss: 1.3127588033676147\n","Training log: 146 epoch (1408 / 50000 train. data). Loss: 1.1275097131729126\n","Training log: 146 epoch (2688 / 50000 train. data). Loss: 1.1647485494613647\n","Training log: 146 epoch (3968 / 50000 train. data). Loss: 1.216810703277588\n","Training log: 146 epoch (5248 / 50000 train. data). Loss: 1.2615265846252441\n","Training log: 146 epoch (6528 / 50000 train. data). Loss: 1.1627726554870605\n","Training log: 146 epoch (7808 / 50000 train. data). Loss: 1.318658709526062\n","Training log: 146 epoch (9088 / 50000 train. data). Loss: 1.197637915611267\n","Training log: 146 epoch (10368 / 50000 train. data). Loss: 1.194285273551941\n","Training log: 146 epoch (11648 / 50000 train. data). Loss: 1.2920037508010864\n","Training log: 146 epoch (12928 / 50000 train. data). Loss: 1.2577990293502808\n","Training log: 146 epoch (14208 / 50000 train. data). Loss: 1.411744475364685\n","Training log: 146 epoch (15488 / 50000 train. data). Loss: 1.1296697854995728\n","Training log: 146 epoch (16768 / 50000 train. data). Loss: 1.2833974361419678\n","Training log: 146 epoch (18048 / 50000 train. data). Loss: 1.3300132751464844\n","Training log: 146 epoch (19328 / 50000 train. data). Loss: 1.079643726348877\n","Training log: 146 epoch (20608 / 50000 train. data). Loss: 1.2024592161178589\n","Training log: 146 epoch (21888 / 50000 train. data). Loss: 1.205494999885559\n","Training log: 146 epoch (23168 / 50000 train. data). Loss: 1.1393646001815796\n","Training log: 146 epoch (24448 / 50000 train. data). Loss: 1.2620189189910889\n","Training log: 146 epoch (25728 / 50000 train. data). Loss: 1.3418679237365723\n","Training log: 146 epoch (27008 / 50000 train. data). Loss: 1.2697004079818726\n","Training log: 146 epoch (28288 / 50000 train. data). Loss: 1.0273351669311523\n","Training log: 146 epoch (29568 / 50000 train. data). Loss: 1.2725753784179688\n","Training log: 146 epoch (30848 / 50000 train. data). Loss: 1.1549111604690552\n","Training log: 146 epoch (32128 / 50000 train. data). Loss: 1.1567879915237427\n","Training log: 146 epoch (33408 / 50000 train. data). Loss: 1.0838565826416016\n","Training log: 146 epoch (34688 / 50000 train. data). Loss: 1.0600404739379883\n","Training log: 146 epoch (35968 / 50000 train. data). Loss: 1.3518434762954712\n","Training log: 146 epoch (37248 / 50000 train. data). Loss: 1.0595495700836182\n","Training log: 146 epoch (38528 / 50000 train. data). Loss: 1.3268455266952515\n","Training log: 146 epoch (39808 / 50000 train. data). Loss: 1.2408068180084229\n","Training log: 146 epoch (41088 / 50000 train. data). Loss: 1.3770897388458252\n","Training log: 146 epoch (42368 / 50000 train. data). Loss: 1.1777790784835815\n","Training log: 146 epoch (43648 / 50000 train. data). Loss: 1.2609963417053223\n","Training log: 146 epoch (44928 / 50000 train. data). Loss: 1.3833481073379517\n","Training log: 146 epoch (46208 / 50000 train. data). Loss: 1.0788661241531372\n","Training log: 146 epoch (47488 / 50000 train. data). Loss: 1.38926362991333\n","Training log: 146 epoch (48768 / 50000 train. data). Loss: 1.1038495302200317\n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 4/391 [00:00<00:11, 32.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training log: 146 epoch (50048 / 50000 train. data). Loss: 1.1216484308242798\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 391/391 [00:11<00:00, 35.20it/s]\n","100%|██████████| 79/79 [00:02<00:00, 32.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy : 0.619500\n","Training log: 147 epoch (128 / 50000 train. data). Loss: 1.2476892471313477\n","Training log: 147 epoch (1408 / 50000 train. data). Loss: 1.3266111612319946\n","Training log: 147 epoch (2688 / 50000 train. data). Loss: 1.215930461883545\n","Training log: 147 epoch (3968 / 50000 train. data). Loss: 1.3105144500732422\n","Training log: 147 epoch (5248 / 50000 train. data). Loss: 1.1725051403045654\n","Training log: 147 epoch (6528 / 50000 train. data). Loss: 1.1147825717926025\n","Training log: 147 epoch (7808 / 50000 train. data). Loss: 1.1401101350784302\n","Training log: 147 epoch (9088 / 50000 train. data). Loss: 1.222610354423523\n","Training log: 147 epoch (10368 / 50000 train. data). Loss: 1.1979117393493652\n","Training log: 147 epoch (11648 / 50000 train. data). Loss: 1.284712553024292\n","Training log: 147 epoch (12928 / 50000 train. data). Loss: 1.144633412361145\n","Training log: 147 epoch (14208 / 50000 train. data). Loss: 1.2870509624481201\n","Training log: 147 epoch (15488 / 50000 train. data). Loss: 1.2457516193389893\n","Training log: 147 epoch (16768 / 50000 train. data). Loss: 1.0736407041549683\n","Training log: 147 epoch (18048 / 50000 train. data). Loss: 1.1065677404403687\n","Training log: 147 epoch (19328 / 50000 train. data). Loss: 1.3547526597976685\n","Training log: 147 epoch (20608 / 50000 train. data). Loss: 1.2093855142593384\n","Training log: 147 epoch (21888 / 50000 train. data). Loss: 1.2230303287506104\n","Training log: 147 epoch (23168 / 50000 train. data). Loss: 1.3247588872909546\n","Training log: 147 epoch (24448 / 50000 train. data). Loss: 1.2445416450500488\n","Training log: 147 epoch (25728 / 50000 train. data). Loss: 1.2313812971115112\n","Training log: 147 epoch (27008 / 50000 train. data). Loss: 1.3192700147628784\n","Training log: 147 epoch (28288 / 50000 train. data). Loss: 1.1626414060592651\n","Training log: 147 epoch (29568 / 50000 train. data). Loss: 1.1511037349700928\n","Training log: 147 epoch (30848 / 50000 train. data). Loss: 1.3624749183654785\n","Training log: 147 epoch (32128 / 50000 train. data). Loss: 1.280977725982666\n","Training log: 147 epoch (33408 / 50000 train. data). Loss: 1.1846085786819458\n","Training log: 147 epoch (34688 / 50000 train. data). Loss: 1.134963035583496\n","Training log: 147 epoch (35968 / 50000 train. data). Loss: 1.2536219358444214\n","Training log: 147 epoch (37248 / 50000 train. data). Loss: 1.1260722875595093\n","Training log: 147 epoch (38528 / 50000 train. data). Loss: 1.2918308973312378\n","Training log: 147 epoch (39808 / 50000 train. data). Loss: 1.290217638015747\n","Training log: 147 epoch (41088 / 50000 train. data). Loss: 1.1123570203781128\n","Training log: 147 epoch (42368 / 50000 train. data). Loss: 1.173858404159546\n","Training log: 147 epoch (43648 / 50000 train. data). Loss: 1.239174485206604\n","Training log: 147 epoch (44928 / 50000 train. data). Loss: 1.2414846420288086\n","Training log: 147 epoch (46208 / 50000 train. data). Loss: 1.1408381462097168\n","Training log: 147 epoch (47488 / 50000 train. data). Loss: 1.403944730758667\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2qhZhb0ar4Lu","executionInfo":{"status":"ok","timestamp":1609686569990,"user_tz":-540,"elapsed":709,"user":{"displayName":"Hioryuki Onishi","photoUrl":"","userId":"17098326242855842755"}},"outputId":"14461c30-8594-4054-e1af-191a8e2a6b50"},"source":["!pwd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ef4Nb2uwr5ob","executionInfo":{"status":"ok","timestamp":1609685751758,"user_tz":-540,"elapsed":626,"user":{"displayName":"Hioryuki Onishi","photoUrl":"","userId":"17098326242855842755"}},"outputId":"1aa1a4c8-bd34-4268-ac23-11cff0389137"},"source":["!ls"],"execution_count":4,"outputs":[{"output_type":"stream","text":["cifar10_cnn_003.ipynb  data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ne5JkmXar_-T"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K2k9UeHCsNrp"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]}]}